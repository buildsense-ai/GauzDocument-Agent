"""
æ–‡æ¡£æœç´¢å·¥å…· - å†…å®¹æœç´¢
ç‹¬ç«‹å·¥å…·ï¼Œè¾“å…¥query_textç­‰å‚æ•°ï¼Œè¾“å‡ºJSONæ ¼å¼çš„retrieved text/image/table
"""
import os
import json
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å¯¼å…¥ç»„ä»¶
from rag_tool_chroma import RAGTool
from llm_client import LLMClient, LLMConfig

logger = logging.getLogger(__name__)

class DocumentSearchTool:
    """
    æ–‡æ¡£æœç´¢å·¥å…· - ç»Ÿä¸€å†…å®¹æœç´¢
    
    åŠŸèƒ½ï¼š
    1. æ”¯æŒæ–‡æœ¬ã€å›¾ç‰‡ã€è¡¨æ ¼çš„ç»Ÿä¸€æœç´¢
    2. æ··åˆæœç´¢å¼•æ“ï¼ˆå‘é‡ç›¸ä¼¼æ€§ + BM25å…³é”®è¯åŒ¹é…ï¼‰
    3. è¿”å›æ ‡å‡†åŒ–JSONæ ¼å¼ç»“æœ
    """
    
    def __init__(self, storage_dir: str = None):
        """åˆå§‹åŒ–æ–‡æ¡£æœç´¢å·¥å…·"""
        # ä½¿ç”¨final_chromadbä½œä¸ºé»˜è®¤å­˜å‚¨ç›®å½•
        self.storage_dir = storage_dir or os.getenv("RAG_STORAGE_DIR", "final_chromadb")
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯ - å¿…é¡»ä½¿ç”¨Qwen v4çš„1024ç»´embedding
        try:
            self.llm_client = LLMClient(LLMConfig())
            logger.info("âœ… LLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨Qwen 1024ç»´embedding")
        except Exception as e:
            logger.error(f"âŒ LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.error("âŒ æ— æ³•ä½¿ç”¨1024ç»´embeddingï¼Œæ•°æ®collectionéœ€è¦1024ç»´")
            logger.error("ğŸ’¡ è¯·æ£€æŸ¥APIå¯†é’¥é…ç½®æˆ–ç½‘ç»œè¿æ¥")
            # ä¸ºäº†ä¸ç°æœ‰çš„1024ç»´æ•°æ®å…¼å®¹ï¼Œè¿™é‡Œå…ˆè®¾ç½®ä¸ºNoneï¼Œè®©RAGToolå¤„ç†
            self.llm_client = None
        
        # åˆå§‹åŒ–RAGå·¥å…·ï¼ˆä¼ å…¥LLMå®¢æˆ·ç«¯ï¼‰
        self.rag_tool = RAGTool(self.storage_dir, self.llm_client)
        
        logger.info("âœ… æ–‡æ¡£æœç´¢å·¥å…·åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“ å­˜å‚¨ç›®å½•: {self.storage_dir}")
        logger.info("ğŸ” åŠŸèƒ½: ç»Ÿä¸€å†…å®¹æœç´¢ (æ–‡æœ¬+å›¾ç‰‡+è¡¨æ ¼)")
    
    def search_documents(self, query_text: str, project_name: str = "all", 
                        top_k: int = 5, content_type: str = "all") -> str:
        """
        æœç´¢æ–‡æ¡£å†…å®¹ - ä¸»è¦æ¥å£
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            project_name: é¡¹ç›®åç§° (é»˜è®¤"all")
            top_k: è¿”å›ç»“æœæ•°é‡ (é»˜è®¤5)
            content_type: å†…å®¹ç±»å‹ (é»˜è®¤"all")
            
        Returns:
            æ ‡å‡†åŒ–çš„JSONå“åº”å­—ç¬¦ä¸²
        """
        try:
            logger.info(f"ğŸ” å¼€å§‹æ–‡æ¡£æœç´¢:")
            logger.info(f"   ğŸ“ æŸ¥è¯¢: {query_text}")
            logger.info(f"   ğŸ“Š é¡¹ç›®: {project_name}")
            logger.info(f"   ğŸ“Š æ•°é‡: {top_k}")
            logger.info(f"   ğŸ¯ ç±»å‹: {content_type}")
            
            if not query_text:
                error_msg = "æŸ¥è¯¢æ–‡æœ¬ä¸èƒ½ä¸ºç©º"
                logger.error(f"âŒ {error_msg}")
                return json.dumps({
                    "status": "error",
                    "message": error_msg,
                    "retrieved_text": [],
                    "retrieved_image": [],
                    "retrieved_table": []
                }, ensure_ascii=False, indent=2)
            
            # æ ¹æ®content_typeå†³å®šæœç´¢ç­–ç•¥
            if content_type == "text":
                # åªæœç´¢æ–‡æœ¬
                text_results = self._unified_content_search(project_name, query_text, top_k, "text_chunk")
                image_results = []
                table_results = []
                
            elif content_type == "image":
                # åªæœç´¢å›¾ç‰‡
                text_results = []
                image_results = self._unified_content_search(project_name, query_text, top_k, "image_chunk")
                table_results = []
                
            elif content_type == "table":
                # åªæœç´¢è¡¨æ ¼
                text_results = []
                image_results = []
                table_results = self._unified_content_search(project_name, query_text, top_k, "table_chunk")
                
            else:
                # content_type="all": æœç´¢æ‰€æœ‰ç±»å‹ï¼Œæ¯ç§ç±»å‹è¿”å›top3
                logger.info("ğŸ”„ æ‰§è¡Œç»Ÿä¸€æ··åˆæœç´¢ - æ‰€æœ‰å†…å®¹ç±»å‹")
                
                # æœç´¢æ‰€æœ‰ç±»å‹çš„å†…å®¹
                text_candidates = self._unified_content_search(project_name, query_text, top_k, "text_chunk")
                image_candidates = self._unified_content_search(project_name, query_text, top_k, "image_chunk")
                table_candidates = self._unified_content_search(project_name, query_text, top_k, "table_chunk")
                
                logger.info(f"ğŸ“Š ç»Ÿä¸€æœç´¢å€™é€‰ç»“æœ: text({len(text_candidates)}) + image({len(image_candidates)}) + table({len(table_candidates)})")
                
                # æ–‡æœ¬ä½¿ç”¨ä¸¤é˜¶æ®µç­–ç•¥çš„å…¨éƒ¨ç»“æœ(å›ºå®š5ä¸ª)ï¼Œå›¾ç‰‡è¡¨æ ¼å–top3
                text_results = text_candidates  # ä¸¤é˜¶æ®µæœç´¢å·²ç»å›ºå®šè¿”å›5ä¸ª
                image_results = image_candidates[:3]
                table_results = table_candidates[:3]
            
            # æ ¼å¼åŒ–ç»“æœ
            formatted_text = [self._format_text_chunk(result) for result in text_results]
            formatted_image = [self._format_image_chunk(result) for result in image_results]
            formatted_table = [self._format_table_chunk(result) for result in table_results]
            
            # ç”Ÿæˆå“åº”
            response = {
                "status": "success",
                "message": f"search_documentæ‰§è¡Œå®Œæˆ",
                "query_text": query_text,
                "project_name": project_name,
                "content_type": content_type,
                "total_results": len(text_results) + len(image_results) + len(table_results),
                "retrieved_text": formatted_text,
                "retrieved_image": formatted_image,
                "retrieved_table": formatted_table,
                "search_metadata": {
                    "search_timestamp": datetime.now().isoformat(),
                    "search_strategy": "ç»Ÿä¸€æ··åˆæœç´¢ (embedding + BM25)",
                    "top_k": top_k,
                    "project_filter": project_name != "all"
                }
            }
            
            logger.info(f"ğŸ“Š æœç´¢å®Œæˆ:")
            logger.info(f"   ğŸ“ æ–‡æœ¬ç‰‡æ®µ: {len(formatted_text)}ä¸ª")
            logger.info(f"   ğŸ–¼ï¸ å›¾ç‰‡ç‰‡æ®µ: {len(formatted_image)}ä¸ª")
            logger.info(f"   ğŸ“Š è¡¨æ ¼ç‰‡æ®µ: {len(formatted_table)}ä¸ª")
            
            return json.dumps(response, ensure_ascii=False, indent=2)
            
        except Exception as e:
            logger.error(f"âŒ æ–‡æ¡£æœç´¢å¤±è´¥: {e}")
            return json.dumps({
                "status": "error",
                "message": f"æœç´¢å¤±è´¥: {str(e)}",
                "retrieved_text": [],
                "retrieved_image": [],
                "retrieved_table": []
            }, ensure_ascii=False, indent=2)
    
    def search_documents_by_params(self, params: Dict[str, Any]) -> str:
        """
        é€šè¿‡å‚æ•°å­—å…¸æœç´¢æ–‡æ¡£ï¼ˆå…¼å®¹åŸæ¥å£æ ¼å¼ï¼‰
        
        Args:
            params: å‚æ•°å­—å…¸ï¼ŒåŒ…å«query_textç­‰å­—æ®µ
            
        Returns:
            æ ‡å‡†åŒ–çš„JSONå“åº”å­—ç¬¦ä¸²
        """
        try:
            # æå–å‚æ•° - æ”¯æŒå¤šç§å‚æ•°æ ¼å¼
            query_text = params.get("query_text", "") or params.get("query", "")
            
            # å¦‚æœæ²¡æœ‰query_textï¼Œå°è¯•ä»querieså‚æ•°ä¸­æå–
            if not query_text and "queries" in params:
                queries = params["queries"]
                if isinstance(queries, list) and queries:
                    query_text = " ".join(queries)
                elif isinstance(queries, str):
                    query_text = queries
                logger.info(f"âœ… ä»querieså‚æ•°è½¬æ¢å¾—åˆ°query_text: '{query_text}'")
            
            project_name = params.get("project_name", "all")
            top_k = params.get("top_k", 5)
            content_type = params.get("content_type", "all")
            
            logger.info(f"ğŸ” å‚æ•°æå–ç»“æœ:")
            logger.info(f"   ğŸ“ åŸå§‹å‚æ•°: {params}")
            logger.info(f"   ğŸ” æå–çš„query_text: '{query_text}'")
            logger.info(f"   ğŸ“Š æå–çš„project_name: '{project_name}'")
            logger.info(f"   ğŸ“Š æå–çš„top_k: {top_k}")
            logger.info(f"   ğŸ¯ æå–çš„content_type: '{content_type}'")
            
            return self.search_documents(query_text, project_name, top_k, content_type)
            
        except Exception as e:
            logger.error(f"âŒ å‚æ•°æœç´¢å¤±è´¥: {e}")
            return json.dumps({
                "status": "error",
                "message": f"å‚æ•°æœç´¢å¤±è´¥: {str(e)}",
                "retrieved_text": [],
                "retrieved_image": [],
                "retrieved_table": []
            }, ensure_ascii=False, indent=2)
    
    def _unified_content_search(self, project_name: str, query: str, top_k: int = 5, chunk_type: str = "text_chunk") -> List[Dict]:
        """
        ç»Ÿä¸€çš„æœç´¢å‡½æ•° - æ”¯æŒæ–‡æœ¬ã€å›¾ç‰‡ã€è¡¨æ ¼çš„ç»Ÿä¸€æœç´¢é€»è¾‘
        
        å¯¹äºtext_chunkä½¿ç”¨ä¸¤é˜¶æ®µæœç´¢ç­–ç•¥ï¼š
        1. æœç´¢chapter_summaryæ‰¾åˆ°æœ€ç›¸å…³ç« èŠ‚
        2. åœ¨è¯¥ç« èŠ‚å†…embeddingæœç´¢text_chunks (top10)
        3. BM25é‡æ’åºåå›ºå®šè¿”å›top5
        
        Args:
            project_name: é¡¹ç›®åç§°
            query: è‡ªç„¶è¯­è¨€æŸ¥è¯¢
            top_k: è¿”å›ç»“æœæ•°é‡ (text_chunkå›ºå®šè¿”å›5ä¸ª)
            chunk_type: å†…å®¹ç±»å‹ ("text_chunk", "image_chunk", "table_chunk")
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        logger.info(f"ğŸ” æ‰§è¡Œç»Ÿä¸€æœç´¢: project={project_name}, query='{query}', type={chunk_type}, top_k={top_k}")
        
        try:
            # å¯¹text_chunkä½¿ç”¨ç‰¹æ®Šçš„ä¸¤é˜¶æ®µæœç´¢ç­–ç•¥
            if chunk_type == "text_chunk":
                return self._two_stage_text_search(project_name, query, top_k)
            
            # å¯¹image_chunkå’Œtable_chunkä½¿ç”¨åŸæœ‰çš„æœç´¢é€»è¾‘
            # 1. æ„å»ºå…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶
            metadata_filter = {
                "type": chunk_type
            }
            
            # å¦‚æœæœ‰é¡¹ç›®åç§°ï¼Œæ·»åŠ åˆ°è¿‡æ»¤æ¡ä»¶
            if project_name and project_name != "all":
                metadata_filter["project_name"] = project_name
            
            # 2. ä½¿ç”¨RAGå·¥å…·è¿›è¡Œå‘é‡æœç´¢
            search_results = self.rag_tool.vector_store.search_documents(
                query=query,
                n_results=top_k,
                where_filter=metadata_filter
            )
            
            # 3. è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            results = []
            for result in search_results:
                # åŸºç¡€å­—æ®µ
                formatted_result = {
                    "content": result.get("content", ""),
                    "metadata": result.get("metadata", {}),
                    "similarity_score": result.get("similarity_score", 0.0),
                    "bm25_score": result.get("bm25_score", 0.0),
                    "final_score": result.get("final_score", 0.0)
                }
                
                # æ ¹æ®chunk_typeæ·»åŠ ç‰¹å®šå­—æ®µ
                if chunk_type == "image_chunk":
                    formatted_result["content_type"] = "image"
                    metadata = result.get("metadata", {})
                    formatted_result["image_path"] = metadata.get("image_path", "")
                    formatted_result["caption"] = metadata.get("caption", "")
                    formatted_result["page_context"] = metadata.get("page_context", "")
                    formatted_result["detailed_description"] = metadata.get("detailed_description", "")
                elif chunk_type == "table_chunk":
                    formatted_result["content_type"] = "table"
                    metadata = result.get("metadata", {})
                    formatted_result["table_path"] = metadata.get("table_path", "")
                    formatted_result["caption"] = metadata.get("caption", "")
                    formatted_result["page_context"] = metadata.get("page_context", "")
                    formatted_result["detailed_description"] = metadata.get("detailed_description", "")
                
                results.append(formatted_result)
            
            logger.info(f"âœ… {chunk_type}æœç´¢å®Œæˆï¼Œæ‰¾åˆ°{len(results)}ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            logger.error(f"âŒ {chunk_type}æœç´¢å¤±è´¥: {e}")
            return []
    
    def _two_stage_text_search(self, project_name: str, query: str, top_k: int = 5) -> List[Dict]:
        """
        ä¸¤é˜¶æ®µæ–‡æœ¬æœç´¢ç­–ç•¥ (æ”¹è¿›ç‰ˆ)
        
        ç¬¬ä¸€é˜¶æ®µï¼šæœç´¢chapter_summaryï¼Œæ‰¾åˆ°æœ€ç›¸å…³çš„top2ç« èŠ‚
        ç¬¬äºŒé˜¶æ®µï¼šæ™ºèƒ½å¤„ç†ç« èŠ‚å’Œtext_chunksçš„ç»„åˆï¼š
        - ä¸¤ä¸ªç« èŠ‚éƒ½æœ‰chunksï¼šæœç´¢æ‰€æœ‰chunks
        - éƒ¨åˆ†ç« èŠ‚æœ‰chunksï¼šæ— chunksç« èŠ‚çš„raw_contentä½œä¸ºè™šæ‹Ÿchunkå‚ä¸æœç´¢
        - ä¸¤ä¸ªç« èŠ‚éƒ½æ— chunksï¼šç›´æ¥è¿”å›ç« èŠ‚raw_content
        ç¬¬ä¸‰é˜¶æ®µï¼šembeddingæœç´¢top10ï¼ŒBM25é‡æ’åºè¿”å›top5
        
        Args:
            project_name: é¡¹ç›®åç§°
            query: è‡ªç„¶è¯­è¨€æŸ¥è¯¢
            top_k: æœ€ç»ˆè¿”å›ç»“æœæ•°é‡ (æ³¨æ„ï¼šä¸¤é˜¶æ®µæœç´¢å›ºå®šè¿”å›5ä¸ªç»“æœ)
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨ (å›ºå®šè¿”å›top5)
        """
        logger.info(f"ğŸ” æ‰§è¡Œä¸¤é˜¶æ®µæ–‡æœ¬æœç´¢: query='{query}', ç­–ç•¥=top2ç« èŠ‚->æ··åˆæœç´¢->top5")
        
        try:
            # ç¬¬ä¸€é˜¶æ®µï¼šæœç´¢chapter_summaryæ‰¾åˆ°æœ€ç›¸å…³çš„top2ç« èŠ‚
            logger.info("ğŸ“– ç¬¬ä¸€é˜¶æ®µï¼šæœç´¢ç« èŠ‚æ‘˜è¦")
            
            chapter_filter = {"type": "chapter_summary"}
            if project_name and project_name != "all":
                chapter_filter["project_name"] = project_name
            
            # æœç´¢æœ€ç›¸å…³çš„top2ç« èŠ‚
            chapter_results = self.rag_tool.vector_store.search_documents(
                query=query,
                n_results=2,  # å–æœ€ç›¸å…³çš„ä¸¤ä¸ªç« èŠ‚
                where_filter=chapter_filter
            )
            
            if not chapter_results:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç« èŠ‚ï¼Œå›é€€åˆ°å…¨å±€æœç´¢")
                return self._fallback_text_search(project_name, query, top_k)
            
            # æå–ç« èŠ‚ä¿¡æ¯
            chapters_info = []
            for chapter_result in chapter_results:
                chapter_metadata = chapter_result.get("metadata", {})
                chapter_id = chapter_metadata.get("chapter_id")
                chapter_title = chapter_metadata.get("chapter_title", "æœªçŸ¥ç« èŠ‚")
                
                if chapter_id:
                    chapters_info.append({
                        "result": chapter_result,
                        "chapter_id": chapter_id,
                        "chapter_title": chapter_title,
                        "metadata": chapter_metadata
                    })
            
            if not chapters_info:
                logger.warning("âš ï¸ æ‰€æœ‰ç« èŠ‚IDä¸ºç©ºï¼Œå›é€€åˆ°å…¨å±€æœç´¢")
                return self._fallback_text_search(project_name, query, top_k)
            
            chapter_ids = [info["chapter_id"] for info in chapters_info]
            chapter_names = [f"{info['chapter_id']} - {info['chapter_title']}" for info in chapters_info]
            logger.info(f"âœ… æ‰¾åˆ°ç›¸å…³ç« èŠ‚: {', '.join(chapter_names)}")
            
            # ç¬¬äºŒé˜¶æ®µï¼šæ™ºèƒ½å¤„ç†ç« èŠ‚å’Œtext_chunksçš„ç»„åˆ
            logger.info(f"ğŸ“ ç¬¬äºŒé˜¶æ®µï¼šåˆ†æç« èŠ‚å†…å®¹ç»„åˆç­–ç•¥")
            
            # æ£€æŸ¥æ¯ä¸ªç« èŠ‚æ˜¯å¦æœ‰text_chunks
            chapters_with_chunks = []
            chapters_without_chunks = []
            
            for chapter_info in chapters_info:
                chapter_id = chapter_info["chapter_id"]
                
                # æ„å»ºç« èŠ‚å†…æœç´¢çš„è¿‡æ»¤æ¡ä»¶
                text_filter = {
                    "$and": [
                        {"type": "text_chunk"},
                        {"chapter_id": chapter_id}
                    ]
                }
                if project_name and project_name != "all":
                    text_filter["$and"].append({"project_name": project_name})
                
                # æ£€æŸ¥è¯¥ç« èŠ‚æ˜¯å¦æœ‰text_chunksï¼ˆåªæŸ¥è¯¢1ä¸ªç”¨äºåˆ¤æ–­ï¼‰
                chapter_chunks = self.rag_tool.vector_store.search_documents(
                    query=query,
                    n_results=1,  # åªç”¨äºæ£€æŸ¥æ˜¯å¦å­˜åœ¨
                    where_filter=text_filter
                )
                
                if chapter_chunks:
                    chapters_with_chunks.append(chapter_info)
                    logger.info(f"  ğŸ“š ç« èŠ‚ {chapter_id} æœ‰text_chunks")
                else:
                    chapters_without_chunks.append(chapter_info)
                    logger.info(f"  ğŸ“„ ç« èŠ‚ {chapter_id} æ— text_chunksï¼Œå°†ä½¿ç”¨raw_content")
            
            # æ ¹æ®ä¸åŒæƒ…å†µå¤„ç†
            if not chapters_with_chunks and not chapters_without_chunks:
                logger.warning("âš ï¸ æ‰€æœ‰ç« èŠ‚éƒ½æ— æ³•å¤„ç†ï¼Œå›é€€åˆ°å…¨å±€æœç´¢")
                return self._fallback_text_search(project_name, query, top_k)
            elif not chapters_with_chunks:
                # æƒ…å†µ3ï¼šæ‰€æœ‰ç« èŠ‚éƒ½æ²¡æœ‰chunksï¼Œç›´æ¥è¿”å›ç« èŠ‚raw_content
                logger.info("ğŸ“– æƒ…å†µ3ï¼šæ‰€æœ‰ç« èŠ‚éƒ½æ— chunksï¼Œè¿”å›ç« èŠ‚æ‘˜è¦å†…å®¹")
                return self._use_multiple_chapter_contents(chapters_without_chunks, query)
            else:
                # æƒ…å†µ1å’Œ2ï¼šè‡³å°‘æœ‰ä¸€ä¸ªç« èŠ‚æœ‰chunksï¼Œè¿›è¡Œæ··åˆæœç´¢
                logger.info(f"ğŸ”„ æƒ…å†µ1/2ï¼šæ··åˆæœç´¢ - {len(chapters_with_chunks)}ä¸ªç« èŠ‚æœ‰chunksï¼Œ{len(chapters_without_chunks)}ä¸ªç« èŠ‚æ— chunks")
                return self._perform_hybrid_chapter_search(chapters_with_chunks, chapters_without_chunks, query, project_name)
            
            # ç§»é™¤äº†åŸæ¥çš„ç¬¬ä¸‰é˜¶æ®µé€»è¾‘ï¼Œç°åœ¨ç”±å…·ä½“çš„å¤„ç†æ–¹æ³•è´Ÿè´£
            
        except Exception as e:
            logger.error(f"âŒ ä¸¤é˜¶æ®µæ–‡æœ¬æœç´¢å¤±è´¥: {e}")
            # å›é€€åˆ°åŸæœ‰çš„å…¨å±€æœç´¢
            return self._fallback_text_search(project_name, query, top_k)
    
    def _fallback_text_search(self, project_name: str, query: str, top_k: int) -> List[Dict]:
        """å›é€€çš„å…¨å±€æ–‡æœ¬æœç´¢"""
        logger.info("ğŸ”„ æ‰§è¡Œå›é€€çš„å…¨å±€æ–‡æœ¬æœç´¢")
        
        try:
            metadata_filter = {"type": "text_chunk"}
            if project_name and project_name != "all":
                metadata_filter["project_name"] = project_name
            
            search_results = self.rag_tool.vector_store.search_documents(
                query=query,
                n_results=top_k,
                where_filter=metadata_filter
            )
            
            results = []
            for result in search_results:
                formatted_result = {
                    "content": result.get("content", ""),
                    "metadata": result.get("metadata", {}),
                    "similarity_score": result.get("similarity_score", 0.0),
                    "bm25_score": result.get("bm25_score", 0.0),
                    "final_score": result.get("final_score", 0.0),
                    "content_type": "text"
                }
                results.append(formatted_result)
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ å›é€€æœç´¢å¤±è´¥: {e}")
            return []
    
    def _apply_text_bm25_reranking(self, results: List[Dict], query: str, top_k: int) -> List[Dict]:
        """
        é’ˆå¯¹æ–‡æœ¬å—çš„BM25é‡æ’åº
        
        ä½¿ç”¨text chunkçš„contentå­—æ®µä½œä¸ºBM25è®¡ç®—æºæ•°æ®
        æœ€ç»ˆåˆ†æ•° = 70% similarity_score + 30% bm25_score
        
        Args:
            results: æœç´¢ç»“æœåˆ—è¡¨
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            top_k: æœ€ç»ˆè¿”å›æ•°é‡
            
        Returns:
            é‡æ’åºåçš„ç»“æœåˆ—è¡¨
        """
        logger.info(f"ğŸ”„ åº”ç”¨æ–‡æœ¬BM25é‡æ’åºï¼Œè¾“å…¥ {len(results)} ä¸ªç»“æœï¼Œè¾“å‡º top{top_k}")
        
        if not results:
            return []
        
        query_terms = query.lower().split()
        
        # BM25å‚æ•°
        k1 = 1.2
        b = 0.75
        
        for result in results:
            # ä½¿ç”¨text chunkçš„contentå­—æ®µè¿›è¡ŒBM25è®¡ç®—
            text_content = result.get("content", "").lower()
            
            if not text_content:
                result["bm25_score"] = 0.0
                continue
            
            # æ–‡æ¡£é•¿åº¦å½’ä¸€åŒ–
            doc_length = len(text_content.split())
            avgdl = 100  # å‡è®¾å¹³å‡æ–‡æ¡£é•¿åº¦ä¸º100è¯
            
            bm25_score = 0.0
            
            for term in query_terms:
                if term in text_content:
                    # è¯é¢‘
                    tf = text_content.count(term)
                    
                    # BM25 TFéƒ¨åˆ†
                    tf_component = (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_length / avgdl)))
                    
                    # ç®€åŒ–çš„IDF
                    idf = 1.0
                    
                    bm25_score += tf_component * idf
            
            # å½’ä¸€åŒ–åˆ†æ•° (0-1èŒƒå›´)
            max_possible_score = len(query_terms) * (k1 + 1) if query_terms else 1
            normalized_bm25 = bm25_score / max_possible_score
            result["bm25_score"] = min(normalized_bm25, 1.0)
            
            # è®¡ç®—æœ€ç»ˆåˆ†æ•°ï¼š70%å‘é‡ç›¸ä¼¼æ€§ + 30%BM25å…³é”®è¯åŒ¹é…
            similarity_score = result.get("similarity_score", 0.0)
            final_score = 0.7 * similarity_score + 0.3 * result["bm25_score"]
            result["final_score"] = final_score
        
        # æŒ‰æœ€ç»ˆåˆ†æ•°é‡æ’åºå¹¶å–top_k
        results.sort(key=lambda x: x.get("final_score", 0), reverse=True)
        top_results = results[:top_k]
        
        logger.info(f"âœ… BM25é‡æ’åºå®Œæˆï¼Œè¿”å›top{len(top_results)}ï¼Œæœ€é«˜åˆ†æ•°: {top_results[0].get('final_score', 0):.3f}")
        return top_results
    
    def _use_multiple_chapter_contents(self, chapters_info: List[Dict], query: str) -> List[Dict]:
        """
        å¤„ç†æ‰€æœ‰ç« èŠ‚éƒ½æ²¡æœ‰text_chunksçš„æƒ…å†µï¼Œè¿”å›ç« èŠ‚raw_content
        
        Args:
            chapters_info: ç« èŠ‚ä¿¡æ¯åˆ—è¡¨
            query: æœç´¢æŸ¥è¯¢
            
        Returns:
            æ ¼å¼åŒ–çš„æœç´¢ç»“æœåˆ—è¡¨
        """
        logger.info(f"ğŸ“– ä½¿ç”¨å¤šä¸ªç« èŠ‚æ‘˜è¦å†…å®¹: {len(chapters_info)}ä¸ªç« èŠ‚")
        
        results = []
        for chapter_info in chapters_info:
            chapter_result = chapter_info["result"]
            chapter_id = chapter_info["chapter_id"]
            chapter_title = chapter_info["chapter_title"]
            
                    # è·å–ç« èŠ‚å†…å®¹ - ä¼˜å…ˆä½¿ç”¨raw_content
        chapter_metadata = chapter_result.get("metadata", {})
        raw_content = chapter_metadata.get("raw_content", "")
        
        # å¦‚æœmetadataä¸­æ²¡æœ‰raw_contentï¼Œå°è¯•ä»ç»“æœçš„contentä¸­è·å–
        if not raw_content:
            raw_content = chapter_result.get("content", "")
        
        # æœ€åæ‰ä½¿ç”¨ai_summaryä½œä¸ºå¤‡é€‰
        if not raw_content:
            raw_content = chapter_metadata.get("ai_summary", "")
            
            if raw_content:
                # è®¡ç®—BM25åˆ†æ•°
                bm25_score = self._calculate_content_bm25_score(raw_content, query)
                
                # æ„é€ ç»“æœ
                formatted_result = {
                    "content": raw_content,
                    "metadata": {
                        "content_id": f"{chapter_id}_summary",
                        "chapter_id": chapter_id,
                        "chapter_title": chapter_title,
                        "content_type": "chapter_content",
                        "source": "chapter_summary"
                    },
                    "similarity_score": chapter_result.get("similarity_score", 0.0),
                    "bm25_score": bm25_score,
                    "final_score": 0.7 * chapter_result.get("similarity_score", 0.0) + 0.3 * bm25_score,
                    "content_type": "text",
                    "chapter_id": chapter_id,
                    "chapter_title": chapter_title,
                    "source": "chapter_summary"
                }
                results.append(formatted_result)
        
        # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åºï¼Œå–å‰5ä¸ª
        results.sort(key=lambda x: x.get("final_score", 0), reverse=True)
        final_results = results[:5]
        
        logger.info(f"âœ… å¤šç« èŠ‚å†…å®¹ç»“æœç”Ÿæˆå®Œæˆï¼Œè¿”å› {len(final_results)} ä¸ªç»“æœ")
        return final_results
    
    def _perform_hybrid_chapter_search(self, chapters_with_chunks: List[Dict], chapters_without_chunks: List[Dict], query: str, project_name: str) -> List[Dict]:
        """
        æ‰§è¡Œæ··åˆç« èŠ‚æœç´¢ï¼šå°†æœ‰chunksçš„ç« èŠ‚çš„text_chunkså’Œæ— chunksç« èŠ‚çš„raw_contentåˆå¹¶æœç´¢
        
        Args:
            chapters_with_chunks: æœ‰text_chunksçš„ç« èŠ‚ä¿¡æ¯
            chapters_without_chunks: æ²¡æœ‰text_chunksçš„ç« èŠ‚ä¿¡æ¯
            query: æœç´¢æŸ¥è¯¢
            project_name: é¡¹ç›®åç§°
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        logger.info(f"ğŸ”„ æ‰§è¡Œæ··åˆç« èŠ‚æœç´¢")
        
        try:
            all_candidates = []
            
            # 1. æ”¶é›†æœ‰chunksçš„ç« èŠ‚çš„æ‰€æœ‰text_chunks
            if chapters_with_chunks:
                logger.info(f"ğŸ“š æ”¶é›†æœ‰chunksç« èŠ‚çš„text_chunks: {[info['chapter_id'] for info in chapters_with_chunks]}")
                
                # æ„å»ºå¤šç« èŠ‚çš„è¿‡æ»¤æ¡ä»¶
                chapter_ids = [info["chapter_id"] for info in chapters_with_chunks]
                
                # æ ¹æ®ç« èŠ‚æ•°é‡æ„å»ºä¸åŒçš„è¿‡æ»¤æ¡ä»¶
                if len(chapter_ids) == 1:
                    # åªæœ‰ä¸€ä¸ªç« èŠ‚ï¼Œä½¿ç”¨ç®€å•æ¡ä»¶
                    text_filter = {
                        "$and": [
                            {"type": "text_chunk"},
                            {"chapter_id": chapter_ids[0]}
                        ]
                    }
                else:
                    # å¤šä¸ªç« èŠ‚ï¼Œä½¿ç”¨$oræ¡ä»¶
                    text_filter = {
                        "$and": [
                            {"type": "text_chunk"},
                            {"$or": [{"chapter_id": chapter_id} for chapter_id in chapter_ids]}
                        ]
                    }
                
                if project_name and project_name != "all":
                    text_filter["$and"].append({"project_name": project_name})
                
                # æœç´¢æ‰€æœ‰ç›¸å…³çš„text_chunks
                text_chunks = self.rag_tool.vector_store.search_documents(
                    query=query,
                    n_results=50,  # æ”¶é›†æ›´å¤šå€™é€‰ï¼Œç¨åä¼šç­›é€‰
                    where_filter=text_filter
                )
                
                logger.info(f"  ğŸ“ æ‰¾åˆ° {len(text_chunks)} ä¸ªtext_chunks")
                all_candidates.extend(text_chunks)
            
            # 2. å°†æ— chunksç« èŠ‚çš„raw_contentè½¬æ¢ä¸ºè™šæ‹Ÿtext_chunk
            if chapters_without_chunks:
                logger.info(f"ğŸ“„ è½¬æ¢æ— chunksç« èŠ‚ä¸ºè™šæ‹Ÿtext_chunk: {[info['chapter_id'] for info in chapters_without_chunks]}")
                
                for chapter_info in chapters_without_chunks:
                    virtual_chunk = self._create_virtual_text_chunk(chapter_info, query)
                    if virtual_chunk:
                        all_candidates.append(virtual_chunk)
                        logger.info(f"  âœ… åˆ›å»ºè™šæ‹Ÿchunk: {chapter_info['chapter_id']}")
            
            # 3. å¦‚æœæ²¡æœ‰å€™é€‰ç»“æœï¼Œå›é€€
            if not all_candidates:
                logger.warning("âš ï¸ æ··åˆæœç´¢æ— å€™é€‰ç»“æœï¼Œå›é€€åˆ°å…¨å±€æœç´¢")
                return self._fallback_text_search(project_name, query, 5)
            
            logger.info(f"ğŸ“Š æ··åˆæœç´¢æ€»å€™é€‰: {len(all_candidates)}ä¸ª")
            
            # 4. å¯¹æ‰€æœ‰å€™é€‰è¿›è¡Œé‡æ–°æ’åºï¼Œé€‰æ‹©top10
            for candidate in all_candidates:
                # å¦‚æœæ²¡æœ‰similarity_scoreï¼Œç»™ä¸€ä¸ªé»˜è®¤å€¼
                if "similarity_score" not in candidate:
                    candidate["similarity_score"] = 0.5
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œå–top10
            all_candidates.sort(key=lambda x: x.get("similarity_score", 0), reverse=True)
            top_candidates = all_candidates[:10]
            
            # 5. å¯¹top10è¿›è¡ŒBM25é‡æ’åºï¼Œè¿”å›top5
            logger.info("ğŸ”„ ç¬¬ä¸‰é˜¶æ®µï¼šBM25é‡æ’åº")
            final_results = self._apply_text_bm25_reranking(top_candidates, query, 5)
            
            # 6. è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            formatted_results = []
            for result in final_results:
                # ä»metadataæˆ–ç›´æ¥ä»resultä¸­è·å–ç« èŠ‚ä¿¡æ¯
                chapter_id = result.get("metadata", {}).get("chapter_id") or result.get("chapter_id", "unknown")
                chapter_title = result.get("metadata", {}).get("chapter_title") or result.get("chapter_title", "æœªçŸ¥ç« èŠ‚")
                
                formatted_result = {
                    "content": result.get("content", ""),
                    "metadata": result.get("metadata", {}),
                    "similarity_score": result.get("similarity_score", 0.0),
                    "bm25_score": result.get("bm25_score", 0.0),
                    "final_score": result.get("final_score", 0.0),
                    "content_type": "text",
                    "chapter_id": chapter_id,
                    "chapter_title": chapter_title
                }
                formatted_results.append(formatted_result)
            
            logger.info(f"âœ… æ··åˆç« èŠ‚æœç´¢å®Œæˆï¼Œè¿”å› {len(formatted_results)} ä¸ªç»“æœ")
            return formatted_results
            
        except Exception as e:
            logger.error(f"âŒ æ··åˆç« èŠ‚æœç´¢å¤±è´¥: {e}")
            return self._fallback_text_search(project_name, query, 5)
    
    def _create_virtual_text_chunk(self, chapter_info: Dict, query: str) -> Dict:
        """
        å°†ç« èŠ‚raw_contentè½¬æ¢ä¸ºè™šæ‹Ÿtext_chunk
        
        Args:
            chapter_info: ç« èŠ‚ä¿¡æ¯
            query: æœç´¢æŸ¥è¯¢
            
        Returns:
            è™šæ‹Ÿtext_chunkå­—å…¸
        """
        chapter_result = chapter_info["result"]
        chapter_id = chapter_info["chapter_id"]
        chapter_title = chapter_info["chapter_title"]
        
        # è·å–ç« èŠ‚å†…å®¹ - ä¼˜å…ˆä½¿ç”¨raw_content
        chapter_metadata = chapter_result.get("metadata", {})
        raw_content = chapter_metadata.get("raw_content", "")
        
        # å¦‚æœmetadataä¸­æ²¡æœ‰raw_contentï¼Œå°è¯•ä»ç»“æœçš„contentä¸­è·å–
        if not raw_content:
            raw_content = chapter_result.get("content", "")
        
        # æœ€åæ‰ä½¿ç”¨ai_summaryä½œä¸ºå¤‡é€‰
        if not raw_content:
            raw_content = chapter_metadata.get("ai_summary", "")
        
        if not raw_content:
            return None
        
        # è®¡ç®—BM25åˆ†æ•°
        bm25_score = self._calculate_content_bm25_score(raw_content, query)
        
        # åˆ›å»ºè™šæ‹Ÿtext_chunk
        virtual_chunk = {
            "content": raw_content,
            "metadata": {
                "content_id": f"{chapter_id}_virtual_chunk",
                "chapter_id": chapter_id,
                "chapter_title": chapter_title,
                "content_type": "virtual_text_chunk",
                "source": "chapter_summary",
                "document_id": chapter_metadata.get("document_id", ""),
                "chunk_type": "virtual",
                "paragraph_index": 0,
                "position_in_chapter": 0,
                "word_count": len(raw_content)
            },
            "similarity_score": chapter_result.get("similarity_score", 0.5),  # ä½¿ç”¨ç« èŠ‚çš„ç›¸ä¼¼åº¦
            "bm25_score": bm25_score,
            "final_score": 0.7 * chapter_result.get("similarity_score", 0.5) + 0.3 * bm25_score,
            "chapter_id": chapter_id,
            "chapter_title": chapter_title
        }
        
        return virtual_chunk
    
    def _use_chapter_content_as_result(self, chapter_result: Dict, chapter_id: str, chapter_title: str, query: str) -> List[Dict]:
        """
        å½“ç« èŠ‚å†…æ²¡æœ‰text_chunksæ—¶ï¼Œä½¿ç”¨ç« èŠ‚æ‘˜è¦çš„raw_contentä½œä¸ºæœç´¢ç»“æœ
        
        Args:
            chapter_result: ç« èŠ‚æœç´¢ç»“æœ
            chapter_id: ç« èŠ‚ID
            chapter_title: ç« èŠ‚æ ‡é¢˜
            query: æœç´¢æŸ¥è¯¢
            
        Returns:
            æ ¼å¼åŒ–çš„æœç´¢ç»“æœåˆ—è¡¨
        """
        logger.info(f"ğŸ“– ä½¿ç”¨ç« èŠ‚æ‘˜è¦å†…å®¹ä½œä¸ºæœç´¢ç»“æœ: {chapter_id}")
        
        try:
            # è·å–ç« èŠ‚å†…å®¹ - ä¼˜å…ˆä½¿ç”¨raw_content
            chapter_metadata = chapter_result.get("metadata", {})
            raw_content = chapter_metadata.get("raw_content", "")
            
            # å¦‚æœmetadataä¸­æ²¡æœ‰raw_contentï¼Œå°è¯•ä»ç»“æœçš„contentä¸­è·å–
            if not raw_content:
                raw_content = chapter_result.get("content", "")
            
            # æœ€åæ‰ä½¿ç”¨ai_summaryä½œä¸ºå¤‡é€‰
            if not raw_content:
                raw_content = chapter_metadata.get("ai_summary", "")
            
            if not raw_content:
                logger.warning(f"âš ï¸ ç« èŠ‚ {chapter_id} æ²¡æœ‰å¯ç”¨å†…å®¹ï¼Œå›é€€åˆ°å…¨å±€æœç´¢")
                return self._fallback_text_search("", query, 3)  # ä½¿ç”¨ç©ºproject_nameé¿å…é‡å¤è¿‡æ»¤
            
            # è®¡ç®—BM25åˆ†æ•°
            bm25_score = self._calculate_content_bm25_score(raw_content, query)
            
            # æ„é€ ç»“æœ
            chapter_content_result = {
                "content": raw_content,
                "metadata": {
                    "content_id": f"{chapter_id}_summary",
                    "chapter_id": chapter_id,
                    "chapter_title": chapter_title,
                    "content_type": "chapter_content",
                    "source": "chapter_summary"
                },
                "similarity_score": chapter_result.get("similarity_score", 0.0),
                "bm25_score": bm25_score,
                "final_score": 0.7 * chapter_result.get("similarity_score", 0.0) + 0.3 * bm25_score
            }
            
            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            formatted_result = {
                "content": raw_content,
                "metadata": chapter_content_result["metadata"],
                "similarity_score": chapter_content_result["similarity_score"],
                "bm25_score": chapter_content_result["bm25_score"],
                "final_score": chapter_content_result["final_score"],
                "content_type": "text",
                "chapter_id": chapter_id,
                "chapter_title": chapter_title,
                "source": "chapter_summary"  # æ˜ç¡®æ ‡è®°æ¥æº
            }
            
            logger.info(f"âœ… ç« èŠ‚å†…å®¹ç»“æœç”Ÿæˆå®Œæˆ: ç›¸ä¼¼åº¦={formatted_result['similarity_score']:.3f}, "
                       f"BM25={formatted_result['bm25_score']:.3f}, æœ€ç»ˆ={formatted_result['final_score']:.3f}")
            
            return [formatted_result]
            
        except Exception as e:
            logger.error(f"âŒ ç« èŠ‚å†…å®¹ç»“æœç”Ÿæˆå¤±è´¥: {e}")
            # æœ€ç»ˆå›é€€åˆ°å…¨å±€æœç´¢
            return self._fallback_text_search("", query, 3)
    
    def _calculate_content_bm25_score(self, content: str, query: str) -> float:
        """
        è®¡ç®—å†…å®¹çš„BM25åˆ†æ•°
        
        Args:
            content: æ–‡æœ¬å†…å®¹
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            
        Returns:
            BM25åˆ†æ•°
        """
        if not content or not query:
            return 0.0
        
        query_terms = query.lower().split()
        text_content = content.lower()
        
        # BM25å‚æ•°
        k1 = 1.2
        b = 0.75
        
        # æ–‡æ¡£é•¿åº¦å½’ä¸€åŒ–
        doc_length = len(text_content.split())
        avgdl = 200  # ç« èŠ‚å†…å®¹é€šå¸¸æ¯”è¾ƒé•¿ï¼Œè°ƒæ•´å¹³å‡é•¿åº¦
        
        bm25_score = 0.0
        
        for term in query_terms:
            if term in text_content:
                # è¯é¢‘
                tf = text_content.count(term)
                
                # BM25 TFéƒ¨åˆ†
                tf_component = (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_length / avgdl)))
                
                # ç®€åŒ–çš„IDF
                idf = 1.0
                
                bm25_score += tf_component * idf
        
        # å½’ä¸€åŒ–åˆ†æ•° (0-1èŒƒå›´)
        max_possible_score = len(query_terms) * (k1 + 1) if query_terms else 1
        normalized_bm25 = bm25_score / max_possible_score
        return min(normalized_bm25, 1.0)
    
    def _format_text_chunk(self, result: Dict) -> Dict[str, Any]:
        """æ ¼å¼åŒ–æ–‡æœ¬å—æ•°æ®"""
        metadata = result.get("metadata", {})
        
        # æ„é€ content_idï¼šå¦‚æœmetadataä¸­æ²¡æœ‰ï¼Œåˆ™æ ¹æ®chapter_idå’Œchunk_indexæ„é€ 
        content_id = metadata.get("content_id", "")
        if not content_id:
            chapter_id = metadata.get("chapter_id", "") or result.get("chapter_id", "")
            chunk_index = metadata.get("chunk_index", 0)
            if chapter_id and chunk_index is not None:
                content_id = f"{chapter_id}_{chunk_index}"
        
        return {
            "content_id": content_id,
            "content": result.get("content", ""),
            "chunk_type": metadata.get("chunk_type", "paragraph"),
            "document_id": metadata.get("document_id", ""),
            "chapter_id": metadata.get("chapter_id", "") or result.get("chapter_id", ""),
            "chapter_title": metadata.get("chapter_title", "") or result.get("chapter_title", ""),
            "paragraph_index": metadata.get("paragraph_index", 0),
            "position_in_chapter": metadata.get("position_in_chapter", 0),
            "word_count": metadata.get("word_count", len(result.get("content", ""))),
            "similarity_score": result.get("similarity_score", 0.0),
            "bm25_score": result.get("bm25_score", 0.0),
            "final_score": result.get("final_score", 0.0),
            "source": metadata.get("source", result.get("source", "text_chunk"))  # æ·»åŠ sourceå­—æ®µ
        }
    
    def _format_image_chunk(self, result: Dict) -> Dict[str, Any]:
        """æ ¼å¼åŒ–å›¾ç‰‡å—æ•°æ®"""
        metadata = result.get("metadata", {})
        image_path = result.get("image_path", "") or metadata.get("image_path", "")
        
        # æ„é€ content_idï¼šå¦‚æœmetadataä¸­æ²¡æœ‰ï¼Œåˆ™æ ¹æ®document_idå’Œimageç¼–å·æ„é€ 
        content_id = metadata.get("content_id", "")
        if not content_id:
            document_id = metadata.get("document_id", "")
            # å°è¯•ä»ç°æœ‰content_idæ¨¡å¼ä¸­æå–ï¼Œæˆ–ä½¿ç”¨å…¶ä»–å­—æ®µæ„é€ 
            image_index = metadata.get("image_index", "") or metadata.get("chunk_index", "")
            if document_id and image_index:
                content_id = f"{document_id}_image_{image_index}"
        
        return {
            "content_id": content_id,
            "image_url": self._generate_minio_url(image_path),
            "image_path": image_path,
            "caption": result.get("caption", "") or metadata.get("caption", ""),
            "ai_description": metadata.get("ai_description", ""),
            "detailed_description": result.get("detailed_description", "") or metadata.get("detailed_description", ""),
            "page_number": metadata.get("page_number", 0),
            "page_context": result.get("page_context", "") or metadata.get("page_context", ""),
            "document_id": metadata.get("document_id", ""),
            "chapter_id": metadata.get("chapter_id", ""),
            "chapter_title": metadata.get("chapter_title", ""),
            "width": metadata.get("width", 0),
            "height": metadata.get("height", 0),
            "aspect_ratio": metadata.get("aspect_ratio", 0.0),
            "similarity_score": result.get("similarity_score", 0.0),
            "bm25_score": result.get("bm25_score", 0.0),
            "final_score": result.get("final_score", 0.0)
        }
    
    def _format_table_chunk(self, result: Dict) -> Dict[str, Any]:
        """æ ¼å¼åŒ–è¡¨æ ¼å—æ•°æ®"""
        metadata = result.get("metadata", {})
        table_path = result.get("table_path", "") or metadata.get("table_path", "")
        
        # æ„é€ content_idï¼šå¦‚æœmetadataä¸­æ²¡æœ‰ï¼Œåˆ™æ ¹æ®document_idå’Œtableç¼–å·æ„é€ 
        content_id = metadata.get("content_id", "")
        if not content_id:
            document_id = metadata.get("document_id", "")
            # å°è¯•ä»ç°æœ‰content_idæ¨¡å¼ä¸­æå–ï¼Œæˆ–ä½¿ç”¨å…¶ä»–å­—æ®µæ„é€ 
            table_index = metadata.get("table_index", "") or metadata.get("chunk_index", "")
            if document_id and table_index:
                content_id = f"{document_id}_table_{table_index}"
        
        return {
            "content_id": content_id,
            "table_url": self._generate_minio_url(table_path),
            "table_path": table_path,
            "caption": result.get("caption", "") or metadata.get("caption", ""),
            "ai_description": metadata.get("ai_description", ""),
            "detailed_description": result.get("detailed_description", "") or metadata.get("detailed_description", ""),
            "page_number": metadata.get("page_number", 0),
            "page_context": result.get("page_context", "") or metadata.get("page_context", ""),
            "document_id": metadata.get("document_id", ""),
            "chapter_id": metadata.get("chapter_id", ""),
            "chapter_title": metadata.get("chapter_title", ""),
            "similarity_score": result.get("similarity_score", 0.0),
            "bm25_score": result.get("bm25_score", 0.0),
            "final_score": result.get("final_score", 0.0)
        }
    
    def _generate_minio_url(self, file_path: str) -> str:
        """
        ç”ŸæˆMinIOè®¿é—®URL
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            å®Œæ•´çš„MinIOè®¿é—®URL
        """
        if not file_path:
            return ""
            
        # ä»ç¯å¢ƒå˜é‡è·å–MinIOé…ç½®
        minio_endpoint = os.getenv("MINIO_ENDPOINT", "http://localhost:9000")
        minio_bucket = os.getenv("MINIO_BUCKET", "document-storage")
        
        # ç¡®ä¿è·¯å¾„æ ¼å¼æ­£ç¡®
        clean_path = file_path.lstrip("/")
        
        return f"{minio_endpoint}/{minio_bucket}/{clean_path}" 