#!/usr/bin/env python3
"""
æµ‹è¯•æ–‡æœ¬åˆ†å—å™¨
éªŒè¯ç¬¬ä¸€çº§ç« èŠ‚åˆ‡å‰²å’Œæœ€å°å—åˆ†å‰²åŠŸèƒ½
"""

import json
import os
import sys
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent / "src"))

# ç›´æ¥å¯¼å…¥TextChunker
from src.pdf_processing.text_chunker import TextChunker

def test_text_chunker():
    """æµ‹è¯•æ–‡æœ¬åˆ†å—å™¨"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•æ–‡æœ¬åˆ†å—å™¨...")
    
    # è¾“å…¥æ–‡ä»¶è·¯å¾„
    test_data_dir = "parser_output/20250714_232720_vvmpc0"
    full_text_file = os.path.join(test_data_dir, "full_text.txt")
    toc_result_file = os.path.join(test_data_dir, "toc_extraction_result.json")
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(full_text_file):
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {full_text_file}")
        return
        
    if not os.path.exists(toc_result_file):
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {toc_result_file}")
        return
    
    # è¯»å–è¾“å…¥æ•°æ®
    print("ğŸ“– è¯»å–è¾“å…¥æ•°æ®...")
    with open(full_text_file, 'r', encoding='utf-8') as f:
        full_text = f.read()
    
    with open(toc_result_file, 'r', encoding='utf-8') as f:
        toc_result = json.load(f)
    
    print(f"âœ… æˆåŠŸè¯»å–æ•°æ®:")
    print(f"   - å…¨æ–‡é•¿åº¦: {len(full_text)} å­—ç¬¦")
    print(f"   - TOCç« èŠ‚æ•°: {len(toc_result['toc'])} ä¸ª")
    
    # åˆ›å»ºåˆ†å—å™¨
    chunker = TextChunker()
    
    # æ‰§è¡Œåˆ†å—
    print("\nğŸ”ª å¼€å§‹æ‰§è¡Œåˆ†å—...")
    result = chunker.chunk_text_with_toc(full_text, toc_result)
    
    # æ˜¾ç¤ºç»“æœç»Ÿè®¡
    print(f"\nğŸ“Š åˆ†å—ç»“æœç»Ÿè®¡:")
    print(f"   - ç¬¬ä¸€çº§ç« èŠ‚æ•°: {result.total_chapters}")
    print(f"   - æœ€å°å—æ€»æ•°: {result.total_chunks}")
    
    # æ˜¾ç¤ºç¬¬ä¸€çº§ç« èŠ‚è¯¦æƒ…
    print(f"\nğŸ“– ç¬¬ä¸€çº§ç« èŠ‚è¯¦æƒ…:")
    for i, chapter in enumerate(result.first_level_chapters):
        print(f"   {i+1}. {chapter.title} (ID: {chapter.chapter_id})")
        print(f"      - å­—æ•°: {chapter.word_count}")
        print(f"      - åŒ…å«å›¾ç‰‡: {'æ˜¯' if chapter.has_images else 'å¦'}")
        print(f"      - åŒ…å«è¡¨æ ¼: {'æ˜¯' if chapter.has_tables else 'å¦'}")
        print(f"      - å†…å®¹é¢„è§ˆ: {chapter.content[:100].strip()}...")
        print()
    
    # æ˜¾ç¤ºæœ€å°å—åˆ†ç±»ç»Ÿè®¡
    chunk_types = {}
    for chunk in result.minimal_chunks:
        chunk_type = chunk.chunk_type
        if chunk_type not in chunk_types:
            chunk_types[chunk_type] = 0
        chunk_types[chunk_type] += 1
    
    print(f"ğŸ“ æœ€å°å—åˆ†ç±»ç»Ÿè®¡:")
    for chunk_type, count in chunk_types.items():
        print(f"   - {chunk_type}: {count} ä¸ª")
    
    # æ˜¾ç¤ºéƒ¨åˆ†æœ€å°å—ç¤ºä¾‹
    print(f"\nğŸ” æœ€å°å—ç¤ºä¾‹:")
    for i, chunk in enumerate(result.minimal_chunks[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
        print(f"   {i+1}. {chunk.chunk_id} ({chunk.chunk_type})")
        print(f"      æ‰€å±ç« èŠ‚: {chunk.chapter_title}")
        print(f"      å­—æ•°: {chunk.word_count}")
        print(f"      å†…å®¹: {chunk.content[:80].strip()}...")
        print()
    
    # ä¿å­˜ç»“æœ
    output_dir = "chunking_test_output"
    print(f"ğŸ’¾ ä¿å­˜åˆ†å—ç»“æœåˆ°: {output_dir}")
    chunker.save_chunking_result(result, output_dir)
    
    # éªŒè¯ä¿å­˜çš„æ–‡ä»¶
    chapters_file = os.path.join(output_dir, "first_level_chapters.json")
    chunks_file = os.path.join(output_dir, "minimal_chunks.json")
    
    if os.path.exists(chapters_file):
        print(f"âœ… ç« èŠ‚æ–‡ä»¶å·²ä¿å­˜: {chapters_file}")
    else:
        print(f"âŒ ç« èŠ‚æ–‡ä»¶ä¿å­˜å¤±è´¥: {chapters_file}")
    
    if os.path.exists(chunks_file):
        print(f"âœ… å—æ–‡ä»¶å·²ä¿å­˜: {chunks_file}")
    else:
        print(f"âŒ å—æ–‡ä»¶ä¿å­˜å¤±è´¥: {chunks_file}")
    
    print("\nğŸ‰ æµ‹è¯•å®Œæˆ!")
    
    return result

def analyze_chunking_quality(result):
    """åˆ†æåˆ†å—è´¨é‡"""
    print("\nğŸ“ˆ åˆ†å—è´¨é‡åˆ†æ:")
    
    # ç« èŠ‚é•¿åº¦åˆ†æ
    chapter_lengths = [chapter.word_count for chapter in result.first_level_chapters]
    avg_chapter_length = sum(chapter_lengths) / len(chapter_lengths)
    max_chapter_length = max(chapter_lengths)
    min_chapter_length = min(chapter_lengths)
    
    print(f"   ç« èŠ‚é•¿åº¦ç»Ÿè®¡:")
    print(f"   - å¹³å‡é•¿åº¦: {avg_chapter_length:.0f} å­—ç¬¦")
    print(f"   - æœ€å¤§é•¿åº¦: {max_chapter_length} å­—ç¬¦")
    print(f"   - æœ€å°é•¿åº¦: {min_chapter_length} å­—ç¬¦")
    
    # æœ€å°å—é•¿åº¦åˆ†æ
    chunk_lengths = [chunk.word_count for chunk in result.minimal_chunks]
    avg_chunk_length = sum(chunk_lengths) / len(chunk_lengths)
    max_chunk_length = max(chunk_lengths)
    min_chunk_length = min(chunk_lengths)
    
    print(f"   æœ€å°å—é•¿åº¦ç»Ÿè®¡:")
    print(f"   - å¹³å‡é•¿åº¦: {avg_chunk_length:.0f} å­—ç¬¦")
    print(f"   - æœ€å¤§é•¿åº¦: {max_chunk_length} å­—ç¬¦")
    print(f"   - æœ€å°é•¿åº¦: {min_chunk_length} å­—ç¬¦")
    
    # æ£€ç´¢å‹å¥½åº¦åˆ†æ
    suitable_chunks = [chunk for chunk in result.minimal_chunks 
                      if 100 <= chunk.word_count <= 500]
    
    print(f"   æ£€ç´¢å‹å¥½åº¦:")
    print(f"   - é€‚åˆæ£€ç´¢çš„å—æ•°: {len(suitable_chunks)}/{len(result.minimal_chunks)}")
    print(f"   - æ¯”ä¾‹: {len(suitable_chunks)/len(result.minimal_chunks)*100:.1f}%")

if __name__ == "__main__":
    result = test_text_chunker()
    if result:
        analyze_chunking_quality(result) 