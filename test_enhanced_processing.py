#!/usr/bin/env python3
"""
æµ‹è¯•å¢å¼ºç‰ˆå¤„ç†æ•ˆæœ
éªŒè¯è·¨é¡µé¢å†…å®¹è¿æ¥ã€å›¾ç‰‡è¡¨æ ¼ç²¾ç¡®å®šä½ã€ç« èŠ‚å…³è”ç­‰åŠŸèƒ½
"""

import json
import os
import sys
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent / "src"))

from src.pdf_processing.enhanced_ai_content_reorganizer import EnhancedAIContentReorganizer
from src.pdf_processing.data_models import PageData, ImageWithContext, TableWithContext

def test_enhanced_processing():
    """æµ‹è¯•å¢å¼ºç‰ˆå¤„ç†"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•å¢å¼ºç‰ˆå¤„ç†...")
    
    # æµ‹è¯•æ•°æ®è·¯å¾„
    test_data_dir = "parser_output/20250714_232720_vvmpc0"
    basic_result_file = os.path.join(test_data_dir, "basic_processing_result.json")
    
    if not os.path.exists(basic_result_file):
        print(f"âŒ æµ‹è¯•æ•°æ®ä¸å­˜åœ¨: {basic_result_file}")
        return
    
    # è¯»å–åŸºç¡€å¤„ç†ç»“æœ
    print("ğŸ“– è¯»å–åŸºç¡€å¤„ç†ç»“æœ...")
    with open(basic_result_file, 'r', encoding='utf-8') as f:
        basic_result = json.load(f)
    
    # é‡å»ºPageDataå¯¹è±¡
    pages = []
    for page_data in basic_result["pages"]:
        # é‡å»ºå›¾ç‰‡å¯¹è±¡
        images = []
        for img_data in page_data["images"]:
            image = ImageWithContext(
                image_path=img_data["image_path"],
                page_number=img_data["page_number"],
                page_context=img_data["page_context"],
                ai_description=img_data.get("ai_description", "å›¾ç‰‡æè¿°"),
                caption=img_data.get("caption"),
                metadata=img_data.get("metadata", {})
            )
            images.append(image)
        
        # é‡å»ºè¡¨æ ¼å¯¹è±¡
        tables = []
        for table_data in page_data["tables"]:
            table = TableWithContext(
                table_path=table_data["table_path"],
                page_number=table_data["page_number"],
                page_context=table_data["page_context"],
                ai_description=table_data.get("ai_description", "è¡¨æ ¼æè¿°"),
                caption=table_data.get("caption"),
                metadata=table_data.get("metadata", {})
            )
            tables.append(table)
        
        # é‡å»ºé¡µé¢å¯¹è±¡
        page = PageData(
            page_number=page_data["page_number"],
            raw_text=page_data["raw_text"],
            cleaned_text=page_data.get("cleaned_text", ""),
            images=images,
            tables=tables
        )
        pages.append(page)
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(pages)} ä¸ªé¡µé¢")
    
    # åˆ›å»ºå¢å¼ºç‰ˆå¤„ç†å™¨
    print("ğŸ”§ åˆ›å»ºå¢å¼ºç‰ˆå¤„ç†å™¨...")
    enhanced_processor = EnhancedAIContentReorganizer()
    
    # æ‰§è¡Œå¢å¼ºç‰ˆå¤„ç†
    print("ğŸš€ æ‰§è¡Œå¢å¼ºç‰ˆå¤„ç†...")
    try:
        enhanced_pages, enhanced_media = enhanced_processor.process_pages_enhanced(
            pages, 
            parallel_processing=True
        )
        
        print(f"âœ… å¢å¼ºç‰ˆå¤„ç†å®Œæˆ!")
        print(f"ğŸ“„ å¤„ç†é¡µé¢æ•°: {len(enhanced_pages)}")
        print(f"ğŸ¯ å¢å¼ºåª’ä½“æ•°: {len(enhanced_media)}")
        
        # åˆ†æå¢å¼ºæ•ˆæœ
        print("\nğŸ“Š å¢å¼ºæ•ˆæœåˆ†æ:")
        
        # 1. åª’ä½“åˆ†æ
        image_count = len([m for m in enhanced_media if m.position_info.media_type == "image"])
        table_count = len([m for m in enhanced_media if m.position_info.media_type == "table"])
        print(f"ğŸ–¼ï¸ å›¾ç‰‡æ•°é‡: {image_count}")
        print(f"ğŸ“‹ è¡¨æ ¼æ•°é‡: {table_count}")
        
        # 2. æ˜¾ç¤ºå¢å¼ºåª’ä½“ä¿¡æ¯
        print("\nğŸ¯ å¢å¼ºåª’ä½“ä¿¡æ¯:")
        for i, media in enumerate(enhanced_media[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"  {i+1}. {media.position_info.media_id}")
            print(f"     ç±»å‹: {media.position_info.media_type}")
            print(f"     å ä½ç¬¦: {media.position_info.unique_placeholder}")
            print(f"     ç« èŠ‚æç¤º: {media.position_info.chapter_hint}")
            print(f"     ä¸Šä¸‹æ–‡å¾—åˆ†: {media.context_score:.2f}")
            print(f"     æè¿°: {media.position_info.ai_description[:50]}...")
            print()
        
        # 3. ä¿å­˜å¢å¼ºç»“æœ
        print("ğŸ’¾ ä¿å­˜å¢å¼ºç»“æœ...")
        output_dir = "enhanced_processing_test_output"
        os.makedirs(output_dir, exist_ok=True)
        
        output_files = enhanced_processor.save_enhanced_processing_result(
            enhanced_pages, 
            enhanced_media, 
            output_dir
        )
        
        print("âœ… ç»“æœå·²ä¿å­˜åˆ°:")
        for name, path in output_files.items():
            print(f"  {name}: {path}")
        
        # 4. æ˜¾ç¤ºå¢å¼ºæ–‡æœ¬æ ·æœ¬
        print("\nğŸ“ å¢å¼ºæ–‡æœ¬æ ·æœ¬:")
        enhanced_full_text = enhanced_processor.generate_enhanced_full_text(enhanced_pages, enhanced_media)
        print(f"æ€»é•¿åº¦: {len(enhanced_full_text)} å­—ç¬¦")
        print("å‰500å­—ç¬¦:")
        print(enhanced_full_text[:500])
        print("...")
        
    except Exception as e:
        print(f"âŒ å¢å¼ºç‰ˆå¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def compare_processing_results():
    """æ¯”è¾ƒåŸå§‹å’Œå¢å¼ºå¤„ç†ç»“æœ"""
    print("\nğŸ” æ¯”è¾ƒåŸå§‹å’Œå¢å¼ºå¤„ç†ç»“æœ...")
    
    # åŸå§‹å®Œæ•´æ–‡æœ¬
    original_full_text_path = "parser_output/20250714_232720_vvmpc0/full_text.txt"
    if os.path.exists(original_full_text_path):
        with open(original_full_text_path, 'r', encoding='utf-8') as f:
            original_text = f.read()
        print(f"ğŸ“„ åŸå§‹æ–‡æœ¬é•¿åº¦: {len(original_text)} å­—ç¬¦")
        print(f"ğŸ“„ åŸå§‹æ–‡æœ¬æ®µè½æ•°: {len(original_text.split('\\n\\n'))}")
    
    # å¢å¼ºç‰ˆå®Œæ•´æ–‡æœ¬
    enhanced_full_text_path = "enhanced_processing_test_output/enhanced_full_text.txt"
    if os.path.exists(enhanced_full_text_path):
        with open(enhanced_full_text_path, 'r', encoding='utf-8') as f:
            enhanced_text = f.read()
        print(f"ğŸ¯ å¢å¼ºæ–‡æœ¬é•¿åº¦: {len(enhanced_text)} å­—ç¬¦")
        print(f"ğŸ¯ å¢å¼ºæ–‡æœ¬æ®µè½æ•°: {len(enhanced_text.split('\\n\\n'))}")
        
        # åˆ†æuniqueå ä½ç¬¦
        import re
        unique_placeholders = re.findall(r'{{([^}]+)}}', enhanced_text)
        print(f"ğŸ¯ uniqueå ä½ç¬¦æ•°é‡: {len(unique_placeholders)}")
        
        if unique_placeholders:
            print("ç¤ºä¾‹å ä½ç¬¦:")
            for placeholder in unique_placeholders[:5]:
                print(f"  {placeholder}")
    
    # å¢å¼ºåª’ä½“ä¿¡æ¯
    enhanced_media_path = "enhanced_processing_test_output/enhanced_media_info.json"
    if os.path.exists(enhanced_media_path):
        with open(enhanced_media_path, 'r', encoding='utf-8') as f:
            media_info = json.load(f)
        print(f"ğŸ¯ å¢å¼ºåª’ä½“é¡¹æ•°: {media_info['total_media']}")
        
        # åˆ†æç« èŠ‚å…³è”
        chapter_hints = {}
        for item in media_info['media_items']:
            hint = item.get('chapter_hint', 'unknown')
            if hint not in chapter_hints:
                chapter_hints[hint] = 0
            chapter_hints[hint] += 1
        
        print("ç« èŠ‚å…³è”åˆ†æ:")
        for hint, count in chapter_hints.items():
            print(f"  {hint}: {count} ä¸ªåª’ä½“é¡¹")

if __name__ == "__main__":
    test_enhanced_processing()
    compare_processing_results() 