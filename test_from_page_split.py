#!/usr/bin/env python3
"""
ä»page_splitç»“æœå¼€å§‹çš„æµ‹è¯•è„šæœ¬
æµ‹è¯•: TOCæå– + AIåˆ†å—
"""

import os
import json
import time
from pathlib import Path

# å¯¼å…¥ç›¸å…³ç»„ä»¶
from src.pdf_processing.toc_extractor import TOCExtractor
from src.pdf_processing.ai_chunker import AIChunker
from src.pdf_processing.text_chunker import TextChunker

def test_from_page_split_result():
    """ä»page_splitç»“æœå¼€å§‹æµ‹è¯•åç»­æµç¨‹"""
    
    # ä½¿ç”¨æœ€æ–°çš„è¾“å‡ºç»“æœ
    result_file = "parser_output/20250715_131307_page_split/page_split_processing_result.json"
    
    if not os.path.exists(result_file):
        print(f"âŒ ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {result_file}")
        return
    
    print("ğŸš€ å¼€å§‹ä»page_splitç»“æœæµ‹è¯•...")
    print("=" * 60)
    
    # è¯»å–page_splitç»“æœ
    with open(result_file, 'r', encoding='utf-8') as f:
        page_split_data = json.load(f)
    
    print(f"ğŸ“„ åŠ è½½å®Œæˆ: {len(page_split_data['pages'])} é¡µ")
    
    # 1. æµ‹è¯•TOCæå–
    print("\nğŸ“– å¼€å§‹TOCæå–æµ‹è¯•...")
    toc_start = time.time()
    
    toc_extractor = TOCExtractor()
    
    # å…ˆæ‹¼æ¥å®Œæ•´æ–‡æœ¬
    full_text = toc_extractor.stitch_full_text(result_file)
    print(f"ğŸ“ å®Œæ•´æ–‡æœ¬é•¿åº¦: {len(full_text)} å­—ç¬¦")
    
    # æå–TOC
    toc_items, reasoning_content = toc_extractor.extract_toc_with_reasoning(full_text)
    
    toc_end = time.time()
    toc_time = toc_end - toc_start
    
    print(f"âœ… TOCæå–å®Œæˆï¼Œè€—æ—¶: {toc_time:.2f} ç§’")
    if toc_items:
        print(f"ğŸ“– è¯†åˆ«ç« èŠ‚æ•°: {len(toc_items)}")
        
        # ä¿å­˜TOCç»“æœ
        output_dir = Path(result_file).parent
        toc_result_dict = {
            "toc": [item.__dict__ for item in toc_items],
            "reasoning_content": reasoning_content
        }
        
        toc_file = output_dir / "toc_test_result.json"
        with open(toc_file, 'w', encoding='utf-8') as f:
            json.dump(toc_result_dict, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ TOCç»“æœå·²ä¿å­˜: {toc_file}")
    else:
        print("âŒ TOCæå–å¤±è´¥")
        return
    
    # 2. æµ‹è¯•AIåˆ†å—
    print("\nğŸ”ª å¼€å§‹AIåˆ†å—æµ‹è¯•...")
    chunk_start = time.time()
    
    # åŸºäºTOCè¿›è¡Œåˆ†å—
    text_chunker = TextChunker()
    
    print(f"ğŸ“ å®Œæ•´æ–‡æœ¬é•¿åº¦: {len(full_text)} å­—ç¬¦")
    
    # ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•è°ƒç”¨
    try:
        chunks_result = text_chunker.chunk_text_with_toc(full_text, toc_result_dict)
        
        if chunks_result:
            print(f"ğŸ“š ç¬¬ä¸€çº§ç« èŠ‚æ•°: {len(chunks_result.first_level_chapters)}")
            print(f"ğŸ”ª æ€»åˆ†å—æ•°: {len(chunks_result.minimal_chunks)}")
            
            # ä¿å­˜åˆ†å—ç»“æœ
            chunks_file = output_dir / "chunks_test_result.json"
            with open(chunks_file, 'w', encoding='utf-8') as f:
                json.dump(chunks_result.to_dict(), f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ åˆ†å—ç»“æœå·²ä¿å­˜: {chunks_file}")
            
            successful_chapters = len(chunks_result.first_level_chapters)
            failed_chapters = 0
            all_chunks = chunks_result.minimal_chunks
        else:
            print("âŒ AIåˆ†å—å¤±è´¥")
            successful_chapters = 0
            failed_chapters = 1
            all_chunks = []
        
    except Exception as e:
        print(f"âŒ AIåˆ†å—å¤±è´¥: {e}")
        successful_chapters = 0
        failed_chapters = 1
        all_chunks = []
    
    chunk_end = time.time()
    chunk_time = chunk_end - chunk_start
    
    print(f"\nâœ… AIåˆ†å—å®Œæˆï¼Œè€—æ—¶: {chunk_time:.2f} ç§’")
    print(f"ğŸ“Š æˆåŠŸç« èŠ‚: {successful_chapters}")
    print(f"âŒ å¤±è´¥ç« èŠ‚: {failed_chapters}")
    print(f"ğŸ”ª æ€»åˆ†å—æ•°: {len(all_chunks)}")
    
    # æ€»ç»“
    total_time = toc_time + chunk_time
    print(f"\nğŸ“Š æµ‹è¯•æ€»ç»“:")
    print(f"â±ï¸  TOCæå–æ—¶é—´: {toc_time:.2f} ç§’")
    print(f"â±ï¸  AIåˆ†å—æ—¶é—´: {chunk_time:.2f} ç§’")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f} ç§’")

if __name__ == "__main__":
    test_from_page_split_result() 