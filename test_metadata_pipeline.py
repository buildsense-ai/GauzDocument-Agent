#!/usr/bin/env python3
"""
æµ‹è¯•Metadata Pipeline
éªŒè¯æ‰€æœ‰æ–°çš„metadataç»„ä»¶èƒ½å¦æ­£å¸¸å·¥ä½œ
"""

import os
import json
import time
from pathlib import Path

# å¯¼å…¥æ–°çš„ç»„ä»¶
from src.pdf_processing.metadata_extractor import MetadataExtractor
from src.pdf_processing.document_summary_generator import DocumentSummaryGenerator
from src.pdf_processing.chapter_summary_generator import ChapterSummaryGenerator
from src.pdf_processing.question_generator import QuestionGenerator
from src.pdf_processing.toc_extractor import TOCExtractor
from src.pdf_processing.text_chunker import TextChunker


def test_metadata_pipeline():
    """æµ‹è¯•å®Œæ•´çš„metadata pipeline"""
    
    print("ğŸš€ å¼€å§‹æµ‹è¯•Metadata Pipeline...")
    print("=" * 60)
    
    # ä½¿ç”¨æœ€æ–°çš„è¾“å‡ºç»“æœ
    page_split_file = "parser_output/20250715_131307_page_split/page_split_processing_result.json"
    toc_file = "parser_output/20250715_131307_page_split/toc_test_result.json"
    chunks_file = "parser_output/20250715_131307_page_split/chunks_test_result.json"
    
    # éªŒè¯è¾“å…¥æ–‡ä»¶å­˜åœ¨
    required_files = [page_split_file, toc_file, chunks_file]
    for file_path in required_files:
        if not os.path.exists(file_path):
            print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("parser_output/metadata_test")
    output_dir.mkdir(exist_ok=True)
    
    # 1. æµ‹è¯•MetadataExtractor
    print("\nğŸ“Š æµ‹è¯•MetadataExtractor...")
    test_metadata_extractor(page_split_file, toc_file, chunks_file, output_dir)
    
    # 2. æµ‹è¯•DocumentSummaryGenerator
    print("\nğŸ“„ æµ‹è¯•DocumentSummaryGenerator...")
    test_document_summary_generator(page_split_file, toc_file, chunks_file, output_dir)
    
    # 3. æµ‹è¯•ChapterSummaryGenerator
    print("\nğŸ“š æµ‹è¯•ChapterSummaryGenerator...")
    test_chapter_summary_generator(page_split_file, toc_file, chunks_file, output_dir)
    
    # 4. æµ‹è¯•QuestionGenerator
    print("\nâ“ æµ‹è¯•QuestionGenerator...")
    test_question_generator(page_split_file, toc_file, chunks_file, output_dir)
    
    print("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")


def test_metadata_extractor(page_split_file, toc_file, chunks_file, output_dir):
    """æµ‹è¯•MetadataExtractor"""
    
    try:
        # åˆå§‹åŒ–extractor
        extractor = MetadataExtractor(project_name="åŒ»çµå¤åº™é¡¹ç›®")
        
        # ä»page_splitç»“æœæå–åŸºç¡€ä¿¡æ¯
        base_info = extractor.extract_from_page_split_result(page_split_file)
        document_id = base_info["document_info"]["document_id"]
        
        print(f"ğŸ“‹ æ–‡æ¡£ID: {document_id}")
        print(f"ğŸ“„ æ€»é¡µæ•°: {base_info['document_info']['total_pages']}")
        print(f"ğŸ–¼ï¸ å›¾ç‰‡æ•°: {len(base_info['image_metadata'])}")
        print(f"ğŸ“Š è¡¨æ ¼æ•°: {len(base_info['table_metadata'])}")
        
        # ä»TOCç»“æœæå–ä¿¡æ¯
        doc_toc, chapter_mapping = extractor.extract_from_toc_result(toc_file, document_id)
        
        print(f"ğŸ“– ç« èŠ‚æ•°: {doc_toc.chapter_count}")
        print(f"ğŸ“‹ æ€»ç›®å½•é¡¹: {doc_toc.total_sections}")
        
        # ä»chunksç»“æœæå–ä¿¡æ¯
        with open(chunks_file, 'r', encoding='utf-8') as f:
            chunks_data = json.load(f)
        
        # æ¨¡æ‹ŸChunkingResultï¼ˆç®€åŒ–ç‰ˆï¼‰
        from src.pdf_processing.text_chunker import ChunkingResult, ChapterContent, MinimalChunk
        
        chapters = []
        chunks = []
        
        for chapter_data in chunks_data.get("first_level_chapters", []):
            chapter = ChapterContent(
                chapter_id=chapter_data["chapter_id"],
                title=chapter_data["title"],
                content=chapter_data["content"],
                start_pos=chapter_data["start_pos"],
                end_pos=chapter_data["end_pos"],
                word_count=chapter_data["word_count"],
                has_images=chapter_data.get("has_images", False),
                has_tables=chapter_data.get("has_tables", False)
            )
            chapters.append(chapter)
        
        for chunk_data in chunks_data.get("minimal_chunks", []):
            chunk = MinimalChunk(
                chunk_id=chunk_data["chunk_id"],
                content=chunk_data["content"],
                chunk_type=chunk_data["chunk_type"],
                belongs_to_chapter=chunk_data["belongs_to_chapter"],
                chapter_title=chunk_data["chapter_title"],
                start_pos=chunk_data["start_pos"],
                end_pos=chunk_data["end_pos"],
                word_count=chunk_data["word_count"]
            )
            chunks.append(chunk)
        
        chunking_result = ChunkingResult(
            first_level_chapters=chapters,
            minimal_chunks=chunks,
            total_chapters=len(chapters),
            total_chunks=len(chunks),
            processing_metadata={}
        )
        
        # å»ºç«‹ç« èŠ‚æ˜ å°„
        extractor.map_chapters_to_content(
            base_info['image_metadata'],
            base_info['table_metadata'],
            chapter_mapping,
            base_info['page_split_data']
        )
        
        # æå–æ–‡æœ¬chunkå…ƒæ•°æ®
        text_chunks = extractor.extract_from_chunking_result(chunking_result, document_id, chapter_mapping)
        
        print(f"ğŸ”¤ æ–‡æœ¬chunks: {len(text_chunks)}")
        
        # ä¿å­˜æå–ç»“æœ
        extractor.save_extracted_metadata(
            str(output_dir / "extractor"),
            document_info=base_info["document_info"],
            doc_toc=doc_toc,
            chapter_mapping=chapter_mapping,
            image_metadata=base_info['image_metadata'],
            table_metadata=base_info['table_metadata'],
            text_chunks=text_chunks
        )
        
        print("âœ… MetadataExtractoræµ‹è¯•å®Œæˆ")
        
        # è¿”å›æ•°æ®ç»™åç»­æµ‹è¯•ä½¿ç”¨
        return {
            "document_id": document_id,
            "document_info": base_info["document_info"],
            "chunking_result": chunking_result,
            "chapter_mapping": chapter_mapping,
            "image_metadata": base_info['image_metadata'],
            "table_metadata": base_info['table_metadata']
        }
        
    except Exception as e:
        print(f"âŒ MetadataExtractoræµ‹è¯•å¤±è´¥: {e}")
        return None


def test_document_summary_generator(page_split_file, toc_file, chunks_file, output_dir):
    """æµ‹è¯•DocumentSummaryGenerator"""
    
    try:
        # å¤ç”¨extractorçš„ç»“æœ
        extractor = MetadataExtractor(project_name="åŒ»çµå¤åº™é¡¹ç›®")
        base_info = extractor.extract_from_page_split_result(page_split_file)
        doc_toc, chapter_mapping = extractor.extract_from_toc_result(toc_file, base_info["document_info"]["document_id"])
        
        # æ¨¡æ‹Ÿchunking_result
        with open(chunks_file, 'r', encoding='utf-8') as f:
            chunks_data = json.load(f)
        
        from src.pdf_processing.text_chunker import ChunkingResult, ChapterContent, MinimalChunk
        
        chapters = []
        chunks = []
        
        for chapter_data in chunks_data.get("first_level_chapters", []):
            chapter = ChapterContent(
                chapter_id=chapter_data["chapter_id"],
                title=chapter_data["title"],
                content=chapter_data["content"],
                start_pos=chapter_data["start_pos"],
                end_pos=chapter_data["end_pos"],
                word_count=chapter_data["word_count"],
                has_images=chapter_data.get("has_images", False),
                has_tables=chapter_data.get("has_tables", False)
            )
            chapters.append(chapter)
        
        for chunk_data in chunks_data.get("minimal_chunks", []):
            chunk = MinimalChunk(
                chunk_id=chunk_data["chunk_id"],
                content=chunk_data["content"],
                chunk_type=chunk_data["chunk_type"],
                belongs_to_chapter=chunk_data["belongs_to_chapter"],
                chapter_title=chunk_data["chapter_title"],
                start_pos=chunk_data["start_pos"],
                end_pos=chunk_data["end_pos"],
                word_count=chunk_data["word_count"]
            )
            chunks.append(chunk)
        
        chunking_result = ChunkingResult(
            first_level_chapters=chapters,
            minimal_chunks=chunks,
            total_chapters=len(chapters),
            total_chunks=len(chunks),
            processing_metadata={}
        )
        
        # åˆå§‹åŒ–ç”Ÿæˆå™¨
        generator = DocumentSummaryGenerator(model="qwen-plus")
        
        # ç”Ÿæˆæ–‡æ¡£æ‘˜è¦
        doc_summary, summary_content = generator.generate_document_summary(
            base_info["document_info"],
            chunking_result,
            {"toc_items": json.loads(doc_toc.toc_json)},
            len(base_info['image_metadata']),
            len(base_info['table_metadata'])
        )
        
        print(f"ğŸ“„ æ–‡æ¡£æ‘˜è¦å·²ç”Ÿæˆ")
        print(f"ğŸ“Š å¤„ç†æ—¶é—´: {doc_summary.processing_time:.2f}ç§’")
        print(f"ğŸ“ æ‘˜è¦é¢„è§ˆ: {summary_content[:100]}...")
        
        # ä¿å­˜æ‘˜è¦
        generator.save_document_summary(doc_summary, summary_content, str(output_dir / "document_summary"))
        
        print("âœ… DocumentSummaryGeneratoræµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ DocumentSummaryGeneratoræµ‹è¯•å¤±è´¥: {e}")


def test_chapter_summary_generator(page_split_file, toc_file, chunks_file, output_dir):
    """æµ‹è¯•ChapterSummaryGenerator"""
    
    try:
        # å¤ç”¨extractorçš„ç»“æœ
        extractor = MetadataExtractor(project_name="åŒ»çµå¤åº™é¡¹ç›®")
        base_info = extractor.extract_from_page_split_result(page_split_file)
        doc_toc, chapter_mapping = extractor.extract_from_toc_result(toc_file, base_info["document_info"]["document_id"])
        
        # æ¨¡æ‹Ÿchunking_result
        with open(chunks_file, 'r', encoding='utf-8') as f:
            chunks_data = json.load(f)
        
        from src.pdf_processing.text_chunker import ChunkingResult, ChapterContent, MinimalChunk
        
        chapters = []
        chunks = []
        
        for chapter_data in chunks_data.get("first_level_chapters", []):
            chapter = ChapterContent(
                chapter_id=chapter_data["chapter_id"],
                title=chapter_data["title"],
                content=chapter_data["content"],
                start_pos=chapter_data["start_pos"],
                end_pos=chapter_data["end_pos"],
                word_count=chapter_data["word_count"],
                has_images=chapter_data.get("has_images", False),
                has_tables=chapter_data.get("has_tables", False)
            )
            chapters.append(chapter)
        
        for chunk_data in chunks_data.get("minimal_chunks", []):
            chunk = MinimalChunk(
                chunk_id=chunk_data["chunk_id"],
                content=chunk_data["content"],
                chunk_type=chunk_data["chunk_type"],
                belongs_to_chapter=chunk_data["belongs_to_chapter"],
                chapter_title=chunk_data["chapter_title"],
                start_pos=chunk_data["start_pos"],
                end_pos=chunk_data["end_pos"],
                word_count=chunk_data["word_count"]
            )
            chunks.append(chunk)
        
        chunking_result = ChunkingResult(
            first_level_chapters=chapters,
            minimal_chunks=chunks,
            total_chapters=len(chapters),
            total_chunks=len(chunks),
            processing_metadata={}
        )
        
        # åˆå§‹åŒ–ç”Ÿæˆå™¨
        generator = ChapterSummaryGenerator(model="qwen-plus")
        
        # å‡†å¤‡å›¾ç‰‡å’Œè¡¨æ ¼å…ƒæ•°æ®
        image_metadata = [{"chapter_id": str(i % 6 + 1)} for i in range(len(base_info['image_metadata']))]
        table_metadata = [{"chapter_id": str(i % 6 + 1)} for i in range(len(base_info['table_metadata']))]
        
        # ç”Ÿæˆç« èŠ‚æ‘˜è¦
        chapter_summaries = generator.generate_chapter_summaries(
            base_info["document_info"]["document_id"],
            chunking_result,
            {"toc_items": json.loads(doc_toc.toc_json)},
            image_metadata,
            table_metadata,
            parallel_processing=True
        )
        
        print(f"ğŸ“š ç« èŠ‚æ‘˜è¦å·²ç”Ÿæˆ: {len(chapter_summaries)}ä¸ª")
        
        # ä¿å­˜æ‘˜è¦
        generator.save_chapter_summaries(chapter_summaries, str(output_dir / "chapter_summaries"))
        
        print("âœ… ChapterSummaryGeneratoræµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ ChapterSummaryGeneratoræµ‹è¯•å¤±è´¥: {e}")


def test_question_generator(page_split_file, toc_file, chunks_file, output_dir):
    """æµ‹è¯•QuestionGenerator"""
    
    try:
        # å¤ç”¨extractorçš„ç»“æœ
        extractor = MetadataExtractor(project_name="åŒ»çµå¤åº™é¡¹ç›®")
        base_info = extractor.extract_from_page_split_result(page_split_file)
        doc_toc, chapter_mapping = extractor.extract_from_toc_result(toc_file, base_info["document_info"]["document_id"])
        
        # æ¨¡æ‹Ÿchunking_result
        with open(chunks_file, 'r', encoding='utf-8') as f:
            chunks_data = json.load(f)
        
        from src.pdf_processing.text_chunker import ChunkingResult, ChapterContent, MinimalChunk
        
        chapters = []
        chunks = []
        
        for chapter_data in chunks_data.get("first_level_chapters", []):
            chapter = ChapterContent(
                chapter_id=chapter_data["chapter_id"],
                title=chapter_data["title"],
                content=chapter_data["content"],
                start_pos=chapter_data["start_pos"],
                end_pos=chapter_data["end_pos"],
                word_count=chapter_data["word_count"],
                has_images=chapter_data.get("has_images", False),
                has_tables=chapter_data.get("has_tables", False)
            )
            chapters.append(chapter)
        
        for chunk_data in chunks_data.get("minimal_chunks", []):
            chunk = MinimalChunk(
                chunk_id=chunk_data["chunk_id"],
                content=chunk_data["content"],
                chunk_type=chunk_data["chunk_type"],
                belongs_to_chapter=chunk_data["belongs_to_chapter"],
                chapter_title=chunk_data["chapter_title"],
                start_pos=chunk_data["start_pos"],
                end_pos=chunk_data["end_pos"],
                word_count=chunk_data["word_count"]
            )
            chunks.append(chunk)
        
        chunking_result = ChunkingResult(
            first_level_chapters=chapters,
            minimal_chunks=chunks,
            total_chapters=len(chapters),
            total_chunks=len(chunks),
            processing_metadata={}
        )
        
        # åˆå§‹åŒ–ç”Ÿæˆå™¨
        generator = QuestionGenerator(model="qwen-turbo")
        
        # ç”Ÿæˆé—®é¢˜ï¼ˆé™åˆ¶æ•°é‡ä»¥èŠ‚çœæ—¶é—´ï¼‰
        questions = generator.generate_questions_from_chunks(
            base_info["document_info"]["document_id"],
            chunking_result,
            chapter_mapping,
            questions_per_chunk=2,
            parallel_processing=True
        )
        
        print(f"â“ æ´¾ç”Ÿé—®é¢˜å·²ç”Ÿæˆ: {len(questions)}ä¸ª")
        
        # ä¿å­˜é—®é¢˜
        generator.save_derived_questions(questions, str(output_dir / "derived_questions"))
        
        print("âœ… QuestionGeneratoræµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ QuestionGeneratoræµ‹è¯•å¤±è´¥: {e}")


if __name__ == "__main__":
    test_metadata_pipeline() 