#!/usr/bin/env python3
"""
Stage 2 Processor: æ™ºèƒ½ä¿®å¤ä¸é‡ç»„å¤„ç†å™¨

å®ç°"ä¿®å¤ä¸é‡ç»„"è€Œé"åˆ›ä½œä¸æ€»ç»“"çš„æ ¸å¿ƒç†å¿µï¼š
- Step 2.1: å±€éƒ¨ä¿®å¤ (Page-Level) - ä¿®æ­£OCRé”™è¯¯å’Œæ ¼å¼é—®é¢˜
- Step 2.2: å…¨å±€ç»“æ„è¯†åˆ« (Document-Level) - TOCæå–å’Œç»“æ„åŒ–
- Step 2.3: å†…å®¹æå–ä¸åˆ‡åˆ† (Chapter-Level) - åŸºäºç»“æ„çš„æ™ºèƒ½åˆ†å—
- Step 2.4: å¤šæ¨¡æ€æè¿°ç”Ÿæˆ (å¹¶è¡Œæ‰§è¡Œ) - å›¾ç‰‡è¡¨æ ¼AIæè¿°

å…³é”®è®¾è®¡åŸåˆ™ï¼š
1. ä¿®å¤è€Œéåˆ›ä½œ - é™ä½AIå¹»è§‰é£é™©
2. ä¿ç•™è¿½æº¯æ€§ - åŸå§‹+æ¸…ç†åŒç‰ˆæœ¬
3. ç»“æ„ä¼˜å…ˆ - å…ˆç†è§£éª¨æ¶å†åˆ†å—
4. å¹¶è¡Œå‹å¥½ - æå‡å¤„ç†æ•ˆç‡
"""

import os
import time
import json
import asyncio
import threading
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple

from .final_schema import FinalMetadataSchema

# å¯¼å…¥ç°æœ‰çš„AIå®¢æˆ·ç«¯
try:
    from ..qwen_client import QwenClient
    QWEN_AVAILABLE = True
    print("âœ… QwenClientå¯ç”¨")
except ImportError as e:
    QWEN_AVAILABLE = False
    print(f"âš ï¸ QwenClientä¸å¯ç”¨: {e}")

try:
    from ..openrouter_client import OpenRouterClient
    OPENROUTER_AVAILABLE = True
    print("âœ… OpenRouterClientå¯ç”¨")
except ImportError as e:
    OPENROUTER_AVAILABLE = False
    print(f"âš ï¸ OpenRouterClientä¸å¯ç”¨: {e}")

# å¯¼å…¥TOCæå–å™¨å’Œåˆ†å—ç»„ä»¶
try:
    from ..pdf_processing.toc_extractor import TOCExtractor
    from ..pdf_processing.ai_chunker import AIChunker
    TOC_EXTRACTOR_AVAILABLE = True
    print("âœ… TOCExtractorå¯ç”¨")
except ImportError as e:
    TOC_EXTRACTOR_AVAILABLE = False
    print(f"âš ï¸ TOCExtractorä¸å¯ç”¨: {e}")


class Stage2IntelligentProcessor:
    """
    Stage2æ™ºèƒ½å¤„ç†å™¨ï¼šä¿®å¤ä¸é‡ç»„
    å®ç°"ä¿®å¤è€Œéåˆ›ä½œ"çš„å¤„ç†é€»è¾‘
    """
    
    def __init__(self):
        """åˆå§‹åŒ–Stage2å¤„ç†å™¨"""
        
        # åˆå§‹åŒ–AIå®¢æˆ·ç«¯
        self.ai_clients = {}
        self._initialize_ai_clients()
        
        # å·¥ä½œç›®å½•
        self.base_output_dir = Path("parser_output_v2")
        
        # ä¼˜åŒ–çš„æ–‡æœ¬æ¸…æ´—prompt - å¹³è¡¡ç‰ˆæœ¬
        self.text_cleaning_prompt = """è¯·ä¿®å¤ä»¥ä¸‹OCRè¯†åˆ«æ–‡æœ¬ä¸­çš„é”™è¯¯ï¼Œä½†ä¸è¦æ”¹å˜åŸæ„æˆ–æ·»åŠ æ–°å†…å®¹ã€‚

è¦æ±‚ï¼š
1. **åªä¿®å¤æ˜æ˜¾çš„OCRé”™è¯¯**ï¼šé”™åˆ«å­—ã€ä¹±ç ã€åˆ†è¯é—®é¢˜
2. **æ•´ç†æ ¼å¼é—®é¢˜**ï¼šåˆ é™¤å¤šä½™ç©ºè¡Œã€ä¿®æ­£ç©ºæ ¼åˆ†å¸ƒã€æ•´ç†æ®µè½é—´è·ã€ä¿®æ­£æ ‡ç‚¹ä½ç½®
3. **ä¿æŒåŸæ–‡ç»“æ„**ï¼šä¸è¦é‡å†™ã€æ€»ç»“æˆ–æ‰©å±•å†…å®¹
4. **ä¿æŒä¸“ä¸šæœ¯è¯­**ï¼šå·¥ç¨‹ã€å»ºç­‘ã€æ³•è§„æœ¯è¯­è¦å‡†ç¡®

åŸæ–‡ï¼š{original_text}

ä¿®å¤åçš„æ–‡æœ¬ï¼š"""

        # å›¾ç‰‡æè¿°prompt - ç»“æ„åŒ–è¾“å‡º
        self.image_description_prompt = """è¯·åˆ†æè¿™å¼ å›¾ç‰‡å¹¶ä»¥JSONæ ¼å¼è¾“å‡ºç»“æ„åŒ–æè¿°ã€‚

å›¾ç‰‡ä¸Šä¸‹æ–‡ï¼š{page_context}

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼ˆç›´æ¥è¿”å›JSONï¼Œä¸è¦ç”¨markdownä»£ç å—åŒ…è£…ï¼‰ï¼š
{{
  "search_summary": "ç®€è¿° - 15å­—ä»¥å†…çš„å…³é”®è¯æè¿°ï¼Œçªå‡ºå›¾ç‰‡ç±»å‹å’Œæ ¸å¿ƒå†…å®¹",
  "detailed_description": "è¯¦ç»†æè¿°å›¾ç‰‡çš„å…·ä½“å†…å®¹ã€å¸ƒå±€ã€æ–‡å­—ç­‰å…ƒç´ ",
  "engineering_details": "å¦‚æœæ˜¯æŠ€æœ¯å›¾çº¸ã€è®¾è®¡å›¾æˆ–å·¥ç¨‹å›¾ï¼Œè¯·æè¿°å…³é”®æŠ€æœ¯ä¿¡æ¯ã€å°ºå¯¸ã€è§„æ ¼ç­‰ä¸“ä¸šç»†èŠ‚ï¼›å¦‚æœä¸æ˜¯æŠ€æœ¯å›¾çº¸ï¼Œåˆ™è¿”å›null"
}}

è¦æ±‚ï¼š
- search_summaryè¦ç²¾ç‚¼ï¼Œé€‚åˆæœç´¢åŒ¹é…
- detailed_descriptionè¦å®Œæ•´å‡†ç¡®
- engineering_detailsä»…é’ˆå¯¹æŠ€æœ¯å›¾çº¸ï¼Œæ™®é€šå›¾ç‰‡è¿”å›null
- è¾“å‡ºå¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œç›´æ¥è¿”å›JSONå¯¹è±¡ï¼Œä¸è¦ç”¨```json```åŒ…è£…"""

        # è¡¨æ ¼æè¿°prompt - ç»“æ„åŒ–è¾“å‡º
        self.table_description_prompt = """è¯·åˆ†æè¿™å¼ è¡¨æ ¼å¹¶ä»¥JSONæ ¼å¼è¾“å‡ºç»“æ„åŒ–æè¿°ã€‚

è¡¨æ ¼ä¸Šä¸‹æ–‡ï¼š{page_context}

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼ˆç›´æ¥è¿”å›JSONï¼Œä¸è¦ç”¨markdownä»£ç å—åŒ…è£…ï¼‰ï¼š
{{
  "search_summary": "ç®€è¿° - 15å­—ä»¥å†…çš„å…³é”®è¯æè¿°ï¼Œçªå‡ºè¡¨æ ¼ç±»å‹å’Œæ ¸å¿ƒå†…å®¹",
  "detailed_description": "è¯¦ç»†æè¿°è¡¨æ ¼çš„å…·ä½“å†…å®¹ã€å¸ƒå±€ã€æ–‡å­—ç­‰å…ƒç´ ",
  "engineering_details": "å¦‚æœæ˜¯æŠ€æœ¯å›¾è¡¨ã€è®¾è®¡å›¾æˆ–å·¥ç¨‹å›¾ï¼Œè¯·æè¿°å…³é”®æŠ€æœ¯ä¿¡æ¯ã€å°ºå¯¸ã€è§„æ ¼ç­‰ä¸“ä¸šç»†èŠ‚ï¼›å¦‚æœä¸æ˜¯æŠ€æœ¯å›¾è¡¨ï¼Œåˆ™è¿”å›null"
}}

è¦æ±‚ï¼š
- search_summaryè¦ç²¾ç‚¼ï¼Œé€‚åˆæœç´¢åŒ¹é…
- detailed_descriptionè¦å®Œæ•´å‡†ç¡®
- engineering_detailsä»…é’ˆå¯¹æŠ€æœ¯å›¾è¡¨ï¼Œæ™®é€šè¡¨æ ¼è¿”å›null
- è¾“å‡ºå¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œç›´æ¥è¿”å›JSONå¯¹è±¡ï¼Œä¸è¦ç”¨```json```åŒ…è£…"""

        # å¤„ç†ç»Ÿè®¡
        self.stats = {
            "pages_repaired": 0,
            "images_described": 0,
            "tables_described": 0,
            "toc_extracted": False
        }
        
        print("ğŸ”§ Stage2å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.base_output_dir}")
        print(f"ğŸ¤– å¯ç”¨AIå®¢æˆ·ç«¯: {list(self.ai_clients.keys())}")

    def _initialize_ai_clients(self):
        """åˆå§‹åŒ–AIå®¢æˆ·ç«¯ - ä½¿ç”¨ç°æœ‰çš„QwenClientå’ŒOpenRouterClient"""
        print("ğŸ”§ åˆå§‹åŒ–AIå®¢æˆ·ç«¯...")
        
        # åˆå§‹åŒ–Qwenå®¢æˆ·ç«¯ï¼ˆç”¨äºæ–‡æœ¬æ¸…æ´—ï¼‰
        if QWEN_AVAILABLE:
            try:
                self.ai_clients['qwen'] = QwenClient(
                    model="qwen-turbo-latest",
                    max_tokens=2000,
                    temperature=0.1,
                    max_retries=3,
                    enable_batch_mode=True
                )
                print("âœ… Qwenå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼ˆç”¨äºæ–‡æœ¬æ¸…æ´—ï¼‰")
            except Exception as e:
                print(f"âš ï¸ Qwenå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # åˆå§‹åŒ–OpenRouterå®¢æˆ·ç«¯ï¼ˆç”¨äºå¤šæ¨¡æ€å¤„ç†ï¼‰
        if OPENROUTER_AVAILABLE:
            try:
                self.ai_clients['openrouter'] = OpenRouterClient()
                print("âœ… OpenRouterå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼ˆç”¨äºå¤šæ¨¡æ€å¤„ç†ï¼‰")
            except Exception as e:
                print(f"âš ï¸ OpenRouterå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        
        if not self.ai_clients:
            print("âŒ è­¦å‘Š: æ²¡æœ‰å¯ç”¨çš„AIå®¢æˆ·ç«¯ï¼ŒStage2åŠŸèƒ½å°†å—é™")
            print("ğŸ’¡ è¯·æ£€æŸ¥ç¯å¢ƒå˜é‡ QWEN_API_KEY å’Œ OPENROUTER_API_KEY")

    def _initialize_prompts(self):
        """åˆå§‹åŒ–å„ç§å¤„ç†æç¤ºè¯"""
        print("ğŸ“ åˆå§‹åŒ–å¤„ç†æç¤ºè¯...")
        
        # æ–‡æœ¬æ¸…æ´—prompt - å¹³è¡¡ç‰ˆæœ¬
        self.text_cleaning_prompt = """è¯·ä¿®å¤ä»¥ä¸‹OCRè¯†åˆ«æ–‡æœ¬ä¸­çš„é”™è¯¯ï¼Œä½†ä¸è¦æ”¹å˜åŸæ„æˆ–æ·»åŠ æ–°å†…å®¹ã€‚

è¦æ±‚ï¼š
1. **åªä¿®å¤æ˜æ˜¾çš„OCRé”™è¯¯**ï¼šé”™åˆ«å­—ã€ä¹±ç ã€åˆ†è¯é—®é¢˜
2. **æ•´ç†æ ¼å¼é—®é¢˜**ï¼šåˆ é™¤å¤šä½™ç©ºè¡Œã€ä¿®æ­£ç©ºæ ¼åˆ†å¸ƒã€æ•´ç†æ®µè½é—´è·ã€ä¿®æ­£æ ‡ç‚¹ä½ç½®
3. **ä¿æŒåŸæ–‡ç»“æ„**ï¼šä¸è¦é‡å†™ã€æ€»ç»“æˆ–æ‰©å±•å†…å®¹
4. **ä¿æŒä¸“ä¸šæœ¯è¯­**ï¼šå·¥ç¨‹ã€å»ºç­‘ã€æ³•è§„æœ¯è¯­è¦å‡†ç¡®

åŸæ–‡ï¼š{original_text}

ä¿®å¤åçš„æ–‡æœ¬ï¼š"""

        # å›¾ç‰‡æè¿°prompt - ç»“æ„åŒ–è¾“å‡º
        self.image_description_prompt = """è¯·åˆ†æè¿™å¼ å›¾ç‰‡å¹¶ä»¥JSONæ ¼å¼è¾“å‡ºç»“æ„åŒ–æè¿°ã€‚

å›¾ç‰‡ä¸Šä¸‹æ–‡ï¼š{page_context}

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼ˆç›´æ¥è¿”å›JSONï¼Œä¸è¦ç”¨markdownä»£ç å—åŒ…è£…ï¼‰ï¼š
{{
  "search_summary": "ç®€è¿° - 15å­—ä»¥å†…çš„å…³é”®è¯æè¿°ï¼Œçªå‡ºå›¾ç‰‡ç±»å‹å’Œæ ¸å¿ƒå†…å®¹",
  "detailed_description": "è¯¦ç»†æè¿°å›¾ç‰‡çš„å…·ä½“å†…å®¹ã€å¸ƒå±€ã€æ–‡å­—ç­‰å…ƒç´ ",
  "engineering_details": "å¦‚æœæ˜¯æŠ€æœ¯å›¾çº¸ã€è®¾è®¡å›¾æˆ–å·¥ç¨‹å›¾ï¼Œè¯·æè¿°å…³é”®æŠ€æœ¯ä¿¡æ¯ã€å°ºå¯¸ã€è§„æ ¼ç­‰ä¸“ä¸šç»†èŠ‚ï¼›å¦‚æœä¸æ˜¯æŠ€æœ¯å›¾çº¸ï¼Œåˆ™è¿”å›null"
}}

è¦æ±‚ï¼š
- search_summaryè¦ç²¾ç‚¼ï¼Œé€‚åˆæœç´¢åŒ¹é…
- detailed_descriptionè¦å®Œæ•´å‡†ç¡®
- engineering_detailsä»…é’ˆå¯¹æŠ€æœ¯å›¾çº¸ï¼Œæ™®é€šå›¾ç‰‡è¿”å›null
- è¾“å‡ºå¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œç›´æ¥è¿”å›JSONå¯¹è±¡ï¼Œä¸è¦ç”¨```json```åŒ…è£…"""

        # è¡¨æ ¼æè¿°prompt - ç»“æ„åŒ–è¾“å‡º
        self.table_description_prompt = """è¯·åˆ†æè¿™å¼ è¡¨æ ¼å¹¶ä»¥JSONæ ¼å¼è¾“å‡ºç»“æ„åŒ–æè¿°ã€‚

è¡¨æ ¼ä¸Šä¸‹æ–‡ï¼š{page_context}

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼ˆç›´æ¥è¿”å›JSONï¼Œä¸è¦ç”¨markdownä»£ç å—åŒ…è£…ï¼‰ï¼š
{{
  "search_summary": "ç®€è¿° - 15å­—ä»¥å†…çš„å…³é”®è¯æè¿°ï¼Œçªå‡ºè¡¨æ ¼ç±»å‹å’Œæ ¸å¿ƒå†…å®¹",
  "detailed_description": "è¯¦ç»†æè¿°è¡¨æ ¼çš„å…·ä½“å†…å®¹ã€å¸ƒå±€ã€æ–‡å­—ç­‰å…ƒç´ ",
  "engineering_details": "å¦‚æœæ˜¯æŠ€æœ¯å›¾è¡¨ã€è®¾è®¡å›¾æˆ–å·¥ç¨‹å›¾ï¼Œè¯·æè¿°å…³é”®æŠ€æœ¯ä¿¡æ¯ã€å°ºå¯¸ã€è§„æ ¼ç­‰ä¸“ä¸šç»†èŠ‚ï¼›å¦‚æœä¸æ˜¯æŠ€æœ¯å›¾è¡¨ï¼Œåˆ™è¿”å›null"
}}

è¦æ±‚ï¼š
- search_summaryè¦ç²¾ç‚¼ï¼Œé€‚åˆæœç´¢åŒ¹é…
- detailed_descriptionè¦å®Œæ•´å‡†ç¡®
- engineering_detailsä»…é’ˆå¯¹æŠ€æœ¯å›¾è¡¨ï¼Œæ™®é€šè¡¨æ ¼è¿”å›null
- è¾“å‡ºå¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œç›´æ¥è¿”å›JSONå¯¹è±¡ï¼Œä¸è¦ç”¨```json```åŒ…è£…"""

    def process(self, input_path: str, output_dir: Optional[str] = None) -> str:
        """
        æ‰§è¡ŒStage2æ™ºèƒ½å¤„ç†
        æ³¨æ„ï¼šç›´æ¥æ›´æ–°input_pathçš„metadataæ–‡ä»¶ï¼Œä¸åˆ›å»ºæ–°æ–‡ä»¶
        """
        print(f"ğŸš€ å¼€å§‹Stage2æ™ºèƒ½å¤„ç†...")
        print(f"ğŸ“‚ è¾“å…¥è·¯å¾„: {input_path}")
        
        start_time = time.time()
        
        try:
            # 1. åŠ è½½Stage1æ•°æ®
            print("ğŸ“– åŠ è½½Stage1å¤„ç†ç»“æœ...")
            final_schema = self._load_stage1_data(input_path)
            
            # 2. Step 2.1: é¡µé¢æ–‡æœ¬ä¿®å¤
            print("ğŸ”§ Step 2.1: æ‰§è¡Œé¡µé¢æ–‡æœ¬ä¿®å¤...")
            self._process_step_2_1_text_repair(final_schema)
            
            # 3. Step 2.2: å…¨å±€ç»“æ„è¯†åˆ« (TOCæå–å’Œç»“æ„åŒ–)
            print("ğŸ”— Step 2.2: æ‰§è¡Œå…¨å±€ç»“æ„è¯†åˆ«...")
            self._process_step_2_2_toc_extraction(final_schema)
            
            # 4. Step 2.3: å†…å®¹æå–ä¸åˆ‡åˆ† (Chapter-Level)
            print("ğŸ§  Step 2.3: æ‰§è¡Œå†…å®¹æå–ä¸åˆ‡åˆ†...")
            self._process_step_2_3_content_chunking(final_schema)
            
            # 5. Step 2.4: å¤šæ¨¡æ€æè¿°ç”Ÿæˆ (æš‚è·³è¿‡Step 2.2å’Œ2.3)
            print("ğŸ–¼ï¸  Step 2.4: æ‰§è¡Œå¤šæ¨¡æ€æè¿°ç”Ÿæˆ...")
            self._process_step_2_4_multimodal(final_schema)
            
            # 6. æ›´æ–°å¤„ç†çŠ¶æ€
            final_schema.processing_status.current_stage = "stage2_completed"
            final_schema.processing_status.completion_percentage = 60
            final_schema.processing_status.last_updated = datetime.now()
            
            # 7. ä¿å­˜æ›´æ–°åçš„æ•°æ® - ç›´æ¥è¦†ç›–åŸæ–‡ä»¶
            output_path = self._save_updated_metadata(final_schema, input_path)
            
            # 8. ç»Ÿè®¡ä¸æŠ¥å‘Š
            processing_time = time.time() - start_time
            self._print_processing_report(final_schema, processing_time)
            
            print(f"âœ… Stage2å¤„ç†å®Œæˆ! ç”¨æ—¶: {processing_time:.2f}ç§’")
            print(f"ğŸ“„ æ›´æ–°æ–‡ä»¶: {output_path}")
            
            return output_path
            
        except Exception as e:
            print(f"âŒ Stage2å¤„ç†å¤±è´¥: {str(e)}")
            raise
    
    def _save_updated_metadata(self, final_schema: FinalMetadataSchema, original_path: str) -> str:
        """
        ä¿å­˜æ›´æ–°åçš„metadata - ç›´æ¥æ›´æ–°åŸæ–‡ä»¶
        """
        # ç¡®å®šmetadataæ–‡ä»¶è·¯å¾„
        original_path_obj = Path(original_path)
        
        if original_path_obj.is_file() and original_path_obj.name.endswith('.json'):
            # è¾“å…¥æ˜¯jsonæ–‡ä»¶ï¼Œç›´æ¥æ›´æ–°
            metadata_path = original_path_obj
        else:
            # è¾“å…¥æ˜¯ç›®å½•ï¼ŒæŸ¥æ‰¾å…¶ä¸­çš„final_metadata.json
            metadata_path = original_path_obj / "final_metadata.json"
        
        if not metadata_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°metadataæ–‡ä»¶: {metadata_path}")
        
        print(f"ğŸ’¾ æ›´æ–°metadataæ–‡ä»¶: {metadata_path}")
        
        # ä¿å­˜æ›´æ–°åçš„æ•°æ®
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(final_schema.to_dict(), f, ensure_ascii=False, indent=2)
        
        return str(metadata_path)
    
    def _process_step_2_1_text_repair(self, final_schema: FinalMetadataSchema):
        """Step 2.1: é¡µé¢æ–‡æœ¬ä¿®å¤ - å¹¶è¡Œå¤„ç†"""
        print("ğŸ“ å¼€å§‹é¡µé¢çº§æ–‡æœ¬ä¿®å¤...")
        
        page_texts = final_schema.document_summary.page_texts if final_schema.document_summary else None
        if not page_texts:
            print("âš ï¸ æ²¡æœ‰é¡µé¢æ–‡æœ¬æ•°æ®ï¼Œè·³è¿‡å±€éƒ¨ä¿®å¤")
            return
        
        # å¦‚æœæœ‰Qwenå®¢æˆ·ç«¯ï¼Œè¿›è¡Œæ–‡æœ¬ä¿®å¤ - å¹¶è¡Œå¤„ç†
        if self.ai_clients.get('qwen'):
            print(f"ğŸš€ å¹¶è¡Œä¿®å¤{len(page_texts)}é¡µæ–‡æœ¬...")
            cleaned_texts = self._repair_pages_parallel(page_texts)
        else:
            print("âš ï¸ æ— å¯ç”¨Qwenå®¢æˆ·ç«¯ï¼Œè·³è¿‡æ–‡æœ¬ä¿®å¤")
            cleaned_texts = page_texts.copy()
        
        # æ›´æ–°schema
        if final_schema.document_summary and final_schema.document_summary.cleaned_page_texts is None:
            final_schema.document_summary.cleaned_page_texts = {}
        if final_schema.document_summary and final_schema.document_summary.cleaned_page_texts is not None:
            final_schema.document_summary.cleaned_page_texts.update(cleaned_texts)
        print(f"âœ… é¡µé¢ä¿®å¤å®Œæˆï¼Œå¤„ç†äº†{len(cleaned_texts)}é¡µ")
    
    def _repair_pages_parallel(self, page_texts: dict, max_workers: int = 5) -> dict:
        """å¹¶è¡Œä¿®å¤é¡µé¢æ–‡æœ¬"""
        cleaned_texts = {}
        
        def repair_single_page(page_item):
            page_num, raw_text = page_item
            try:
                print(f"ğŸ”§ ä¿®å¤ç¬¬{page_num}é¡µ...")
                cleaned_text = self._clean_page_text(raw_text)
                return page_num, cleaned_text, True
            except Exception as e:
                print(f"âš ï¸ ç¬¬{page_num}é¡µä¿®å¤å¤±è´¥: {e}")
                return page_num, raw_text, False
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_page = {
                executor.submit(repair_single_page, item): item[0] 
                for item in page_texts.items()
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_page):
                page_num, cleaned_text, success = future.result()
                cleaned_texts[page_num] = cleaned_text
                if success:
                    self.stats["pages_repaired"] += 1
        
        print(f"âœ… å¹¶è¡Œä¿®å¤å®Œæˆ: {self.stats['pages_repaired']}/{len(page_texts)}é¡µæˆåŠŸ")
        return cleaned_texts
    
    def _process_step_2_2_toc_extraction(self, final_schema: FinalMetadataSchema):
        """
        Step 2.2: å…¨å±€ç»“æ„è¯†åˆ« (Document-Level)
        åŸºäºä¿®å¤åçš„æ–‡æœ¬æå–TOCç»“æ„
        """
        print("ğŸ—ºï¸ å¼€å§‹å…¨å±€ç»“æ„è¯†åˆ«...")
        
        if not TOC_EXTRACTOR_AVAILABLE:
            print("âš ï¸ TOCæå–å™¨ä¸å¯ç”¨ï¼Œè·³è¿‡ç»“æ„è¯†åˆ«")
            return
        
        # 1. æ‹¼æ¥å®Œæ•´çš„æ¸…ç†åæ–‡æœ¬
        full_cleaned_text = self._stitch_cleaned_full_text(final_schema)
        if not full_cleaned_text:
            print("âš ï¸ æ— æ³•è·å–æ¸…ç†åçš„å®Œæ•´æ–‡æœ¬ï¼Œè·³è¿‡TOCæå–")
            return
        
        # 2. ä½¿ç”¨TOCæå–å™¨
        try:
            toc_extractor = TOCExtractor(model="qwen-plus")
            toc_items, reasoning_content = toc_extractor.extract_toc_with_reasoning(full_cleaned_text)
            
            if toc_items:
                # 3. ç›´æ¥å­˜å‚¨åˆ°document_summary.toc
                # è½¬æ¢TOCItemä¸ºå­—å…¸æ ¼å¼
                toc_dict = []
                for item in toc_items:
                    toc_dict.append({
                        "title": item.title,
                        "level": item.level,
                        "start_text": item.start_text,
                        "chapter_id": f"chapter_{item.id}",
                        "id": item.id,
                        "parent_id": item.parent_id
                    })
                
                # ç›´æ¥å­˜å‚¨TOCåˆ°document_summary.toc
                if final_schema.document_summary:
                    final_schema.document_summary.toc = toc_dict
                self.stats["toc_extracted"] = True
                
                print(f"âœ… TOCæå–æˆåŠŸï¼Œå…±è¯†åˆ« {len(toc_items)} ä¸ªç« èŠ‚")
                
                # æ˜¾ç¤ºæå–ç»“æœ
                for item in toc_items:
                    indent = "  " * (item.level - 1)
                    print(f"{indent}{item.level}. {item.title}")
                    
            else:
                print("âš ï¸ TOCæå–å¤±è´¥ï¼Œæœªè¯†åˆ«åˆ°ç« èŠ‚ç»“æ„")
                
        except Exception as e:
            print(f"âŒ TOCæå–è¿‡ç¨‹å¤±è´¥: {e}")
    
    def _process_step_2_3_content_chunking(self, final_schema: FinalMetadataSchema):
        """
        Step 2.3: å†…å®¹æå–ä¸åˆ‡åˆ† (Chapter-Level)
        """
        print("\n" + "="*50)
        print("ğŸ”€ å¼€å§‹ Step 2.3: å†…å®¹æå–ä¸åˆ‡åˆ†")
        print("="*50)
        
        try:
            # è·å–TOCæ•°æ®
            if not final_schema.document_summary or not final_schema.document_summary.toc:
                print("âš ï¸ æ²¡æœ‰TOCæ•°æ®ï¼Œè·³è¿‡ç« èŠ‚åˆ‡åˆ†")
                return
            
            toc_data = final_schema.document_summary.toc
            
            # 1. ç”Ÿæˆå®Œæ•´çš„æ¸…ç†æ–‡æœ¬
            cleaned_full_text = self._generate_cleaned_full_text(final_schema)
            if not cleaned_full_text:
                print("âš ï¸ æ— æ³•è·å–æ¸…ç†åçš„å®Œæ•´æ–‡æœ¬ï¼Œè·³è¿‡ç« èŠ‚åˆ‡åˆ†")
                return
                
            print(f"ğŸ“„ ç”Ÿæˆå®Œæ•´æ–‡æœ¬: {len(cleaned_full_text)} å­—ç¬¦")
            
            # 2. åŸºäºTOCåˆ‡åˆ†ä¸»è¦ç« èŠ‚
            chapters = self._cut_chapters_by_toc_simple(cleaned_full_text, toc_data)
            
            if not chapters:
                print("âš ï¸ ç« èŠ‚åˆ‡åˆ†å¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆç« èŠ‚")
                return
                
            print(f"âœ… ç« èŠ‚åˆ‡åˆ†å®Œæˆ: {len(chapters)}ä¸ªç« èŠ‚")
            
            # 3. åˆ›å»ºChapterSummaryå¯¹è±¡ï¼ˆæš‚æ—¶åªæœ‰åŸå§‹å†…å®¹ï¼‰
            chapter_summaries = self._create_chapter_summaries(chapters, final_schema.document_id)
            final_schema.chapter_summaries = chapter_summaries
            
            # 4. ä½¿ç”¨ AI Chunker è¿›è¡Œç»†ç²’åº¦åˆ†å—
            if TOC_EXTRACTOR_AVAILABLE:  # AI chunker å’Œ TOC extractor åœ¨åŒä¸€ä¸ªæ¨¡å—ä¸­
                print("ğŸ¤– å¼€å§‹ä½¿ç”¨ AI Chunker è¿›è¡Œç»†ç²’åº¦åˆ†å—...")
                text_chunks = self._perform_ai_chunking(chapters, final_schema.document_id)
                final_schema.text_chunks = text_chunks
                print(f"ğŸ”¤ AI Chunking å®Œæˆ: {len(text_chunks)}ä¸ªç»†ç²’åº¦åˆ†å—")
            else:
                print("âš ï¸ AI Chunker ä¸å¯ç”¨ï¼Œåˆ›å»ºç®€åŒ–æ–‡æœ¬å—")
                text_chunks = self._create_text_chunks_for_embedding(chapters, final_schema.document_id)
                final_schema.text_chunks = text_chunks
            
            # 5. æ›´æ–° table å’Œ image çš„ chapter_id ä¿¡æ¯
            print("ğŸ”— æ›´æ–°å›¾ç‰‡å’Œè¡¨æ ¼çš„ç« èŠ‚å…³è”...")
            self._update_multimodal_chapter_mapping(final_schema, chapters)
            
            # 6. ç”Ÿæˆç« èŠ‚æ€»ç»“
            print("ğŸ“ ç”Ÿæˆç« èŠ‚æ€»ç»“...")
            self._generate_chapter_summaries(final_schema)
            
            print(f"ğŸ“Š æœ€ç»ˆç»“æœ:")
            print(f"   ğŸ“– ç« èŠ‚æ‘˜è¦: {len(chapter_summaries)}ä¸ª")
            print(f"   ğŸ”¤ æ–‡æœ¬å—: {len(text_chunks)}ä¸ª")
            
            # æ˜¾ç¤ºåˆ‡åˆ†ç»“æœ
            for i, chapter in enumerate(chapters):
                print(f"   ğŸ“– ç« èŠ‚{i+1}: {chapter['title']} ({chapter['word_count']}å­—ç¬¦)")
            
        except Exception as e:
            print(f"âŒ ç« èŠ‚åˆ‡åˆ†è¿‡ç¨‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
    def _generate_cleaned_full_text(self, final_schema: FinalMetadataSchema) -> str:
        """
        ä»cleaned_page_textsç”Ÿæˆå®Œæ•´çš„æ¸…ç†æ–‡æœ¬
        """
        if not final_schema.document_summary or not final_schema.document_summary.cleaned_page_texts:
            print("âš ï¸ æ²¡æœ‰cleaned_page_textsæ•°æ®")
            return ""
            
        cleaned_page_texts = final_schema.document_summary.cleaned_page_texts
        
        # æŒ‰é¡µç æ’åºå¹¶è¿æ¥
        page_numbers = sorted([int(k) for k in cleaned_page_texts.keys() if k.isdigit()])
        pages = []
        
        for page_num in page_numbers:
            page_text = cleaned_page_texts.get(str(page_num), "")
            if page_text and page_text.strip():
                pages.append(page_text.strip())
        
        # ç”¨åŒæ¢è¡Œè¿æ¥å„é¡µ
        cleaned_full_text = "\n\n".join(pages)
        print(f"ğŸ“– ç¼åˆå®Œæˆ: {len(pages)}é¡µ â†’ {len(cleaned_full_text)}å­—ç¬¦")
        
        return cleaned_full_text
        
    def _cut_chapters_by_toc_simple(self, full_text: str, toc_data: List[Dict]) -> List[Dict]:
        """
        åŸºäºTOCç®€å•åˆ‡åˆ†ç« èŠ‚ - åªå¤„ç†level=1çš„ä¸»è¦ç« èŠ‚
        """
        import re
        
        chapters = []
        
        # åªå¤„ç†ç¬¬ä¸€çº§ç« èŠ‚
        first_level_toc = [item for item in toc_data if item.get('level') == 1]
        
        # è¿‡æ»¤å‡ºä¸»è¦ç« èŠ‚ï¼ˆåŒ…å«"ç¯‡ç« "å…³é”®è¯çš„ï¼‰
        major_chapters = []
        for item in first_level_toc:
            title = item.get('title', '')
            # åªä¿ç•™åŒ…å«"ç¯‡ç« "çš„çœŸæ­£ä¸»ç« èŠ‚
            if 'ç¯‡ç« ' in title:
                major_chapters.append(item)
        
        print(f"ğŸ” æ‰¾åˆ°ä¸»è¦ç« èŠ‚: {[item.get('title') for item in major_chapters]}")
        
        for i, toc_item in enumerate(major_chapters):
            chapter_id = toc_item.get('chapter_id', f"chapter_{i+1}")
            title = toc_item.get('title', f"ç¬¬{i+1}ç« ")
            start_text = toc_item.get('start_text', '')
            
            # æŸ¥æ‰¾ç« èŠ‚å¼€å§‹ä½ç½®
            start_pos = self._find_chapter_position(full_text, start_text, title)
            if start_pos == -1:
                print(f"âš ï¸ æ— æ³•æ‰¾åˆ°ç« èŠ‚ '{title}' çš„å¼€å§‹ä½ç½®")
                continue
                
            # ç¡®å®šç« èŠ‚ç»“æŸä½ç½®
            if i + 1 < len(major_chapters):
                next_start_text = major_chapters[i + 1].get('start_text', '')
                next_title = major_chapters[i + 1].get('title', '')
                end_pos = self._find_chapter_position(full_text, next_start_text, next_title)
                if end_pos == -1:
                    end_pos = len(full_text)
            else:
                end_pos = len(full_text)
            
            # æå–ç« èŠ‚å†…å®¹
            content = full_text[start_pos:end_pos].strip()
            
            chapter = {
                "chapter_id": chapter_id,
                "title": title,
                "content": content,
                "start_pos": start_pos,
                "end_pos": end_pos,
                "word_count": len(content),
                "order": i + 1
            }
            
            chapters.append(chapter)
            print(f"âœ… ç« èŠ‚åˆ‡åˆ†: {title} ({len(content)} å­—ç¬¦)")
        
        return chapters
    
    def _find_chapter_position(self, full_text: str, start_text: str, title: str = "") -> int:
        """
        åœ¨å…¨æ–‡ä¸­æŸ¥æ‰¾ç« èŠ‚ä½ç½® - å¿…é¡»ä½¿ç”¨start_texté¿å…åŒ¹é…åˆ°ç›®å½•
        """
        import re
        
        # 1. å¿…é¡»ä¼˜å…ˆä½¿ç”¨start_textï¼Œå› ä¸ºtitleä¼šåœ¨ç›®å½•ä¸­é‡å¤å‡ºç°
        if start_text:
            # å»é™¤å¤šä½™ç©ºæ ¼ï¼Œä½†ä¿æŒåŸºæœ¬ç»“æ„
            cleaned_start = re.sub(r'\s+', ' ', start_text.strip())
            
            # ç²¾ç¡®åŒ¹é…
            pos = full_text.find(cleaned_start)
            if pos != -1:
                print(f"âœ… ç²¾ç¡®åŒ¹é…start_text: {cleaned_start[:30]}...")
                return pos
            
            # æ¨¡ç³ŠåŒ¹é…ï¼šå…è®¸ç©ºæ ¼å’Œæ¢è¡Œçš„å˜åŒ–
            if len(cleaned_start) > 10:
                # å°†ç©ºæ ¼æ›¿æ¢ä¸ºæ­£åˆ™æ¨¡å¼ï¼Œå…è®¸å¤šä¸ªç©ºæ ¼/æ¢è¡Œ
                pattern = re.escape(cleaned_start).replace(r'\ ', r'\s+')
                match = re.search(pattern, full_text, re.IGNORECASE)
                if match:
                    print(f"âœ… æ¨¡ç³ŠåŒ¹é…start_text: {pattern[:30]}...")
                    return match.start()
            
            # å¦‚æœstart_textå¤ªé•¿ï¼Œå°è¯•ç”¨å‰åŠéƒ¨åˆ†åŒ¹é…
            if len(cleaned_start) > 20:
                first_half = cleaned_start[:len(cleaned_start)//2]
                pattern = re.escape(first_half).replace(r'\ ', r'\s+')
                matches = list(re.finditer(pattern, full_text, re.IGNORECASE))
                if matches:
                    # å¦‚æœæœ‰å¤šä¸ªåŒ¹é…ï¼Œé€‰æ‹©ä¸åœ¨æ–‡æ¡£å¼€å¤´çš„ï¼ˆé¿å…ç›®å½•ï¼‰
                    for match in matches:
                        if match.start() > 1000:  # å‡è®¾ç›®å½•åœ¨å‰1000å­—ç¬¦å†…
                            print(f"âœ… éƒ¨åˆ†åŒ¹é…start_text (è·³è¿‡ç›®å½•): {first_half[:20]}...")
                            return match.start()
                    # å¦‚æœéƒ½åœ¨å¼€å¤´ï¼Œå–æœ€åä¸€ä¸ª
                    print(f"âœ… éƒ¨åˆ†åŒ¹é…start_text (æœ€åä¸€ä¸ª): {first_half[:20]}...")
                    return matches[-1].start()
        
        # 2. å›é€€åˆ°æ ‡é¢˜åŒ¹é…ï¼ˆä½†è¦é¿å…ç›®å½•ï¼‰
        if title:
            clean_title = title.strip()
            matches = []
            start = 0
            while True:
                pos = full_text.find(clean_title, start)
                if pos == -1:
                    break
                matches.append(pos)
                start = pos + 1
            
            if matches:
                # å¦‚æœæœ‰å¤šä¸ªåŒ¹é…ï¼Œé€‰æ‹©ä¸åœ¨æ–‡æ¡£å¼€å¤´çš„ï¼ˆé¿å…ç›®å½•ï¼‰
                for pos in matches:
                    if pos > 1000:  # å‡è®¾ç›®å½•åœ¨å‰1000å­—ç¬¦å†…
                        print(f"âœ… æ ‡é¢˜åŒ¹é… (è·³è¿‡ç›®å½•): {clean_title}")
                        return pos
                # å¦‚æœéƒ½åœ¨å¼€å¤´ï¼Œå–æœ€åä¸€ä¸ª
                print(f"âœ… æ ‡é¢˜åŒ¹é… (æœ€åä¸€ä¸ª): {clean_title}")
                return matches[-1]
        
        print(f"âŒ æ— æ³•å®šä½ç« èŠ‚: title='{title}', start_text='{start_text[:50]}...'")
        return -1
        
    def _create_text_chunks_for_embedding(self, chapters: List[Dict], document_id: str) -> List:
        """
        åŸºäºç« èŠ‚åˆ›å»ºè½»é‡çº§TextChunkå¯¹è±¡ç”¨äºembedding
        """
        from .final_schema import TextChunk
        
        text_chunks = []
        
        for i, chapter in enumerate(chapters):
            # åˆ›å»ºç« èŠ‚æ‘˜è¦è€Œä¸æ˜¯å®Œæ•´å†…å®¹
            content_summary = f"ç« èŠ‚: {chapter['title']}\nå†…å®¹æ‘˜è¦: {chapter['content'][:200]}..." if len(chapter['content']) > 200 else f"ç« èŠ‚: {chapter['title']}\nå†…å®¹: {chapter['content']}"
            
            chunk = TextChunk(
                content_id=f"{chapter['chapter_id']}_summary",
                document_id=document_id,
                content=content_summary,
                chapter_id=chapter['chapter_id'],
                chunk_index=i + 1,
                word_count=len(content_summary)
            )
            text_chunks.append(chunk)
        
        return text_chunks
    
    def _create_chapter_summaries(self, chapters: List[Dict], document_id: str) -> List:
        """
        å°†ç« èŠ‚æ•°æ®è½¬æ¢ä¸ºChapterSummaryå¯¹è±¡
        """
        from .final_schema import ChapterSummary
        
        chapter_summaries = []
        
        for chapter in chapters:
            chapter_summary = ChapterSummary(
                content_id=f"{document_id}_{chapter['chapter_id']}",
                document_id=document_id,
                chapter_id=chapter['chapter_id'],
                chapter_title=chapter['title'],
                raw_content=chapter['content'],  # å­˜å‚¨å®Œæ•´ç« èŠ‚å†…å®¹
                word_count=chapter['word_count']
            )
            chapter_summaries.append(chapter_summary)
        
        return chapter_summaries
    
    def _create_chapter_text_chunks(self, chapters: List[Dict], document_id: str) -> List:
        """
        åŸºäºç« èŠ‚åˆ›å»ºç®€åŒ–çš„TextChunkå¯¹è±¡ç”¨äºembedding
        æ³¨æ„ï¼šä¸å­˜å‚¨å®Œæ•´å†…å®¹ï¼Œåªå­˜å‚¨æ‘˜è¦ä¿¡æ¯
        """
        from .final_schema import TextChunk
        
        text_chunks = []
        
        for chapter in chapters:
            # åˆ›å»ºç« èŠ‚æ‘˜è¦è€Œä¸æ˜¯å®Œæ•´å†…å®¹
            content_summary = self._create_chapter_summary(chapter)
            
            chunk = TextChunk(
                content_id=f"{chapter['chapter_id']}_summary",
                document_id=document_id,
                content=content_summary,  # å­˜å‚¨æ‘˜è¦è€Œä¸æ˜¯å®Œæ•´å†…å®¹
                chapter_id=chapter['chapter_id'],
                chunk_index=1,
                word_count=len(content_summary)
            )
            text_chunks.append(chunk)
        
        return text_chunks
    
    def _create_chapter_summary(self, chapter: Dict) -> str:
        """
        ä¸ºç« èŠ‚åˆ›å»ºç®€çŸ­æ‘˜è¦ç”¨äºembedding
        """
        title = chapter.get('title', 'æœªçŸ¥ç« èŠ‚')
        content = chapter.get('content', '')
        
        # åˆ›å»ºç« èŠ‚æ‘˜è¦ï¼ˆå‰200å­—ç¬¦ + æ ‡é¢˜ï¼‰
        if len(content) > 200:
            summary = f"ç« èŠ‚: {title}\nå†…å®¹æ‘˜è¦: {content[:200]}..."
        else:
            summary = f"ç« èŠ‚: {title}\nå†…å®¹: {content}"
        
        return summary
    
    def _perform_ai_chunking(self, chapters: List[Dict], document_id: str) -> List:
        """
        ä½¿ç”¨ AI Chunker å¯¹ç« èŠ‚è¿›è¡Œç»†ç²’åº¦åˆ†å—
        """
        from ..pdf_processing.ai_chunker import chunk_chapters_with_ai
        from .final_schema import TextChunk
        
        try:
            # å‡†å¤‡ç« èŠ‚æ•°æ®ï¼Œæ ¼å¼åŒ–ä¸º AI Chunker éœ€è¦çš„æ ¼å¼
            chunker_chapters = []
            for chapter in chapters:
                chunker_chapters.append({
                    'chapter_id': chapter['chapter_id'],
                    'title': chapter['title'],
                    'content': chapter['content']
                })
            
            # è°ƒç”¨ AI Chunker è¿›è¡Œæ‰¹é‡åˆ†å—ï¼ˆä½¿ç”¨åŒæ­¥ä¾¿æ·å‡½æ•°ï¼‰
            print(f"ğŸš€ è°ƒç”¨ AI Chunker å¤„ç† {len(chunker_chapters)} ä¸ªç« èŠ‚...")
            minimal_chunks = chunk_chapters_with_ai(
                chunker_chapters, 
                model="qwen-turbo",
                max_workers=3
            )
            
            # è½¬æ¢ä¸º TextChunk å¯¹è±¡
            text_chunks = []
            for i, minimal_chunk in enumerate(minimal_chunks):
                text_chunk = TextChunk(
                    content_id=minimal_chunk.chunk_id,
                    document_id=document_id,
                    content=minimal_chunk.content,
                    chapter_id=minimal_chunk.belongs_to_chapter,
                    page_number=None,  # AI chunker æ²¡æœ‰é¡µç ä¿¡æ¯
                    chunk_index=i + 1,
                    word_count=minimal_chunk.word_count
                )
                text_chunks.append(text_chunk)
            
            print(f"âœ… AI Chunking æˆåŠŸï¼Œç”Ÿæˆ {len(text_chunks)} ä¸ªæ™ºèƒ½åˆ†å—")
            return text_chunks
            
        except Exception as e:
            print(f"âŒ AI Chunking å¤±è´¥: {e}")
            print("ğŸ”„ å›é€€åˆ°ç®€å•åˆ†å—ç­–ç•¥...")
            # å›é€€åˆ°ç®€å•ç­–ç•¥
            return self._create_text_chunks_for_embedding(chapters, document_id)
    
    def _update_multimodal_chapter_mapping(self, final_schema: FinalMetadataSchema, chapters: List[Dict]):
        """
        æ›´æ–°å›¾ç‰‡å’Œè¡¨æ ¼çš„ç« èŠ‚å…³è”ä¿¡æ¯
        åŸºäºé¡µç èŒƒå›´æˆ–å†…å®¹åŒ¹é…æ¥ç¡®å®šå›¾ç‰‡/è¡¨æ ¼å±äºå“ªä¸ªç« èŠ‚
        """
        print("ğŸ”— å¼€å§‹æ›´æ–°å¤šæ¨¡æ€å†…å®¹çš„ç« èŠ‚æ˜ å°„...")
        
        # 1. æ›´æ–°å›¾ç‰‡çš„ chapter_id
        updated_images = 0
        for img_chunk in final_schema.image_chunks:
            if not img_chunk.chapter_id:  # åªæ›´æ–°æœªåˆ†é…ç« èŠ‚çš„å›¾ç‰‡
                chapter_id = self._find_matching_chapter_for_content(
                    img_chunk.page_number, 
                    img_chunk.page_context, 
                    chapters
                )
                if chapter_id:
                    img_chunk.chapter_id = chapter_id
                    updated_images += 1
        
        # 2. æ›´æ–°è¡¨æ ¼çš„ chapter_id
        updated_tables = 0
        for table_chunk in final_schema.table_chunks:
            if not table_chunk.chapter_id:  # åªæ›´æ–°æœªåˆ†é…ç« èŠ‚çš„è¡¨æ ¼
                chapter_id = self._find_matching_chapter_for_content(
                    table_chunk.page_number,
                    table_chunk.page_context,
                    chapters
                )
                if chapter_id:
                    table_chunk.chapter_id = chapter_id
                    updated_tables += 1
        
        print(f"âœ… ç« èŠ‚æ˜ å°„æ›´æ–°å®Œæˆ: {updated_images}å¼ å›¾ç‰‡, {updated_tables}ä¸ªè¡¨æ ¼")
    
    def _find_matching_chapter_for_content(self, page_number: int, page_context: str, chapters: List[Dict]) -> Optional[str]:
        """
        ä¸ºå¤šæ¨¡æ€å†…å®¹æ‰¾åˆ°åŒ¹é…çš„ç« èŠ‚
        """
        if not chapters:
            return None
        
        # ç­–ç•¥1: å¦‚æœpage_contextä¸­åŒ…å«ç« èŠ‚å…³é”®è¯ï¼Œç›´æ¥åŒ¹é…
        if page_context:
            for chapter in chapters:
                chapter_title = chapter.get('title', '')
                # æ£€æŸ¥ç« èŠ‚æ ‡é¢˜çš„å…³é”®è¯æ˜¯å¦å‡ºç°åœ¨ä¸Šä¸‹æ–‡ä¸­
                if chapter_title and len(chapter_title) > 3:
                    title_keywords = chapter_title.replace('ç¯‡ç« ', '').strip()
                    if title_keywords and title_keywords in page_context:
                        print(f"ğŸ“ é€šè¿‡ä¸Šä¸‹æ–‡åŒ¹é…: {title_keywords} -> {chapter['chapter_id']}")
                        return chapter['chapter_id']
        
        # ç­–ç•¥2: åŸºäºé¡µç èŒƒå›´ï¼ˆç®€å•å¯å‘å¼ï¼‰
        # å‡è®¾ç« èŠ‚æŒ‰é¡ºåºåˆ†å¸ƒåœ¨æ–‡æ¡£ä¸­
        if page_number > 0:
            # æ ¹æ®é¡µç æ¯”ä¾‹åˆ†é…ç« èŠ‚
            total_pages = max(page_number, 10)  # ä¿å®ˆä¼°è®¡
            chapter_index = min(int((page_number - 1) / total_pages * len(chapters)), len(chapters) - 1)
            matched_chapter = chapters[chapter_index]
            print(f"ğŸ“ é€šè¿‡é¡µç åŒ¹é…: ç¬¬{page_number}é¡µ -> {matched_chapter['chapter_id']}")
            return matched_chapter['chapter_id']
        
        # ç­–ç•¥3: é»˜è®¤åˆ†é…ç»™ç¬¬ä¸€ä¸ªç« èŠ‚
        if chapters:
            return chapters[0]['chapter_id']
        
        return None
    
    def _generate_chapter_summaries(self, final_schema: FinalMetadataSchema):
        """
        ç”Ÿæˆç« èŠ‚æ€»ç»“ï¼Œæ›´æ–° ChapterSummary çš„ ai_summary å­—æ®µ
        """
        if not self.ai_clients.get('qwen'):
            print("âš ï¸ æ— å¯ç”¨ Qwen å®¢æˆ·ç«¯ï¼Œè·³è¿‡ç« èŠ‚æ€»ç»“ç”Ÿæˆ")
            return
        
        print("ğŸ“ å¼€å§‹ç”Ÿæˆç« èŠ‚æ€»ç»“...")
        
        chapter_summary_prompt = """è¯·ä¸ºä»¥ä¸‹ç« èŠ‚å†…å®¹ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ€»ç»“ï¼Œçªå‡ºå…³é”®ä¿¡æ¯å’Œé‡ç‚¹å†…å®¹ã€‚

è¦æ±‚ï¼š
1. æ€»ç»“åº”è¯¥æ˜¯200-300å­—
2. çªå‡ºç« èŠ‚çš„æ ¸å¿ƒå†…å®¹å’Œå…³é”®ä¿¡æ¯
3. ä¿æŒä¸“ä¸šæ€§ï¼Œé€‚åˆç”¨äºæ–‡æ¡£æ£€ç´¢
4. åŒ…å«ç« èŠ‚ä¸­çš„é‡è¦æŠ€æœ¯ç»†èŠ‚ã€æ•°æ®æˆ–ç»“è®º

ç« èŠ‚æ ‡é¢˜ï¼š{chapter_title}

ç« èŠ‚å†…å®¹ï¼š
{chapter_content}

è¯·ç”Ÿæˆç®€æ´å‡†ç¡®çš„ç« èŠ‚æ€»ç»“ï¼š"""
        
        qwen_client = self.ai_clients['qwen']
        updated_summaries = 0
        
        for chapter_summary in final_schema.chapter_summaries:
            if not chapter_summary.ai_summary and chapter_summary.raw_content:
                try:
                    print(f"ğŸ“ ç”Ÿæˆç« èŠ‚æ€»ç»“: {chapter_summary.chapter_title}")
                    
                    prompt = chapter_summary_prompt.format(
                        chapter_title=chapter_summary.chapter_title,
                        chapter_content=chapter_summary.raw_content[:2000]  # é™åˆ¶é•¿åº¦é¿å…è¶…å‡ºtokené™åˆ¶
                    )
                    
                    ai_summary = qwen_client.generate_response(prompt)
                    if ai_summary and ai_summary.strip():
                        chapter_summary.ai_summary = ai_summary.strip()
                        updated_summaries += 1
                        print(f"âœ… ç« èŠ‚æ€»ç»“ç”Ÿæˆå®Œæˆ: {chapter_summary.chapter_title}")
                    
                except Exception as e:
                    print(f"âš ï¸ ç« èŠ‚æ€»ç»“ç”Ÿæˆå¤±è´¥ ({chapter_summary.chapter_title}): {e}")
        
        print(f"âœ… ç« èŠ‚æ€»ç»“ç”Ÿæˆå®Œæˆ: {updated_summaries}/{len(final_schema.chapter_summaries)} ä¸ªæˆåŠŸ")
    
    def _stitch_cleaned_full_text(self, final_schema: FinalMetadataSchema) -> str:
        """
        æ‹¼æ¥æ¸…ç†åçš„å®Œæ•´æ–‡æœ¬ï¼Œä¼˜å…ˆä½¿ç”¨cleaned_page_textsï¼Œå›é€€åˆ°åŸå§‹page_texts
        """
        if not final_schema.document_summary:
            return ""
        
        # ä¼˜å…ˆä½¿ç”¨æ¸…ç†åçš„æ–‡æœ¬
        if (final_schema.document_summary.cleaned_page_texts):
            page_texts = final_schema.document_summary.cleaned_page_texts
            print("âœ… ä½¿ç”¨æ¸…ç†åçš„é¡µé¢æ–‡æœ¬")
        elif final_schema.document_summary.page_texts:
            page_texts = final_schema.document_summary.page_texts
            print("âš ï¸ ä½¿ç”¨åŸå§‹é¡µé¢æ–‡æœ¬ï¼ˆæ¸…ç†ç‰ˆæœ¬ä¸å¯ç”¨ï¼‰")
        else:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„é¡µé¢æ–‡æœ¬")
            return ""
        
        # æŒ‰é¡µç é¡ºåºæ‹¼æ¥
        page_numbers = sorted([int(k) for k in page_texts.keys() if k.isdigit()])
        text_parts = []
        
        for page_num in page_numbers:
            page_text = page_texts.get(str(page_num), "")
            if page_text and page_text.strip():
                text_parts.append(page_text.strip())
        
        full_text = "\n\n".join(text_parts)
        print(f"ğŸ“– æ‹¼æ¥å®Œæˆï¼Œæ€»é•¿åº¦: {len(full_text)} å­—ç¬¦ï¼ŒåŒ…å« {len(page_numbers)} é¡µ")
        
        return full_text
    
    def _cut_chapters_by_toc(self, full_text: str, toc_data: List[Dict]) -> List[Dict]:
        """
        åŸºäºTOCæ•°æ®åˆ‡åˆ†ç« èŠ‚å†…å®¹ï¼ˆåªå¤„ç†ä¸»è¦ç¯‡ç« ï¼‰
        """
        chapters = []
        
        # åªå¤„ç†ç¬¬ä¸€çº§ç« èŠ‚ï¼Œå¹¶è¿‡æ»¤æ‰éä¸»è¦ç« èŠ‚
        first_level_toc = [item for item in toc_data if item.get('level') == 1]
        
        # è¿‡æ»¤å‡ºçœŸæ­£çš„ä¸»è¦ç« èŠ‚ï¼ˆç¯‡ç« ä¸€ã€ç¯‡ç« äºŒã€ç¯‡ç« ä¸‰ç­‰ï¼‰
        major_chapters = []
        for item in first_level_toc:
            title = item.get('title', '')
            if any(keyword in title for keyword in ['ç¯‡ç« ', 'ç« èŠ‚', 'ç¬¬ä¸€ç« ', 'ç¬¬äºŒç« ', 'ç¬¬ä¸‰ç« ']):
                major_chapters.append(item)
        
        print(f"ğŸ” æ‰¾åˆ°ä¸»è¦ç« èŠ‚: {[item.get('title') for item in major_chapters]}")
        print(f"ğŸ“ å…¨æ–‡æ€»é•¿åº¦: {len(full_text)} å­—ç¬¦")
        
        for i, toc_item in enumerate(major_chapters):
            chapter_id = toc_item.get('chapter_id', f"chapter_{toc_item.get('id', i+1)}")
            title = toc_item.get('title', f"ç¬¬{i+1}ç« ")
            start_text = toc_item.get('start_text', '')
            
            # æŸ¥æ‰¾ç« èŠ‚å¼€å§‹ä½ç½®
            start_pos = self._find_chapter_start_in_full_text(full_text, start_text, title)
            if start_pos == -1:
                print(f"âš ï¸ æ— æ³•æ‰¾åˆ°ç« èŠ‚ '{title}' çš„å¼€å§‹ä½ç½®")
                continue
                
            # ç¡®å®šç« èŠ‚ç»“æŸä½ç½®
            if i + 1 < len(major_chapters):
                next_start_text = major_chapters[i + 1].get('start_text', '')
                next_title = major_chapters[i + 1].get('title', '')
                end_pos = self._find_chapter_start_in_full_text(full_text, next_start_text, next_title)
                if end_pos == -1:
                    end_pos = len(full_text)
            else:
                end_pos = len(full_text)
            
            print(f"ğŸ“ ç« èŠ‚ '{title}': ä½ç½® {start_pos} -> {end_pos}")
            
            # æå–ç« èŠ‚å†…å®¹
            content = full_text[start_pos:end_pos].strip()
            
            # æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
            preview = content[:100].replace('\n', ' ') if content else "(ç©ºå†…å®¹)"
            print(f"ğŸ“– å†…å®¹é¢„è§ˆ: {preview}...")
            
            chapter = {
                "chapter_id": chapter_id,
                "title": title,
                "content": content,
                "start_pos": start_pos,
                "end_pos": end_pos,
                "word_count": len(content),
                "order": i + 1
            }
            
            chapters.append(chapter)
            print(f"âœ… ç« èŠ‚åˆ‡åˆ†: {title} ({len(content)} å­—ç¬¦)")
        
        return chapters
    
    def _find_chapter_start_in_full_text(self, full_text: str, start_text: str, title: str = "") -> int:
        """
        åœ¨å®Œæ•´æ–‡æœ¬ä¸­æŸ¥æ‰¾ç« èŠ‚å¼€å§‹ä½ç½®ï¼Œä¼˜å…ˆä½¿ç”¨start_textï¼Œå›é€€åˆ°æ ‡é¢˜æœç´¢
        """
        import re
        
        # ç‰¹æ®Šå¤„ç†ä¸»è¦ç¯‡ç« æ ‡é¢˜
        if title:
            # ç›´æ¥æœç´¢ç¯‡ç« æ ‡é¢˜å…³é”®è¯
            if "ç¯‡ç« ä¸€" in title or "é¡¹ç›®èµ„æ–™" in title:
                # æœç´¢"é¡¹ç›®èƒŒæ™¯"ä½œä¸ºç¯‡ç« ä¸€çš„å¼€å§‹
                patterns = [
                    r"é¡¹ç›®èƒŒæ™¯[:ï¼š]",
                    r"## é¡¹ç›®èƒŒæ™¯",
                    r"é¡¹ç›®èƒŒæ™¯åŠåŒºä½",
                    r"è¯¥é¡¹ç›®ä½äºå¹¿å·å¸‚ç™½äº‘åŒº"
                ]
                for pattern in patterns:
                    match = re.search(pattern, full_text)
                    if match:
                        print(f"âœ… æ‰¾åˆ°ç¯‡ç« ä¸€å¼€å§‹ä½ç½®ï¼Œä½¿ç”¨æ¨¡å¼: {pattern}")
                        return match.start()
            
            elif "ç¯‡ç« äºŒ" in title or "æ–¹æ¡ˆ" in title:
                # æœç´¢"ç¯‡ç« äºŒ"æˆ–ç›¸å…³æ–¹æ¡ˆå†…å®¹
                patterns = [
                    r"ç¯‡ç« äºŒ",
                    r"å·¥ç¨‹åç§°[:ï¼š]",
                    r"é¹¤è¾¹ä¸€ç¤¾ç¤¾æ–‡ä½“æ´»åŠ¨ä¸­å¿ƒ",
                    r"å·¥ç¨‹æ¦‚å†µåŠä¾æ®"
                ]
                for pattern in patterns:
                    match = re.search(pattern, full_text)
                    if match:
                        print(f"âœ… æ‰¾åˆ°ç¯‡ç« äºŒå¼€å§‹ä½ç½®ï¼Œä½¿ç”¨æ¨¡å¼: {pattern}")
                        return match.start()
            
            elif "ç¯‡ç« ä¸‰" in title or "æ€»ç»“" in title:
                # æœç´¢"æ€»ç»“"ç›¸å…³å†…å®¹
                patterns = [
                    r"ç¯‡ç« ä¸‰",
                    r"ç¤¾ä¼šä»·å€¼[:ï¼š]",
                    r"ç¯å¢ƒä»·å€¼[:ï¼š]",
                    r"ç¤¾åŒºæ–‡ä½“æ´»åŠ¨ä¸­å¿ƒå»ºç­‘å·²ä¸¥é‡æ®‹æŸ"
                ]
                for pattern in patterns:
                    match = re.search(pattern, full_text)
                    if match:
                        print(f"âœ… æ‰¾åˆ°ç¯‡ç« ä¸‰å¼€å§‹ä½ç½®ï¼Œä½¿ç”¨æ¨¡å¼: {pattern}")
                        return match.start()
        
        # åŸæœ‰çš„start_textåŒ¹é…é€»è¾‘
        if start_text:
            # 1. å°è¯•ç²¾ç¡®åŒ¹é…start_text
            pos = full_text.find(start_text)
            if pos != -1:
                print(f"âœ… ä½¿ç”¨ç²¾ç¡®åŒ¹é…æ‰¾åˆ°ç« èŠ‚: {start_text[:30]}...")
                return pos
            
            # 2. å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆå»é™¤ç©ºæ ¼å’Œæ ‡ç‚¹ï¼‰
            cleaned_start = re.sub(r'[^\w\u4e00-\u9fff]', '', start_text)
            if len(cleaned_start) > 5:  # è‡³å°‘5ä¸ªæœ‰æ•ˆå­—ç¬¦
                pattern = '.*?'.join(re.escape(char) for char in cleaned_start[:10])  # å‰10ä¸ªå­—ç¬¦
                match = re.search(pattern, full_text, re.IGNORECASE)
                if match:
                    print(f"âœ… ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…æ‰¾åˆ°ç« èŠ‚: {pattern}")
                    return match.start()
        
        # 3. å›é€€åˆ°æ ‡é¢˜æœç´¢
        if title:
            # æ¸…ç†æ ‡é¢˜ï¼Œå»é™¤åºå·
            clean_title = re.sub(r'^[\d\.\s]*', '', title).strip()
            if len(clean_title) > 2:
                pos = full_text.find(clean_title)
                if pos != -1:
                    print(f"âœ… ä½¿ç”¨æ ‡é¢˜åŒ¹é…æ‰¾åˆ°ç« èŠ‚: {clean_title}")
                    return pos
                    
                # å°è¯•éƒ¨åˆ†åŒ¹é…
                if len(clean_title) > 6:
                    partial_title = clean_title[:6]
                    pos = full_text.find(partial_title)
                    if pos != -1:
                        print(f"âœ… ä½¿ç”¨éƒ¨åˆ†æ ‡é¢˜åŒ¹é…æ‰¾åˆ°ç« èŠ‚: {partial_title}")
                        return pos
        
        print(f"âŒ æ— æ³•æ‰¾åˆ°ç« èŠ‚ä½ç½®: title='{title}', start_text='{start_text[:50]}...'")
        return -1


    def _process_step_2_4_multimodal(self, final_schema: FinalMetadataSchema):
        """
        Step 2.4: å¤šæ¨¡æ€æè¿°ç”Ÿæˆï¼ˆå¹¶è¡Œå¤„ç†ï¼‰
        æ›´æ–°åŸæœ‰image_chunkså’Œtable_chunksçš„ç»“æ„åŒ–æè¿°å­—æ®µ
        """
        print("ğŸ–¼ï¸  å¼€å§‹å¤šæ¨¡æ€æè¿°ç”Ÿæˆ...")
        
        # æ”¶é›†éœ€è¦å¤„ç†çš„å›¾ç‰‡å’Œè¡¨æ ¼
        image_tasks = []
        table_tasks = []
        
        for img_chunk in final_schema.image_chunks:
            if not img_chunk.search_summary:  # åªå¤„ç†è¿˜æ²¡æœ‰ç»“æ„åŒ–æè¿°çš„
                image_tasks.append(img_chunk)
        
        for table_chunk in final_schema.table_chunks:
            if not table_chunk.search_summary:  # åªå¤„ç†è¿˜æ²¡æœ‰ç»“æ„åŒ–æè¿°çš„
                table_tasks.append(table_chunk)
        
        print(f"ğŸ“Š å¾…å¤„ç†: {len(image_tasks)}å¼ å›¾ç‰‡, {len(table_tasks)}ä¸ªè¡¨æ ¼")
        
        if not image_tasks and not table_tasks:
            print("â„¹ï¸  æ‰€æœ‰å¤šæ¨¡æ€å†…å®¹å·²æœ‰ç»“æ„åŒ–æè¿°ï¼Œè·³è¿‡å¤„ç†")
            return
        
        # å¹¶è¡Œå¤„ç†å›¾ç‰‡
        if image_tasks and self.ai_clients.get('openrouter'):
            print(f"ğŸ”„ å¹¶è¡Œå¤„ç†{len(image_tasks)}å¼ å›¾ç‰‡...")
            self._parallel_process_images(image_tasks)
        
        # å¹¶è¡Œå¤„ç†è¡¨æ ¼ (å¦‚æœéœ€è¦çš„è¯)
        if table_tasks and self.ai_clients.get('openrouter'):
            print(f"ğŸ”„ å¹¶è¡Œå¤„ç†{len(table_tasks)}ä¸ªè¡¨æ ¼...")
            self._parallel_process_tables(table_tasks)
        
        print("âœ… å¤šæ¨¡æ€æè¿°ç”Ÿæˆå®Œæˆ")
    
    def _parallel_process_images(self, image_tasks: List):
        """å¹¶è¡Œå¤„ç†å›¾ç‰‡æè¿°ç”Ÿæˆ"""
        import concurrent.futures
        
        def process_single_image(img_chunk):
            try:
                # è°ƒç”¨AIç”Ÿæˆç»“æ„åŒ–æè¿°
                result = self._generate_structured_image_description(img_chunk)
                return img_chunk, result, None
            except Exception as e:
                return img_chunk, None, str(e)
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†ï¼ˆAIè°ƒç”¨æ˜¯IOå¯†é›†å‹ï¼‰
        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            futures = [executor.submit(process_single_image, img) for img in image_tasks]
            
            for i, future in enumerate(concurrent.futures.as_completed(futures)):
                img_chunk, result, error = future.result()
                
                if error:
                    print(f"âš ï¸  å›¾ç‰‡ {img_chunk.content_id} å¤„ç†å¤±è´¥: {error}")
                elif result:
                    # æ›´æ–°ç»“æ„åŒ–æè¿°å­—æ®µ
                    img_chunk.search_summary = result.get('search_summary')
                    img_chunk.detailed_description = result.get('detailed_description') 
                    img_chunk.engineering_details = result.get('engineering_details')
                    print(f"âœ… å›¾ç‰‡ {i+1}/{len(image_tasks)} å¤„ç†å®Œæˆ")
    
    def _generate_structured_image_description(self, img_chunk) -> Optional[Dict[str, Any]]:
        """ä¸ºå•å¼ å›¾ç‰‡ç”Ÿæˆç»“æ„åŒ–æè¿°"""
        if not self.ai_clients.get('openrouter'):
            return None
        
        try:
            # æ„å»ºprompt
            prompt = self.image_description_prompt.format(
                page_context=img_chunk.page_context or "æ— ä¸Šä¸‹æ–‡ä¿¡æ¯"
            )
            
            # è°ƒç”¨AIç”Ÿæˆæè¿°ï¼ˆåŒ…å«å›¾ç‰‡ï¼‰
            client = self.ai_clients['openrouter']
            
            # æ„å»ºå›¾ç‰‡è·¯å¾„
            image_path = Path(img_chunk.image_path)
            if not image_path.is_absolute():
                # ç›¸å¯¹è·¯å¾„ï¼Œéœ€è¦ä»é¡¹ç›®æ ¹ç›®å½•è§£æ
                project_root = Path(__file__).parent.parent.parent
                image_path = project_root / image_path
            
            if not image_path.exists():
                print(f"âš ï¸  å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
                return None
            
            # è°ƒç”¨OpenRouterå®¢æˆ·ç«¯ç”Ÿæˆæè¿°
            response = client.get_image_description_gemini(str(image_path), prompt)
            
            if not response:
                return None
            
            # å°è¯•è§£æJSONå“åº”ï¼ˆå¤„ç†markdownåŒ…è£…çš„æƒ…å†µï¼‰
            try:
                import json
                import re
                
                # å…ˆå°è¯•ç›´æ¥è§£æ
                result = json.loads(response)
                return result
            except json.JSONDecodeError:
                # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•ä»markdownä¸­æå–JSON
                # æŸ¥æ‰¾ ```json ... ``` æˆ– ``` ... ``` æ ¼å¼
                json_match = re.search(r'```(?:json)?\s*\n?(.*?)\n?```', response, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1).strip()
                    try:
                        result = json.loads(json_str)
                        return result
                    except json.JSONDecodeError:
                        pass
                
                # å¦‚æœä»ç„¶å¤±è´¥ï¼ŒæŸ¥æ‰¾ç¬¬ä¸€ä¸ª { åˆ°æœ€åä¸€ä¸ª } ä¹‹é—´çš„å†…å®¹
                brace_match = re.search(r'\{.*\}', response, re.DOTALL)
                if brace_match:
                    json_str = brace_match.group(0)
                    try:
                        result = json.loads(json_str)
                        return result
                    except json.JSONDecodeError:
                        pass
                
                # æœ€åçš„fallbackï¼šæŠŠæ•´ä¸ªå“åº”ä½œä¸ºdetailed_description
                print(f"âš ï¸  AIè¿”å›éJSONæ ¼å¼ï¼Œä½¿ç”¨fallbackå¤„ç†: {response[:100]}...")
                return {
                    "search_summary": "AIç”Ÿæˆçš„å›¾ç‰‡æè¿°",
                    "detailed_description": response,
                    "engineering_details": None
                }
        
        except Exception as e:
            print(f"âš ï¸  å›¾ç‰‡æè¿°ç”Ÿæˆå¤±è´¥: {str(e)}")
            return None
    
    def _parallel_process_tables(self, table_tasks: List):
        """å¹¶è¡Œå¤„ç†è¡¨æ ¼æè¿°ç”Ÿæˆï¼ˆç±»ä¼¼å›¾ç‰‡å¤„ç†ï¼‰"""
        if not table_tasks:
            print("ğŸ“‹ æ— è¡¨æ ¼éœ€è¦å¤„ç†")
            return
        
        print(f"ğŸ“‹ å¼€å§‹å¤„ç† {len(table_tasks)} ä¸ªè¡¨æ ¼...")
        
        def process_single_table(table_chunk):
            """å¤„ç†å•ä¸ªè¡¨æ ¼çš„æè¿°ç”Ÿæˆ"""
            try:
                # è°ƒç”¨AIç”Ÿæˆç»“æ„åŒ–æè¿°
                result = self._generate_structured_table_description(table_chunk)
                return table_chunk, result, None
            except Exception as e:
                return table_chunk, None, str(e)
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†ï¼ˆAIè°ƒç”¨æ˜¯IOå¯†é›†å‹ï¼‰
        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            futures = [executor.submit(process_single_table, table) for table in table_tasks]
            
            for i, future in enumerate(concurrent.futures.as_completed(futures)):
                table_chunk, result, error = future.result()
                
                if error:
                    print(f"âš ï¸  è¡¨æ ¼ {table_chunk.content_id} å¤„ç†å¤±è´¥: {error}")
                elif result:
                    # æ›´æ–°ç»“æ„åŒ–æè¿°å­—æ®µ
                    table_chunk.search_summary = result.get('search_summary')
                    table_chunk.detailed_description = result.get('detailed_description') 
                    table_chunk.engineering_details = result.get('engineering_details')
                    print(f"âœ… è¡¨æ ¼ {i+1}/{len(table_tasks)} å¤„ç†å®Œæˆ")

    def _generate_structured_table_description(self, table_chunk) -> Optional[Dict[str, Any]]:
        """ä¸ºå•ä¸ªè¡¨æ ¼ç”Ÿæˆç»“æ„åŒ–æè¿°"""
        if not self.ai_clients.get('openrouter'):
            return None
        
        try:
            # æ„å»ºprompt
            prompt = self.table_description_prompt.format(
                page_context=table_chunk.page_context or "æ— ä¸Šä¸‹æ–‡ä¿¡æ¯"
            )
            
            # è°ƒç”¨AIç”Ÿæˆæè¿°ï¼ˆåŒ…å«è¡¨æ ¼å›¾ç‰‡ï¼‰
            client = self.ai_clients['openrouter']
            
            # æ„å»ºè¡¨æ ¼å›¾ç‰‡è·¯å¾„
            table_path = Path(table_chunk.table_path)
            if not table_path.is_absolute():
                # ç›¸å¯¹è·¯å¾„ï¼Œéœ€è¦ä»é¡¹ç›®æ ¹ç›®å½•è§£æ
                project_root = Path(__file__).parent.parent.parent
                table_path = project_root / table_path
            
            if not table_path.exists():
                print(f"âš ï¸  è¡¨æ ¼æ–‡ä»¶ä¸å­˜åœ¨: {table_path}")
                return None
            
            # è°ƒç”¨OpenRouterå®¢æˆ·ç«¯ç”Ÿæˆæè¿°
            response = client.get_image_description_gemini(str(table_path), prompt)
            
            if not response:
                return None
            
            # å°è¯•è§£æJSONå“åº”ï¼ˆå¤„ç†markdownåŒ…è£…çš„æƒ…å†µï¼‰
            try:
                import json
                import re
                
                # å…ˆå°è¯•ç›´æ¥è§£æ
                result = json.loads(response)
                return result
            except json.JSONDecodeError:
                # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•ä»markdownä¸­æå–JSON
                # æŸ¥æ‰¾ ```json ... ``` æˆ– ``` ... ``` æ ¼å¼
                json_match = re.search(r'```(?:json)?\s*\n?(.*?)\n?```', response, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1).strip()
                    try:
                        result = json.loads(json_str)
                        return result
                    except json.JSONDecodeError:
                        pass
                
                # å¦‚æœä»ç„¶å¤±è´¥ï¼ŒæŸ¥æ‰¾ç¬¬ä¸€ä¸ª { åˆ°æœ€åä¸€ä¸ª } ä¹‹é—´çš„å†…å®¹
                brace_match = re.search(r'\{.*\}', response, re.DOTALL)
                if brace_match:
                    json_str = brace_match.group(0)
                    try:
                        result = json.loads(json_str)
                        return result
                    except json.JSONDecodeError:
                        pass
                
                # æœ€åçš„fallbackï¼šæŠŠæ•´ä¸ªå“åº”ä½œä¸ºdetailed_description
                print(f"âš ï¸  AIè¿”å›éJSONæ ¼å¼ï¼Œä½¿ç”¨fallbackå¤„ç†: {response[:100]}...")
                return {
                    "search_summary": "AIç”Ÿæˆçš„è¡¨æ ¼æè¿°",
                    "detailed_description": response,
                    "engineering_details": None
                }
        
        except Exception as e:
            print(f"âš ï¸  è¡¨æ ¼æè¿°ç”Ÿæˆå¤±è´¥: {str(e)}")
            return None

    def _load_stage1_data(self, input_path: str) -> FinalMetadataSchema:
        """åŠ è½½Stage1å¤„ç†çš„æ•°æ®"""
        input_path_obj = Path(input_path)
        
        if input_path_obj.is_file() and input_path_obj.name.endswith('.json'):
            # è¾“å…¥æ˜¯jsonæ–‡ä»¶ï¼Œç›´æ¥åŠ è½½
            metadata_path = input_path_obj
        else:
            # è¾“å…¥æ˜¯ç›®å½•ï¼ŒæŸ¥æ‰¾å…¶ä¸­çš„final_metadata.json
            metadata_path = input_path_obj / "final_metadata.json"
        
        if not metadata_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°metadataæ–‡ä»¶: {metadata_path}")
        
        print(f"ğŸ“– ä»{metadata_path}åŠ è½½Stage1æ•°æ®...")
        final_schema = FinalMetadataSchema.load(str(metadata_path))
        
        # æ£€æŸ¥å¿…è¦æ•°æ®
        if not final_schema.document_summary:
            raise ValueError("ç¼ºå°‘document_summaryæ•°æ®ï¼Œæ— æ³•ç»§ç»­Stage2å¤„ç†")
            
        if not final_schema.document_summary.page_texts:
            raise ValueError("ç¼ºå°‘page_textsæ•°æ®ï¼Œæ— æ³•ç»§ç»­Stage2å¤„ç†")
            
        print(f"ğŸ“Š å¾…å¤„ç†æ•°æ®: {len(final_schema.document_summary.page_texts)}é¡µæ–‡æœ¬")
        return final_schema
    
    def _clean_page_text(self, raw_text: str) -> str:
        """æ¸…ç†å•é¡µæ–‡æœ¬"""
        if not raw_text or not raw_text.strip():
            return raw_text
            
        try:
            qwen_client = self.ai_clients['qwen']
            prompt = self.text_cleaning_prompt.format(original_text=raw_text)
            
            # è°ƒç”¨Qwenè¿›è¡Œæ–‡æœ¬æ¸…æ´—
            cleaned_text = qwen_client.generate_response(prompt)
            return cleaned_text.strip() if cleaned_text else raw_text
            
        except Exception as e:
            print(f"âŒ æ–‡æœ¬æ¸…æ´—å¤±è´¥: {e}")
            return raw_text
    
    def _print_processing_report(self, final_schema: FinalMetadataSchema, processing_time: float):
        """æ‰“å°å¤„ç†æŠ¥å‘Š"""
        print(f"\nğŸ“Š Stage2å¤„ç†æŠ¥å‘Š:")
        print(f"   â±ï¸  å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
        print(f"   ğŸ“„ é¡µé¢ä¿®å¤: {self.stats.get('pages_repaired', 0)}é¡µ")
        print(f"   ğŸ–¼ï¸  å›¾ç‰‡æè¿°: {self.stats.get('images_described', 0)}å¼ ")  
        print(f"   ğŸ“Š è¡¨æ ¼æè¿°: {self.stats.get('tables_described', 0)}ä¸ª")
        
        if final_schema.image_chunks:
            structured_images = sum(1 for img in final_schema.image_chunks if img.search_summary)
            print(f"   ğŸ¯ ç»“æ„åŒ–å›¾ç‰‡æè¿°: {structured_images}/{len(final_schema.image_chunks)}")
        
        if final_schema.table_chunks:
            structured_tables = sum(1 for table in final_schema.table_chunks if table.search_summary)
            print(f"   ğŸ¯ ç»“æ„åŒ–è¡¨æ ¼æè¿°: {structured_tables}/{len(final_schema.table_chunks)}")
            
    def process_demo(self, final_metadata_path: str) -> Tuple[FinalMetadataSchema, str]:
        """
        æ¼”ç¤ºç‰ˆæœ¬çš„å¤„ç†æ–¹æ³•ï¼Œä¿æŒåŸæœ‰çš„è¿”å›ç±»å‹ç”¨äºæµ‹è¯•
        """
        print(f"ğŸš€ å¼€å§‹Stage2æ¼”ç¤ºå¤„ç†...")
        
        # åŠ è½½æ•°æ®å¹¶å¤„ç†
        final_schema = self._load_stage1_data(final_metadata_path)
        self._process_step_2_1_text_repair(final_schema)
        self._process_step_2_4_multimodal(final_schema)
        
        # ä¿å­˜æ¼”ç¤ºç»“æœï¼ˆåˆ›å»ºæ–°æ–‡ä»¶ï¼‰
        output_path = final_metadata_path.replace('.json', '_stage2_demo.json')
        final_schema.save(output_path)
        
        return final_schema, output_path


def create_stage2_processor() -> Stage2IntelligentProcessor:
    """åˆ›å»ºStage2å¤„ç†å™¨å®ä¾‹"""
    return Stage2IntelligentProcessor()


def process_stage2_from_file(final_metadata_path: str) -> Tuple[FinalMetadataSchema, str]:
    """
    ä»final_metadata.jsonæ–‡ä»¶å¼€å§‹Stage2å¤„ç†
    
    Args:
        final_metadata_path: Stage1è¾“å‡ºçš„final_metadata.jsonè·¯å¾„
        
    Returns:
        Tuple[FinalMetadataSchema, str]: (æ›´æ–°åçš„schema, ä¿å­˜è·¯å¾„)
    """
    processor = create_stage2_processor()
    output_path = processor.process(final_metadata_path)
    
    # é‡æ–°åŠ è½½æ›´æ–°åçš„schema
    final_schema = FinalMetadataSchema.load(output_path)
    
    return final_schema, output_path 


def main():
    """ä¸»å‡½æ•°ï¼Œæ”¯æŒå‘½ä»¤è¡Œå‚æ•°"""
    import sys
    import os
    
    if len(sys.argv) != 2:
        print("Usage: python -m src.pdf_processing_2.stage2_intelligent_processor <output_dir>")
        print("Example: python -m src.pdf_processing_2.stage2_intelligent_processor parser_output_v2/test_stage1_20250716_154105/")
        sys.exit(1)
    
    output_dir = sys.argv[1]
    final_metadata_path = os.path.join(output_dir, "final_metadata.json")
    
    if not os.path.exists(final_metadata_path):
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {final_metadata_path}")
        sys.exit(1)
    
    try:
        print(f"ğŸš€ å¼€å§‹Stage2æ™ºèƒ½å¤„ç†: {final_metadata_path}")
        processor = create_stage2_processor()
        output_path = processor.process(final_metadata_path)
        print(f"âœ… Stage2å¤„ç†å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {output_path}")
        
    except Exception as e:
        print(f"âŒ Stage2å¤„ç†å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main() 