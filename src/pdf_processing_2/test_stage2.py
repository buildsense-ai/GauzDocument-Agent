#!/usr/bin/env python3
"""
Stage2æ™ºèƒ½å¤„ç†å™¨å®Œæ•´æµ‹è¯•è„šæœ¬
æµ‹è¯•å››ä¸ªæ ¸å¿ƒæ­¥éª¤ï¼š
- Step 2.1: é¡µé¢æ–‡æœ¬ä¿®å¤
- Step 2.2: å…¨å±€ç»“æ„è¯†åˆ«ï¼ˆTOCæå–ï¼‰
- Step 2.3: å†…å®¹æå–ä¸åˆ‡åˆ†ï¼ˆç« èŠ‚åˆ‡åˆ†ï¼‰
- Step 2.4: å¤šæ¨¡æ€æè¿°ç”Ÿæˆ
"""

import os
import sys
import json
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.pdf_processing_2.stage2_intelligent_processor import Stage2IntelligentProcessor, process_stage2_from_file
from src.pdf_processing_2.final_schema import FinalMetadataSchema


def test_stage2_complete():
    """æµ‹è¯•Stage2çš„å®Œæ•´å¤„ç†æµç¨‹"""
    print("ğŸ¯ æµ‹è¯•Stage2å®Œæ•´å¤„ç†åŠŸèƒ½...")
    
    # æµ‹è¯•è·¯å¾„
    metadata_path = "parser_output_v2/test_stage1_20250716_110158/final_metadata.json"
    
    if not os.path.exists(metadata_path):
        print(f"âŒ æ‰¾ä¸åˆ°æµ‹è¯•æ–‡ä»¶: {metadata_path}")
        return False
    
    print(f"ğŸ“„ ä½¿ç”¨æµ‹è¯•æ–‡ä»¶: {metadata_path}")
    
    try:
        # æ‰§è¡ŒStage2å¤„ç†
        print("\nğŸš€ å¼€å§‹æ‰§è¡ŒStage2å®Œæ•´å¤„ç†...")
        start_time = time.time()
        
        final_schema, output_path = process_stage2_from_file(metadata_path)
        
        processing_time = time.time() - start_time
        
        # éªŒè¯å¤„ç†ç»“æœ
        print(f"\nğŸ“Š Stage2å¤„ç†ç»“æœéªŒè¯:")
        print(f"âœ… å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
        print(f"âœ… è¾“å‡ºæ–‡ä»¶: {output_path}")
        
        # éªŒè¯æ–‡æ¡£æ‘˜è¦å’Œé¡µé¢ä¿®å¤
        if final_schema.document_summary:
            print(f"âœ… æ–‡æ¡£æ€»é¡µæ•°: {final_schema.document_summary.total_pages}")
            
            # æ£€æŸ¥é¡µé¢æ–‡æœ¬ä¿®å¤
            original_pages = len(final_schema.document_summary.page_texts) if final_schema.document_summary.page_texts else 0
            cleaned_pages = len(final_schema.document_summary.cleaned_page_texts) if final_schema.document_summary.cleaned_page_texts else 0
            print(f"âœ… åŸå§‹é¡µé¢æ–‡æœ¬: {original_pages}é¡µ")
            print(f"âœ… ä¿®å¤é¡µé¢æ–‡æœ¬: {cleaned_pages}é¡µ")
            
            # æ£€æŸ¥TOCæå–ç»“æœ
            if final_schema.document_summary.metadata and final_schema.document_summary.metadata.get('toc'):
                toc_items = final_schema.document_summary.metadata['toc']
                print(f"âœ… TOCæå–æˆåŠŸ: {len(toc_items)}ä¸ªç« èŠ‚")
                
                # æ˜¾ç¤ºTOCç»“æ„
                print(f"ğŸ“– TOCç»“æ„é¢„è§ˆ:")
                for item in toc_items[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                    indent = "  " * (item.get('level', 1) - 1)
                    print(f"  {indent}{item.get('level', 1)}. {item.get('title', 'æœªçŸ¥æ ‡é¢˜')}")
                if len(toc_items) > 3:
                    print(f"  ...ï¼ˆå…±{len(toc_items)}ä¸ªç« èŠ‚ï¼‰")
            else:
                print(f"âš ï¸ TOCæå–ç»“æœ: æœªæ‰¾åˆ°ç« èŠ‚ç»“æ„")
        else:
            print("âš ï¸ æ–‡æ¡£æ‘˜è¦ä¸ºç©º")
        
        # éªŒè¯æ–‡æœ¬åˆ†å—ç»“æœ
        print(f"âœ… æ–‡æœ¬åˆ†å—æ•°: {len(final_schema.text_chunks)}")
        if final_schema.text_chunks:
            # æ˜¾ç¤ºåˆ†å—é¢„è§ˆ
            print(f"ğŸ“ åˆ†å—é¢„è§ˆ:")
            for i, chunk in enumerate(final_schema.text_chunks[:2]):  # åªæ˜¾ç¤ºå‰2ä¸ª
                content_preview = chunk.content[:100] + "..." if len(chunk.content) > 100 else chunk.content
                print(f"  å—{i+1}: {chunk.chapter_id} ({chunk.word_count}å­—ç¬¦)")
                print(f"       {content_preview}")
            if len(final_schema.text_chunks) > 2:
                print(f"  ...ï¼ˆå…±{len(final_schema.text_chunks)}ä¸ªåˆ†å—ï¼‰")
        
        # éªŒè¯å¤šæ¨¡æ€æè¿°
        print(f"âœ… å›¾ç‰‡æ•°: {len(final_schema.image_chunks)}")
        print(f"âœ… è¡¨æ ¼æ•°: {len(final_schema.table_chunks)}")
        
        # æ£€æŸ¥ç»“æ„åŒ–æè¿°ç”Ÿæˆæƒ…å†µ
        structured_images = sum(1 for img in final_schema.image_chunks if img.search_summary)
        structured_tables = sum(1 for table in final_schema.table_chunks if table.search_summary)
        
        print(f"âœ… ç»“æ„åŒ–å›¾ç‰‡æè¿°: {structured_images}/{len(final_schema.image_chunks)}")
        print(f"âœ… ç»“æ„åŒ–è¡¨æ ¼æè¿°: {structured_tables}/{len(final_schema.table_chunks)}")
        
        # éªŒè¯å¤„ç†çŠ¶æ€
        if final_schema.processing_status:
            print(f"âœ… å¤„ç†é˜¶æ®µ: {final_schema.processing_status.current_stage}")
            print(f"âœ… å®Œæˆåº¦: {final_schema.processing_status.completion_percentage}%")
        
        # éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
        print(f"\nğŸ” æ–‡ä»¶å®Œæ•´æ€§éªŒè¯:")
        if os.path.exists(output_path):
            file_size = os.path.getsize(output_path)
            print(f"âœ… final_metadata.json: {file_size:,} bytes")
            
            # å°è¯•é‡æ–°åŠ è½½éªŒè¯
            try:
                reloaded_schema = FinalMetadataSchema.load(output_path)
                print(f"âœ… JSONé‡æ–°åŠ è½½æˆåŠŸ")
                print(f"   - {len(reloaded_schema.image_chunks)}ä¸ªå›¾ç‰‡")
                print(f"   - {len(reloaded_schema.table_chunks)}ä¸ªè¡¨æ ¼")
                print(f"   - {len(reloaded_schema.text_chunks)}ä¸ªæ–‡æœ¬åˆ†å—")
            except Exception as e:
                print(f"âŒ JSONé‡æ–°åŠ è½½å¤±è´¥: {e}")
                return False
        else:
            print(f"âŒ è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨: {output_path}")
            return False
        
        print(f"\nâœ… Stage2å®Œæ•´æµ‹è¯•é€šè¿‡!")
        return True
        
    except Exception as e:
        print(f"âŒ Stage2å¤„ç†å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def test_step_by_step():
    """åˆ†æ­¥æµ‹è¯•å„ä¸ªStage2æ­¥éª¤"""
    print("\nğŸ” åˆ†æ­¥æµ‹è¯•Stage2å„ä¸ªæ­¥éª¤...")
    
    metadata_path = "parser_output_v2/test_stage1_20250716_110158/final_metadata.json"
    
    if not os.path.exists(metadata_path):
        print(f"âŒ æ‰¾ä¸åˆ°æµ‹è¯•æ–‡ä»¶: {metadata_path}")
        return False
    
    try:
        # åŠ è½½æ•°æ®
        final_schema = FinalMetadataSchema.load(metadata_path)
        processor = Stage2IntelligentProcessor()
        
        print(f"ğŸ“– åŸå§‹æ•°æ®çŠ¶æ€:")
        print(f"   - é¡µé¢æ–‡æœ¬: {len(final_schema.document_summary.page_texts)}é¡µ" if final_schema.document_summary and final_schema.document_summary.page_texts else "   - é¡µé¢æ–‡æœ¬: æ— ")
        print(f"   - å›¾ç‰‡: {len(final_schema.image_chunks)}ä¸ª")
        print(f"   - è¡¨æ ¼: {len(final_schema.table_chunks)}ä¸ª")
        
        # Step 2.1 æµ‹è¯•
        print(f"\nğŸ”§ æµ‹è¯•Step 2.1: é¡µé¢æ–‡æœ¬ä¿®å¤...")
        processor._process_step_2_1_text_repair(final_schema)
        cleaned_count = len(final_schema.document_summary.cleaned_page_texts) if final_schema.document_summary and final_schema.document_summary.cleaned_page_texts else 0
        print(f"   âœ… ä¿®å¤é¡µé¢æ•°: {cleaned_count}")
        
        # Step 2.2 æµ‹è¯•
        print(f"\nğŸ—ºï¸ æµ‹è¯•Step 2.2: å…¨å±€ç»“æ„è¯†åˆ«...")
        processor._process_step_2_2_toc_extraction(final_schema)
        toc_count = len(final_schema.document_summary.metadata.get('toc', [])) if final_schema.document_summary and final_schema.document_summary.metadata else 0
        print(f"   âœ… è¯†åˆ«ç« èŠ‚æ•°: {toc_count}")
        
        # Step 2.3 æµ‹è¯•
        print(f"\nâœ‚ï¸ æµ‹è¯•Step 2.3: å†…å®¹æå–ä¸åˆ‡åˆ†...")
        original_chunks = len(final_schema.text_chunks)
        processor._process_step_2_3_content_chunking(final_schema)
        new_chunks = len(final_schema.text_chunks) - original_chunks
        print(f"   âœ… æ–°å¢åˆ†å—æ•°: {new_chunks}")
        
        # Step 2.4 æµ‹è¯•
        print(f"\nğŸ–¼ï¸ æµ‹è¯•Step 2.4: å¤šæ¨¡æ€æè¿°ç”Ÿæˆ...")
        processor._process_step_2_4_multimodal(final_schema)
        structured_images = sum(1 for img in final_schema.image_chunks if img.search_summary)
        print(f"   âœ… ç»“æ„åŒ–å›¾ç‰‡æè¿°: {structured_images}/{len(final_schema.image_chunks)}")
        
        print(f"\nâœ… åˆ†æ­¥æµ‹è¯•å®Œæˆ!")
        return True
        
    except Exception as e:
        print(f"âŒ åˆ†æ­¥æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶"""
    print("ğŸš€ PDF Processing V2 - Stage2æ™ºèƒ½å¤„ç†å™¨æµ‹è¯•")
    print("=" * 60)
    print("ğŸ“‹ æµ‹è¯•å†…å®¹:")
    print("   - Step 2.1: é¡µé¢æ–‡æœ¬ä¿®å¤ (ä¿®æ­£OCRé”™è¯¯)")
    print("   - Step 2.2: å…¨å±€ç»“æ„è¯†åˆ« (TOCæå–)")
    print("   - Step 2.3: å†…å®¹æå–ä¸åˆ‡åˆ† (ç« èŠ‚åˆ†å—)")
    print("   - Step 2.4: å¤šæ¨¡æ€æè¿°ç”Ÿæˆ (å›¾ç‰‡è¡¨æ ¼æè¿°)")
    print()
    
    # è¿è¡Œå®Œæ•´æµ‹è¯•
    success = test_stage2_complete()
    
    if success:
        print("\n" + "="*50)
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("ğŸ’¡ Stage2æ™ºèƒ½å¤„ç†å™¨åŠŸèƒ½æ­£å¸¸")
        print("ğŸ“‹ ä¸‹ä¸€æ­¥: å¯ä»¥å¼€å§‹Stage3çš„è®¾è®¡å’Œå®ç°")
        print("ğŸ¯ Stage3å†…å®¹: é«˜çº§å†…å®¹åˆ†å—ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰")
    else:
        print("\n" + "="*50)
        print("âŒ æµ‹è¯•å¤±è´¥ï¼")
        print("ğŸ’¡ å»ºè®®: æ£€æŸ¥é”™è¯¯ä¿¡æ¯ï¼Œä¿®å¤é—®é¢˜åé‡æ–°æµ‹è¯•")
    
        # è¿è¡Œåˆ†æ­¥æµ‹è¯•å¸®åŠ©è¯Šæ–­é—®é¢˜
        print("\nğŸ” è¿è¡Œåˆ†æ­¥æµ‹è¯•è¿›è¡Œè¯Šæ–­...")
        test_step_by_step()


if __name__ == "__main__":
    main() 