#!/usr/bin/env python3
"""
Stage 1 Processor: Doclingè§£æ + åˆå§‹Schemaå¡«å……

è´Ÿè´£ä½¿ç”¨Doclingè§£æPDFï¼Œæå–æ–‡æœ¬ã€å›¾ç‰‡ã€è¡¨æ ¼ï¼Œå¹¶åˆå§‹åŒ–Final Schema
è¿™æ˜¯æ•´ä¸ªé‡æ„pipelineçš„ç¬¬ä¸€é˜¶æ®µï¼Œä¸ºåç»­é˜¶æ®µæä¾›åŸºç¡€æ•°æ®

V2ç‰ˆæœ¬ï¼šé‡‡ç”¨é¡µé¢åˆ‡å‰²æ–¹å¼ï¼Œç¡®ä¿é¡µç å‡†ç¡®ï¼Œæ”¯æŒå¹¶è¡Œå¤„ç†
"""

import os
import sys
import time
import tempfile
import shutil
import multiprocessing
from typing import Dict, List, Tuple, Optional, Any
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed

# å¯¼å…¥PDFå¤„ç†åº“
try:
    import fitz  # PyMuPDF
    FITZ_AVAILABLE = True
    print("âœ… PyMuPDF (fitz) å¯ç”¨")
except ImportError:
    FITZ_AVAILABLE = False
    print("âŒ PyMuPDF (fitz) ä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install PyMuPDF")

# å¯¼å…¥doclingç»„ä»¶
try:
    from docling.datamodel.base_models import InputFormat
    from docling.datamodel.pipeline_options import PdfPipelineOptions, EasyOcrOptions
    from docling.document_converter import DocumentConverter, PdfFormatOption
    DOCLING_AVAILABLE = True
    print("âœ… Doclingç»„ä»¶å¯ç”¨")
except ImportError as e:
    DOCLING_AVAILABLE = False
    print(f"âŒ Doclingç»„ä»¶ä¸å¯ç”¨: {e}")

# å¯¼å…¥V1ç‰ˆæœ¬çš„é…ç½®å’Œæ•°æ®æ¨¡å‹
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from pdf_processing.config import PDFProcessingConfig
from pdf_processing.data_models import PageData, ImageWithContext, TableWithContext

# å¯¼å…¥V2ç‰ˆæœ¬çš„Schema
from .final_schema import FinalMetadataSchema, DocumentSummary, ImageChunk, TableChunk


class Stage1DoclingProcessor:
    """
    é˜¶æ®µ1å¤„ç†å™¨ï¼šDoclingè§£æ + åˆå§‹Schemaå¡«å……
    
    V2ç‰ˆæœ¬ä¸»è¦åŠŸèƒ½ï¼š
    1. å°†PDFæŒ‰é¡µé¢åˆ‡å‰²ä¸ºå•é¡µPDFæ–‡ä»¶
    2. å¹¶è¡Œå¤„ç†æ¯é¡µï¼Œç¡®ä¿é¡µç å‡†ç¡®
    3. ç”Ÿæˆfull_raw_textå¹¶æ’å…¥å›¾ç‰‡è¡¨æ ¼æ ‡è®°
    4. åˆå§‹åŒ–FinalMetadataSchemaå¹¶å¡«å……åŸºç¡€ä¿¡æ¯
    5. ä¿å­˜åˆå§‹åŒ–çš„final_metadata.json
    """
    
    def __init__(self, config: Optional[PDFProcessingConfig] = None, use_process_pool: bool = True):
        """
        åˆå§‹åŒ–å¤„ç†å™¨
        
        Args:
            config: PDFå¤„ç†é…ç½®ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
            use_process_pool: æ˜¯å¦ä½¿ç”¨è¿›ç¨‹æ± ï¼ˆæ¨èç”¨äºCPUå¯†é›†å‹ä»»åŠ¡ï¼‰
            
        ğŸ›ï¸ å…³äº use_process_pool å‚æ•°çš„é€‰æ‹©æŒ‡å—ï¼š
        
        âœ… use_process_pool=True (æ¨èï¼Œé»˜è®¤é€‰æ‹©)ï¼š
        ğŸ“ˆ æ€§èƒ½ä¼˜åŠ¿ï¼š
        - çœŸæ­£çš„å¹¶è¡Œè®¡ç®—ï¼Œå¯ä»¥100%åˆ©ç”¨å¤šæ ¸CPU
        - ç»•è¿‡Python GILé™åˆ¶ï¼Œæ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹è¿è¡Œ
        - ç†è®ºåŠ é€Ÿæ¯”æ¥è¿‘è¿›ç¨‹æ•°ï¼ˆ5è¿›ç¨‹â‰ˆ5å€é€Ÿåº¦ï¼‰
        
        ğŸ›¡ï¸ ç¨³å®šæ€§ä¼˜åŠ¿ï¼š
        - è¿›ç¨‹éš”ç¦»ï¼šä¸€ä¸ªé¡µé¢å´©æºƒä¸å½±å“å…¶ä»–é¡µé¢
        - å†…å­˜ç‹¬ç«‹ï¼šé¿å…å†…å­˜æ³„æ¼ç´¯ç§¯
        - æ•…éšœæ¢å¤ï¼šå•ä¸ªè¿›ç¨‹å¤±è´¥åä¼šè‡ªåŠ¨é‡å¯
        
        âš ï¸ æ³¨æ„äº‹é¡¹ï¼š
        - è¿›ç¨‹å¯åŠ¨æœ‰å¼€é”€ï¼ˆçº¦1-2ç§’ï¼‰
        - å†…å­˜ä½¿ç”¨ç¨é«˜ï¼ˆæ¯è¿›ç¨‹ç‹¬ç«‹å†…å­˜ç©ºé—´ï¼‰
        - éœ€è¦è¶³å¤Ÿçš„ç³»ç»Ÿèµ„æº
        
        ğŸ§µ use_process_pool=False (çº¿ç¨‹æ± æ¨¡å¼)ï¼š
        ğŸ“‹ é€‚ç”¨åœºæ™¯ï¼š
        - ç³»ç»Ÿèµ„æºå—é™ï¼ˆå†…å­˜ä¸è¶³ã€CPUæ ¸å¿ƒå°‘ï¼‰
        - éœ€è¦é¢‘ç¹å…±äº«æ•°æ®
        - ä»»åŠ¡è¾ƒè½»é‡ï¼Œè¿›ç¨‹å¯åŠ¨å¼€é”€å¤ªå¤§
        
        âš ï¸ æ€§èƒ½é™åˆ¶ï¼š
        - å—Python GILé™åˆ¶ï¼ŒCPUå¯†é›†å‹ä»»åŠ¡å¯èƒ½æ— æ³•å¹¶è¡Œ
        - ä½†doclingæœ‰Cæ‰©å±•ï¼Œéƒ¨åˆ†æ“ä½œå¯ä»¥é‡Šæ”¾GIL
        - é€‚åˆI/Oå¯†é›†å‹ä»»åŠ¡ï¼ˆæ–‡ä»¶è¯»å†™ã€ç½‘ç»œè¯·æ±‚ï¼‰
        
        ğŸ¯ å¦‚ä½•é€‰æ‹©ï¼Ÿ
        - å¦‚æœä½ çš„ç”µè„‘æœ‰4æ ¸ä»¥ä¸ŠCPUï¼šé€‰æ‹©è¿›ç¨‹æ± 
        - å¦‚æœå¤„ç†å¤§å‹PDFï¼ˆ>20é¡µï¼‰ï¼šé€‰æ‹©è¿›ç¨‹æ± 
        - å¦‚æœç”µè„‘æ€§èƒ½è¾ƒå¼±æˆ–å†…å­˜ç´§å¼ ï¼šé€‰æ‹©çº¿ç¨‹æ± 
        - å¦‚æœä¸ç¡®å®šï¼šä½¿ç”¨é»˜è®¤çš„è¿›ç¨‹æ± æ¨¡å¼
        """
        self.config = config or PDFProcessingConfig.from_env()
        self.use_process_pool = use_process_pool
        self.doc_converter = None
        
        if not FITZ_AVAILABLE:
            raise RuntimeError("PyMuPDFä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œé¡µé¢åˆ†å‰²")
        
        if not DOCLING_AVAILABLE:
            raise RuntimeError("Doclingä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œé¡µé¢å¤„ç†")
        
        self._init_docling_converter()
        
        # ğŸ’¡ æ™ºèƒ½æ¨¡å¼æç¤º
        cpu_cores = multiprocessing.cpu_count()
        if use_process_pool:
            print(f"ğŸ­ å·²é€‰æ‹©è¿›ç¨‹æ± æ¨¡å¼ï¼ˆé€‚åˆ{cpu_cores}æ ¸CPUçš„å¹¶è¡Œè®¡ç®—ï¼‰")
            if cpu_cores < 4:
                print(f"ğŸ’¡ æç¤ºï¼šä½ çš„CPUåªæœ‰{cpu_cores}æ ¸ï¼Œè€ƒè™‘ä½¿ç”¨çº¿ç¨‹æ± å¯èƒ½æ›´åˆé€‚")
        else:
            print(f"ğŸ§µ å·²é€‰æ‹©çº¿ç¨‹æ± æ¨¡å¼ï¼ˆè½»é‡çº§å¹¶å‘ï¼‰")
            if cpu_cores >= 8:
                print(f"ğŸ’¡ æç¤ºï¼šä½ çš„CPUæœ‰{cpu_cores}æ ¸ï¼Œä½¿ç”¨è¿›ç¨‹æ± å¯èƒ½è·å¾—æ›´å¥½æ€§èƒ½")
                
        print("âœ… Stage1DoclingProcessor V2 åˆå§‹åŒ–å®Œæˆ")
    
    def _init_docling_converter(self) -> None:
        """åˆå§‹åŒ–Doclingè½¬æ¢å™¨"""
        try:
            # è®¾ç½®æœ¬åœ°æ¨¡å‹è·¯å¾„
            models_cache_dir = Path("models_cache")
            artifacts_path = None
            if models_cache_dir.exists():
                artifacts_path = str(models_cache_dir.absolute())
            elif self.config.docling.artifacts_path:
                artifacts_path = self.config.docling.artifacts_path
            
            # ğŸ”§ é…ç½®ç¦»çº¿æ¨¡å¼ï¼Œé¿å…ç½‘ç»œè¿æ¥é—®é¢˜
            import os
            os.environ['HF_HUB_OFFLINE'] = '1'  # å¼ºåˆ¶HuggingFace Hubç¦»çº¿æ¨¡å¼
            os.environ['TRANSFORMERS_OFFLINE'] = '1'  # Transformersç¦»çº¿æ¨¡å¼
            print("ğŸ”’ å·²å¯ç”¨ç¦»çº¿æ¨¡å¼ï¼Œé¿å…ç½‘ç»œè¿æ¥é—®é¢˜")
            
            # åˆ›å»ºç®¡é“é€‰é¡¹
            if self.config.docling.ocr_enabled:
                # ğŸ”¥ å¢å¼ºOCRé…ç½®ï¼Œç¡®ä¿æ–‡å­—æå–æˆåŠŸ
                ocr_options = EasyOcrOptions()
                try:
                    # æ ¸å¿ƒOCRè®¾ç½®
                    ocr_options.force_full_page_ocr = True  # å¼ºåˆ¶å…¨é¡µOCR
                    
                    # ğŸš€ æ·»åŠ æ›´å¤šOCRä¼˜åŒ–è®¾ç½®ï¼ˆåªä½¿ç”¨å®é™…å­˜åœ¨çš„å±æ€§ï¼‰
                    if hasattr(ocr_options, 'use_gpu'):
                        ocr_options.use_gpu = False  # å¼ºåˆ¶ä½¿ç”¨CPUï¼Œé¿å…GPUå…¼å®¹æ€§é—®é¢˜
                    
                    if hasattr(ocr_options, 'lang'):
                        ocr_options.lang = ['ch_sim', 'en']  # æ”¯æŒä¸­è‹±æ–‡
                    
                    if hasattr(ocr_options, 'confidence_threshold'):
                        ocr_options.confidence_threshold = 0.3  # é™ä½ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œæé«˜æ£€å‡ºç‡
                    
                    if hasattr(ocr_options, 'bitmap_area_threshold'):
                        ocr_options.bitmap_area_threshold = 0.01  # é™ä½åŒºåŸŸé˜ˆå€¼ï¼Œæ£€æµ‹æ›´å°æ–‡å­—
                    
                    print("âœ… åº”ç”¨å¢å¼ºOCRè®¾ç½®ï¼š")
                    print("   - force_full_page_ocr=Trueï¼ˆå¼ºåˆ¶å…¨é¡µOCRï¼‰")
                    print("   - use_gpu=Falseï¼ˆCPUæ¨¡å¼ï¼Œé¿å…å…¼å®¹æ€§é—®é¢˜ï¼‰")
                    print("   - lang=['ch_sim', 'en']ï¼ˆä¸­è‹±æ–‡æ”¯æŒï¼‰")
                    print("   - confidence_threshold=0.3ï¼ˆé™ä½ç½®ä¿¡åº¦é˜ˆå€¼ï¼‰")
                    print("   - bitmap_area_threshold=0.01ï¼ˆæ£€æµ‹æ›´å°æ–‡å­—ï¼‰")
                    
                except Exception as e:
                    print(f"âš ï¸ åº”ç”¨å¢å¼ºOCRè®¾ç½®å¤±è´¥: {e}")
                
                pipeline_options = PdfPipelineOptions(
                    ocr_options=ocr_options,
                    artifacts_path=artifacts_path
                )
            else:
                print("âš ï¸ OCRå·²ç¦ç”¨ï¼Œå¯èƒ½å¯¼è‡´æ–‡å­—æå–ä¸å®Œæ•´")
                pipeline_options = PdfPipelineOptions(
                    artifacts_path=artifacts_path
                )
            
            # è®¾ç½®è§£æé€‰é¡¹
            pipeline_options.images_scale = self.config.docling.images_scale
            pipeline_options.generate_page_images = self.config.docling.generate_page_images
            pipeline_options.generate_picture_images = self.config.docling.generate_picture_images
            
            # ğŸš€ æ·»åŠ æ›´å¤šè§£æé€‰é¡¹
            if hasattr(pipeline_options, 'do_ocr'):
                pipeline_options.do_ocr = True  # ç¡®ä¿OCRæ‰§è¡Œ
            
            if hasattr(pipeline_options, 'do_table_structure'):
                pipeline_options.do_table_structure = True  # è¡¨æ ¼ç»“æ„è¯†åˆ«
            
            # åˆ›å»ºæ–‡æ¡£è½¬æ¢å™¨
            self.doc_converter = DocumentConverter(
                format_options={
                    InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
                }
            )
            
            print("âœ… Doclingè½¬æ¢å™¨åˆå§‹åŒ–æˆåŠŸï¼ˆå¢å¼ºOCRé…ç½®ï¼‰")
            
        except Exception as e:
            print(f"âŒ Doclingè½¬æ¢å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.doc_converter = None
    
    def process(self, pdf_path: str, output_dir: str) -> Tuple[FinalMetadataSchema, str]:
        """
        æ‰§è¡Œé˜¶æ®µ1å¤„ç†
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            Tuple[FinalMetadataSchema, str]: (final_schemaå¯¹è±¡, final_metadata.jsonæ–‡ä»¶è·¯å¾„)
        """
        print(f"ğŸš€ å¼€å§‹é˜¶æ®µ1å¤„ç†ï¼ˆé¡µé¢åˆ‡å‰²ç‰ˆæœ¬ï¼‰: {pdf_path}")
        stage_start_time = time.time()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # åˆå§‹åŒ–Final Schema
        final_schema = FinalMetadataSchema()
        final_schema.update_processing_status("stage1_started", 10)
        
        try:
            # æ­¥éª¤1: åˆ†å‰²PDFä¸ºå•é¡µæ–‡ä»¶
            print("ğŸ“„ æ­¥éª¤1: åˆ†å‰²PDFä¸ºå•é¡µæ–‡ä»¶")
            single_page_files = self._split_pdf_to_pages(pdf_path)
            print(f"ğŸ“„ PDFåˆ†å‰²å®Œæˆï¼Œå…± {len(single_page_files)} é¡µ")
            final_schema.update_processing_status("pdf_split", 15)
            
            # æ­¥éª¤2: å¹¶è¡Œå¤„ç†æ‰€æœ‰é¡µé¢
            print("ğŸ”„ æ­¥éª¤2: å¹¶è¡Œå¤„ç†æ‰€æœ‰é¡µé¢")
            if self.config.media_extractor.parallel_processing:
                pages_data = self._process_pages_parallel(single_page_files, output_dir)
            else:
                pages_data = self._process_pages_sequential(single_page_files, output_dir)
            final_schema.update_processing_status("pages_processed", 25)
            
            # æ­¥éª¤3: æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            print("ğŸ§¹ æ­¥éª¤3: æ¸…ç†ä¸´æ—¶æ–‡ä»¶")
            self._cleanup_temp_files(single_page_files)
            
            # æ­¥éª¤4: ç”Ÿæˆfull_raw_textå¹¶æ’å…¥åª’ä½“æ ‡è®°
            print("ğŸ“ æ­¥éª¤4: ç”Ÿæˆfull_raw_text")
            full_raw_text, page_texts = self._generate_full_raw_text_with_media_markers(pages_data)
            final_schema.update_processing_status("full_text_generated", 27)
            
            # æ­¥éª¤5: å¡«å……Final Schema
            print("ğŸ“‹ æ­¥éª¤5: å¡«å……Final Schema")
            self._populate_final_schema(
                final_schema, pdf_path, full_raw_text, page_texts, pages_data, stage_start_time
            )
            final_schema.update_processing_status("stage1_completed", 30)
            
            # æ­¥éª¤6: ä¿å­˜Final Schema
            final_metadata_path = os.path.join(output_dir, "final_metadata.json")
            final_schema.save(final_metadata_path)
            
            stage_duration = time.time() - stage_start_time
            print(f"âœ… é˜¶æ®µ1å¤„ç†å®Œæˆï¼Œè€—æ—¶: {stage_duration:.2f} ç§’")
            print(f"ğŸ“ ä¿å­˜ä½ç½®: {final_metadata_path}")
            print(f"ğŸ“Š åˆå§‹å¡«å……å®Œæˆåº¦: {final_schema.get_completion_percentage()}%")
            
            return final_schema, final_metadata_path
            
        except Exception as e:
            # ç¡®ä¿æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if 'single_page_files' in locals():
                self._cleanup_temp_files(single_page_files)
            
            error_msg = f"é˜¶æ®µ1å¤„ç†å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            final_schema.update_processing_status("stage1_failed", 0, error_msg)
            raise RuntimeError(error_msg) from e
    
    def _split_pdf_to_pages(self, pdf_path: str) -> List[Tuple[int, str]]:
        """
        å°†PDFåˆ†å‰²ä¸ºå•é¡µæ–‡ä»¶
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            
        Returns:
            List[Tuple[int, str]]: [(é¡µç , å•é¡µPDFæ–‡ä»¶è·¯å¾„), ...]
        """
        doc = fitz.open(pdf_path)
        total_pages = len(doc)
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = tempfile.mkdtemp(prefix="pdf_pages_")
        single_page_files = []
        
        try:
            for page_num in range(total_pages):
                # åˆ›å»ºæ–°çš„PDFæ–‡æ¡£ï¼ŒåªåŒ…å«å½“å‰é¡µ
                single_page_doc = fitz.open()
                single_page_doc.insert_pdf(doc, from_page=page_num, to_page=page_num)
                
                # ä¿å­˜å•é¡µPDF
                single_page_path = os.path.join(temp_dir, f"page_{page_num + 1}.pdf")
                single_page_doc.save(single_page_path)
                single_page_doc.close()
                
                single_page_files.append((page_num + 1, single_page_path))
                
        finally:
            doc.close()
        
        return single_page_files
    
    def _get_optimal_worker_count(self, total_tasks: int) -> int:
        """
        æ ¹æ®ä»»åŠ¡æ•°é‡å’Œç³»ç»Ÿé…ç½®è®¡ç®—æœ€ä¼˜workeræ•°é‡
        
        ğŸ¤” ä»€ä¹ˆæ˜¯è¿›ç¨‹æ± å’Œçº¿ç¨‹æ± ï¼Ÿ
        
        ğŸ“š ç§‘æ™®æ—¶é—´ï¼š
        - è¿›ç¨‹æ±  (ProcessPoolExecutor)ï¼š
          ğŸ­ æƒ³è±¡æˆä¸€ä¸ªå·¥å‚ï¼Œæ¯ä¸ªå·¥äººï¼ˆè¿›ç¨‹ï¼‰éƒ½æœ‰ç‹¬ç«‹çš„å·¥ä½œå°å’Œå·¥å…·
          ğŸ’ª ä¼˜ç‚¹ï¼šæ¯ä¸ªå·¥äººå®Œå…¨ç‹¬ç«‹ï¼Œä¸€ä¸ªå‡ºé—®é¢˜ä¸å½±å“å…¶ä»–äººï¼Œèƒ½å……åˆ†åˆ©ç”¨å¤šæ ¸CPU
          ğŸŒ ç¼ºç‚¹ï¼šå¯åŠ¨å·¥äººéœ€è¦æ—¶é—´ï¼Œå·¥äººä¹‹é—´äº¤æµæ¯”è¾ƒéº»çƒ¦ï¼ˆéœ€è¦é€šè¿‡æ–‡ä»¶/ç®¡é“ï¼‰
          
        - çº¿ç¨‹æ±  (ThreadPoolExecutor)ï¼š
          ğŸ¢ æƒ³è±¡æˆä¸€ä¸ªå¼€æ”¾å¼åŠå…¬å®¤ï¼Œæ‰€æœ‰å‘˜å·¥ï¼ˆçº¿ç¨‹ï¼‰å…±äº«åŠå…¬è®¾å¤‡å’Œèµ„æ–™
          âš¡ ä¼˜ç‚¹ï¼šå¯åŠ¨å¿«ï¼Œå‘˜å·¥ä¹‹é—´äº¤æµæ–¹ä¾¿ï¼ˆå…±äº«å†…å­˜ï¼‰
          âš ï¸ ç¼ºç‚¹ï¼šä¸€ä¸ªå‘˜å·¥å‡ºå¤§é—®é¢˜å¯èƒ½å½±å“æ•´ä¸ªåŠå…¬å®¤ï¼Œå—Python GILé™åˆ¶
        
        ğŸ¯ å¯¹äºæˆ‘ä»¬çš„PDFå¤„ç†ä»»åŠ¡ï¼š
        - Doclingæ˜¯CPUå¯†é›†å‹ä»»åŠ¡ï¼ˆå¤§é‡å›¾åƒè¯†åˆ«ã€æ–‡å­—æå–ï¼‰
        - é€‰æ‹©è¿›ç¨‹æ± å¯ä»¥ç»•è¿‡Pythonçš„GILé™åˆ¶ï¼ŒçœŸæ­£å®ç°å¹¶è¡Œè®¡ç®—
        
        Args:
            total_tasks: æ€»ä»»åŠ¡æ•°é‡
            
        Returns:
            int: æœ€ä¼˜workeræ•°é‡
        """
        if self.use_process_pool:
            # ğŸ”¥ ä¸ºä»€ä¹ˆCPUæ ¸å¿ƒæ•°å¾ˆé‡è¦ï¼Ÿ
            # 
            # ğŸ’» ä½ çš„ç”µè„‘CPUæ ¸å¿ƒæƒ…å†µï¼š
            # - æ¯ä¸ªCPUæ ¸å¿ƒåŒæ—¶åªèƒ½æ‰§è¡Œä¸€ä¸ªè¿›ç¨‹çš„è®¡ç®—
            # - å¦‚æœè¿›ç¨‹æ•° > æ ¸å¿ƒæ•°ï¼Œä¼šå¯¼è‡´"æŠ¢å¤ºèµ„æº"ï¼Œåè€Œå˜æ…¢
            # 
            # âš ï¸ ä¸ºä»€ä¹ˆcpu_count-1ä¼šè®©ç”µè„‘"æ‹‰æ»¡"ï¼Ÿ
            # - å‡è®¾ä½ æœ‰8æ ¸CPUï¼Œå¼€8ä¸ªè¿›ç¨‹æ„å‘³ç€100%å ç”¨æ‰€æœ‰æ ¸å¿ƒ
            # - æ“ä½œç³»ç»Ÿã€æµè§ˆå™¨ã€å…¶ä»–è½¯ä»¶éƒ½æ²¡æœ‰CPUèµ„æºäº†
            # - ç”µè„‘ä¼šå˜å¾—å¡é¡¿ï¼Œé£æ‰‡ç‹‚è½¬ï¼Œæ¸©åº¦é£™å‡
            # 
            # ğŸ¯ åˆç†çš„è®¾ç½®ç­–ç•¥ï¼š
            # - ä½¿ç”¨æ ¸å¿ƒæ•°çš„ä¸€åŠï¼šç»™ç³»ç»Ÿå’Œå…¶ä»–ç¨‹åºç•™ç©ºé—´
            # - è¿™æ ·æ—¢èƒ½äº«å—å¹¶è¡Œå¤„ç†çš„é€Ÿåº¦æå‡ï¼Œåˆä¸ä¼šè®©ç”µè„‘"æ­»æœº"
            
            cpu_count = multiprocessing.cpu_count()
            # ğŸ“Š CPUè´Ÿè½½å½±å“åˆ†æï¼š
            # - 1æ ¸ï¼šé€‚åˆè½»é‡ä»»åŠ¡ï¼Œä½†é€Ÿåº¦æ…¢
            # - æ ¸å¿ƒæ•°/4ï¼šä¿å®ˆè®¾ç½®ï¼Œé€‚åˆåœ¨å·¥ä½œæ—¶åå°è¿è¡Œ
            # - æ ¸å¿ƒæ•°/2ï¼šå¹³è¡¡è®¾ç½®ï¼Œæ—¢å¿«åˆä¸å½±å“å…¶ä»–å·¥ä½œ
            # - æ ¸å¿ƒæ•°-1ï¼šæ¿€è¿›è®¾ç½®ï¼Œå‡ ä¹100%CPUä½¿ç”¨ï¼Œç”µè„‘å¯èƒ½å¡é¡¿
            # - æ ¸å¿ƒæ•°ï¼šå±é™©è®¾ç½®ï¼Œç³»ç»Ÿå¯èƒ½æ— å“åº”
            max_workers = max(1, cpu_count // 2)
            
            print(f"ğŸ§  CPUåˆ†æï¼šä½ çš„ç”µè„‘æœ‰{cpu_count}ä¸ªæ ¸å¿ƒ")
            print(f"âš–ï¸ ä¸ºäº†å¹³è¡¡æ€§èƒ½å’Œç¨³å®šæ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨{max_workers}ä¸ªè¿›ç¨‹")
            print(f"ğŸ“ˆ è¿™æ ·èƒ½æå‡çº¦{max_workers}å€çš„å¤„ç†é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒç”µè„‘å“åº”æµç•…")
            
        else:
            # ğŸ§µ çº¿ç¨‹æ± çš„ç‰¹æ®Šè€ƒè™‘ï¼š
            # 
            # ğŸ Pythonçš„GILï¼ˆå…¨å±€è§£é‡Šå™¨é”ï¼‰é™åˆ¶ï¼š
            # - å³ä½¿å¼€100ä¸ªçº¿ç¨‹ï¼ŒCPUå¯†é›†å‹ä»»åŠ¡å®é™…ä¸Šè¿˜æ˜¯ä¸²è¡Œæ‰§è¡Œ
            # - ä½†å¯¹äºI/Oå¯†é›†å‹ä»»åŠ¡ï¼ˆç½‘ç»œè¯·æ±‚ã€æ–‡ä»¶è¯»å†™ï¼‰ï¼Œçº¿ç¨‹æ± å¾ˆæœ‰ç”¨
            # - Doclingè™½ç„¶æ˜¯CPUå¯†é›†ï¼Œä½†ä¸­é—´æœ‰å¾ˆå¤šæ–‡ä»¶è¯»å†™æ“ä½œ
            # 
            # ğŸ’¡ ä¸ºä»€ä¹ˆé™åˆ¶ä¸º8ä¸ªçº¿ç¨‹ï¼Ÿ
            # - çº¿ç¨‹å¤ªå¤šä¼šå¢åŠ ä¸Šä¸‹æ–‡åˆ‡æ¢çš„å¼€é”€
            # - å¯¹äºæ··åˆå‹ä»»åŠ¡ï¼Œ8ä¸ªçº¿ç¨‹é€šå¸¸æ˜¯ç”œç‚¹
            max_workers = min(8, multiprocessing.cpu_count())
            
            print(f"ğŸ§µ ä½¿ç”¨çº¿ç¨‹æ± æ¨¡å¼ï¼Œåˆ›å»º{max_workers}ä¸ªçº¿ç¨‹")
            print(f"ğŸ’­ æ³¨æ„ï¼šç”±äºPython GILé™åˆ¶ï¼Œå®é™…CPUåˆ©ç”¨ç‡å¯èƒ½ä¸ä¼šæ»¡è½½")
        
        # ğŸ“ æœ€ç»ˆä¼˜åŒ–ï¼šä¸è¦åˆ›å»ºæ¯”ä»»åŠ¡æ›´å¤šçš„worker
        # å¦‚æœåªæœ‰3é¡µPDFï¼Œå¼€10ä¸ªè¿›ç¨‹å°±æ˜¯æµªè´¹èµ„æº
        optimal_workers = min(max_workers, total_tasks)
        
        print(f"ğŸ’» ç³»ç»Ÿé…ç½®: CPUæ ¸å¿ƒæ•°={multiprocessing.cpu_count()}, ä½¿ç”¨{'è¿›ç¨‹æ± ' if self.use_process_pool else 'çº¿ç¨‹æ± '}")
        print(f"âš™ï¸ å·¥ä½œè¿›ç¨‹/çº¿ç¨‹æ•°: {optimal_workers} (æ€»ä»»åŠ¡: {total_tasks})")
        
        # ğŸ“ å°è´´å£«ï¼šå¦‚ä½•è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Ÿ
        if optimal_workers == 1:
            print("ğŸ’¡ æç¤ºï¼šä»»åŠ¡è¾ƒå°‘ï¼Œè€ƒè™‘ä¸²è¡Œå¤„ç†å¯èƒ½æ›´ç®€å•")
        elif self.use_process_pool and optimal_workers >= multiprocessing.cpu_count() * 0.75:
            print("âš ï¸ æç¤ºï¼šä½¿ç”¨äº†è¾ƒå¤šCPUèµ„æºï¼Œå¤„ç†æœŸé—´ç”µè„‘å¯èƒ½ä¼šæ¯”è¾ƒå¡")
        
        # ğŸ›¡ï¸ å®‰å…¨æé†’ï¼šæœ€å¤§è®¾ç½®å»ºè®®
        max_safe_workers = multiprocessing.cpu_count() - 1
        if optimal_workers > max_safe_workers:
            print(f"ğŸš¨ è­¦å‘Šï¼šworkeræ•°é‡({optimal_workers})æ¥è¿‘CPUæ ¸å¿ƒæ•°({multiprocessing.cpu_count()})")
            print(f"ğŸ”§ å»ºè®®ï¼šæœ€å¤§ä¸è¦è¶…è¿‡{max_safe_workers}ä¸ªworkerï¼Œå¦åˆ™ç³»ç»Ÿå¯èƒ½æ— å“åº”")
            print(f"ğŸ’Š è§£å†³æ–¹æ¡ˆï¼šå¦‚éœ€æ›´é«˜æ€§èƒ½ï¼Œè€ƒè™‘ä½¿ç”¨æ›´å¼ºçš„ç¡¬ä»¶æˆ–åˆ†æ‰¹å¤„ç†")
        
        return optimal_workers
    
    def _process_pages_parallel(self, 
                               single_page_files: List[Tuple[int, str]], 
                               output_dir: str) -> List[PageData]:
        """
        å¹¶è¡Œå¤„ç†æ‰€æœ‰é¡µé¢ï¼ˆæ”¯æŒçº¿ç¨‹æ± å’Œè¿›ç¨‹æ± ï¼‰
        
        ğŸ¯ ä¸ºä»€ä¹ˆé€‰æ‹©å¹¶è¡Œå¤„ç†ï¼Ÿ
        
        ğŸ“Š æ€§èƒ½å¯¹æ¯”ï¼ˆä»¥12é¡µPDFä¸ºä¾‹ï¼‰ï¼š
        - ä¸²è¡Œå¤„ç†ï¼š12é¡µ Ã— 10ç§’/é¡µ = 120ç§’
        - 5è¿›ç¨‹å¹¶è¡Œï¼š12é¡µ Ã· 5è¿›ç¨‹ â‰ˆ 25ç§’ï¼ˆæå‡80%ï¼‰
        - ä½†è¦è€ƒè™‘ï¼šè¿›ç¨‹å¯åŠ¨å¼€é”€ + ç³»ç»Ÿç¨³å®šæ€§
        
        ğŸ¤” ä»€ä¹ˆæ—¶å€™ç”¨è¿›ç¨‹æ±  vs çº¿ç¨‹æ± ï¼Ÿ
        
        ğŸ’¡ è¿›ç¨‹æ± é€‚åˆçš„åœºæ™¯ï¼š
        - CPUå¯†é›†å‹ä»»åŠ¡ï¼ˆå›¾åƒå¤„ç†ã€æœºå™¨å­¦ä¹ æ¨ç†ï¼‰
        - ä»»åŠ¡ä¹‹é—´ç›¸äº’ç‹¬ç«‹
        - ä¸éœ€è¦é¢‘ç¹å…±äº«æ•°æ®
        - æˆ‘ä»¬çš„PDFå¤„ç†å®Œç¾ç¬¦åˆè¿™äº›æ¡ä»¶ï¼
        
        ğŸ§µ çº¿ç¨‹æ± é€‚åˆçš„åœºæ™¯ï¼š
        - I/Oå¯†é›†å‹ä»»åŠ¡ï¼ˆç½‘ç»œè¯·æ±‚ã€æ–‡ä»¶ä¸‹è½½ï¼‰
        - éœ€è¦é¢‘ç¹å…±äº«æ•°æ®
        - ä»»åŠ¡å¯åŠ¨/ç»“æŸå¾ˆé¢‘ç¹
        """
        
        # è®¡ç®—æœ€ä¼˜workeræ•°é‡
        optimal_workers = self._get_optimal_worker_count(len(single_page_files))
        
        print(f"âš¡ å¯ç”¨å¹¶è¡Œå¤„ç†æ¨¡å¼: {'è¿›ç¨‹æ± ' if self.use_process_pool else 'çº¿ç¨‹æ± '}")
        print(f"ğŸ”§ å·¥ä½œè¿›ç¨‹/çº¿ç¨‹æ•°: {optimal_workers}")
        
        # ğŸ”„ é‡è¯•æœºåˆ¶é…ç½®
        max_retries = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
        retry_delay = 2.0  # é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
        
        print(f"ğŸ”„ é‡è¯•é…ç½®: æœ€å¤§{max_retries}æ¬¡é‡è¯•ï¼Œé—´éš”{retry_delay}ç§’")
        
        # ğŸ­ ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªå¤æ‚çš„åˆ—è¡¨ï¼Ÿ
        # - å¹¶è¡Œå¤„ç†çš„ç»“æœå¯èƒ½ä¹±åºè¿”å›ï¼ˆé¡µé¢2å¯èƒ½æ¯”é¡µé¢1å…ˆå®Œæˆï¼‰
        # - æˆ‘ä»¬éœ€è¦æŒ‰é¡µç é¡ºåºé‡æ–°æ’åˆ—ç»“æœ
        # - é¢„åˆ†é…åˆ—è¡¨ç¡®ä¿æ¯ä¸ªé¡µé¢éƒ½æœ‰å›ºå®šä½ç½®
        pages_data: List[Optional[PageData]] = [None] * len(single_page_files)
        
        # ğŸ”„ é‡è¯•é˜Ÿåˆ—ç®¡ç†
        current_batch = single_page_files.copy()  # å½“å‰å¤„ç†æ‰¹æ¬¡
        retry_count = 0
        
        # ğŸ­ é€‰æ‹©åˆé€‚çš„"å·¥å‚"ç±»å‹
        executor_class = ProcessPoolExecutor if self.use_process_pool else ThreadPoolExecutor
        
        while current_batch and retry_count <= max_retries:
            if retry_count > 0:
                print(f"ğŸ”„ ç¬¬{retry_count}æ¬¡é‡è¯•ï¼Œå¤„ç†{len(current_batch)}ä¸ªå¤±è´¥é¡µé¢...")
                if retry_count > 1:  # ç¬¬äºŒæ¬¡é‡è¯•åå¢åŠ å»¶è¿Ÿ
                    import time
                    time.sleep(retry_delay)
            
            failed_pages = []  # æœ¬è½®å¤±è´¥çš„é¡µé¢
            
            try:
                # ğŸª å¼€å§‹å¹¶è¡Œå¤„ç†çš„"é©¬æˆå›¢è¡¨æ¼”"
                with executor_class(max_workers=optimal_workers) as executor:
                    # ğŸ“‹ ä»»åŠ¡åˆ†å‘ï¼šæŠŠå·¥ä½œåˆ†é…ç»™ä¸åŒçš„worker
                    if self.use_process_pool:
                        # ğŸ­ è¿›ç¨‹æ± æ¨¡å¼ï¼š
                        # 
                        # âš ï¸ é‡è¦é™åˆ¶ï¼šè¿›ç¨‹ä¹‹é—´ä¸èƒ½ç›´æ¥å…±äº«å¯¹è±¡ï¼
                        # - ä¸èƒ½ä¼ é€’ selfï¼ˆç±»å®ä¾‹ï¼‰
                        # - å¿…é¡»ä½¿ç”¨ç‹¬ç«‹çš„é™æ€å‡½æ•°
                        # - æ‰€æœ‰å‚æ•°éƒ½è¦èƒ½"åºåˆ—åŒ–"ï¼ˆè½¬æ¢æˆäºŒè¿›åˆ¶æ•°æ®ä¼ è¾“ï¼‰
                        # 
                        # ğŸ’¾ æ•°æ®ä¼ è¾“å¼€é”€ï¼š
                        # - æ¯ä¸ªè¿›ç¨‹å¯åŠ¨æ—¶éƒ½è¦ä¼ é€’configå¯¹è±¡
                        # - è¿›ç¨‹é—´é€šä¿¡é€šè¿‡ç®¡é“/å…±äº«å†…å­˜
                        # - è¿™å°±æ˜¯ä¸ºä»€ä¹ˆè¿›ç¨‹å¯åŠ¨æ¯”çº¿ç¨‹æ…¢çš„åŸå› 
                        
                        future_to_page = {
                            executor.submit(
                                _process_single_page_static,  # ğŸ“ è°ƒç”¨ç‹¬ç«‹çš„é™æ€å‡½æ•°
                                page_num, 
                                single_page_path, 
                                output_dir,
                                self.config  # ğŸšš é…ç½®å¯¹è±¡ä¼šè¢«"æ‰“åŒ…"å‘é€ç»™å­è¿›ç¨‹
                            ): page_num
                            for page_num, single_page_path in current_batch
                        }
                        
                        print(f"ğŸš€ å·²å‘{optimal_workers}ä¸ªç‹¬ç«‹è¿›ç¨‹åˆ†å‘{len(current_batch)}ä¸ªä»»åŠ¡")
                        print(f"ğŸ’¡ æ¯ä¸ªè¿›ç¨‹éƒ½æ˜¯ç‹¬ç«‹çš„Pythonè§£é‡Šå™¨ï¼Œå¯ä»¥å……åˆ†åˆ©ç”¨CPUæ ¸å¿ƒ")
                        
                    else:
                        # ğŸ§µ çº¿ç¨‹æ± æ¨¡å¼ï¼š
                        # 
                        # âœ… ä¾¿åˆ©æ€§ï¼šå¯ä»¥ç›´æ¥è°ƒç”¨ç±»æ–¹æ³•
                        # - æ‰€æœ‰çº¿ç¨‹å…±äº«åŒä¸€ä¸ªå†…å­˜ç©ºé—´
                        # - å¯ä»¥ç›´æ¥è®¿é—® self å’Œæ‰€æœ‰å®ä¾‹å˜é‡
                        # - æ•°æ®ä¼ é€’å‡ ä¹æ²¡æœ‰å¼€é”€
                        # 
                        # âš ï¸ GILé™åˆ¶ï¼š
                        # - Pythonçš„å…¨å±€è§£é‡Šå™¨é”æ„å‘³ç€åŒæ—¶åªæœ‰ä¸€ä¸ªçº¿ç¨‹åœ¨æ‰§è¡ŒPythonä»£ç 
                        # - å¯¹äºçº¯CPUä»»åŠ¡ï¼Œå¤šçº¿ç¨‹å¯èƒ½ä¸ä¼šå¸¦æ¥é€Ÿåº¦æå‡
                        # - ä½†doclingä¸­æœ‰å¾ˆå¤šCæ‰©å±•å’ŒI/Oæ“ä½œï¼Œè¿™äº›å¯ä»¥é‡Šæ”¾GIL
                        
                        future_to_page = {
                            executor.submit(
                                self._process_single_page,  # ğŸ“ ç›´æ¥è°ƒç”¨å®ä¾‹æ–¹æ³•
                                page_num, 
                                single_page_path, 
                                output_dir
                            ): page_num
                            for page_num, single_page_path in current_batch
                        }
                        
                        print(f"ğŸ§µ å·²å‘{optimal_workers}ä¸ªçº¿ç¨‹åˆ†å‘{len(current_batch)}ä¸ªä»»åŠ¡")
                        print(f"ğŸ’­ æ‰€æœ‰çº¿ç¨‹å…±äº«å†…å­˜ï¼Œä½†å—Python GILé™åˆ¶å¯èƒ½æ— æ³•æ»¡è½½CPU")
                    
                    # ğŸ­ æ”¶é›†è¡¨æ¼”ç»“æœï¼šç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                    completed_count = 0
                    for future in as_completed(future_to_page):
                        page_num = future_to_page[future]
                        try:
                            page_data = future.result()
                            # âœ… æ£€æŸ¥é¡µé¢æ•°æ®è´¨é‡
                            if self._is_page_data_valid(page_data):
                                pages_data[page_num - 1] = page_data  # é¡µç ä»1å¼€å§‹ï¼Œç´¢å¼•ä»0å¼€å§‹
                                completed_count += 1
                                print(f"âœ… é¡µé¢ {page_num} å¤„ç†å®Œæˆ ({completed_count}/{len(current_batch)})")
                            else:
                                # ğŸ“ è®°å½•è´¨é‡æ£€æŸ¥å¤±è´¥çš„é¡µé¢
                                failed_pages.append((page_num, next(path for num, path in current_batch if num == page_num)))
                                print(f"âš ï¸ é¡µé¢ {page_num} è´¨é‡æ£€æŸ¥å¤±è´¥ï¼ŒåŠ å…¥é‡è¯•é˜Ÿåˆ—")
                        except Exception as e:
                            # ğŸ›¡ï¸ å®¹é”™å¤„ç†ï¼šå•é¡µå¤±è´¥åŠ å…¥é‡è¯•é˜Ÿåˆ—
                            print(f"âŒ é¡µé¢ {page_num} å¤„ç†å¤±è´¥: {e}")
                            failed_pages.append((page_num, next(path for num, path in current_batch if num == page_num)))
                            
            except Exception as e:
                # ğŸš¨ ç³»ç»Ÿçº§é”™è¯¯ï¼šæ•´ä¸ªæ‰¹æ¬¡å¤±è´¥
                print(f"âŒ å¹¶è¡Œå¤„ç†æ‰¹æ¬¡å¤±è´¥: {e}")
                print(f"ğŸ’¡ å¯èƒ½åŸå› ï¼šç³»ç»Ÿèµ„æºä¸è¶³ã€æƒé™é—®é¢˜ã€æˆ–è¿›ç¨‹æ± åˆå§‹åŒ–å¤±è´¥")
                
                # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡å°è¯•ï¼Œå›é€€åˆ°ä¸²è¡Œå¤„ç†
                if retry_count == 0:
                    print("ğŸ”„ å›é€€åˆ°ä¸²è¡Œå¤„ç†æ¨¡å¼...")
                    return self._process_pages_sequential(single_page_files, output_dir)
                else:
                    # é‡è¯•æ—¶çš„ç³»ç»Ÿçº§é”™è¯¯ï¼Œæ ‡è®°æ‰€æœ‰å½“å‰é¡µé¢ä¸ºå¤±è´¥
                    failed_pages.extend(current_batch)
            
            # ğŸ”„ å‡†å¤‡ä¸‹ä¸€è½®é‡è¯•
            current_batch = failed_pages
            retry_count += 1
            
            if failed_pages:
                if retry_count <= max_retries:
                    print(f"ğŸ“ æœ¬è½®æœ‰{len(failed_pages)}ä¸ªé¡µé¢å¤±è´¥ï¼Œå°†åœ¨ç¬¬{retry_count}è½®é‡è¯•")
                else:
                    print(f"âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°({max_retries})ï¼Œ{len(failed_pages)}ä¸ªé¡µé¢æœ€ç»ˆå¤±è´¥")
        
        # ğŸš¨ å¤„ç†æœ€ç»ˆå¤±è´¥çš„é¡µé¢
        if current_batch:  # ä»æœ‰å¤±è´¥é¡µé¢
            print(f"âš ï¸ æœ€ç»ˆå¤±è´¥é¡µé¢: {[page_num for page_num, _ in current_batch]}")
            for page_num, _ in current_batch:
                if pages_data[page_num - 1] is None:
                    # åˆ›å»ºç©ºçš„å¤±è´¥é¡µé¢æ•°æ®
                    pages_data[page_num - 1] = PageData(
                        page_number=page_num,
                        raw_text="",  # ç©ºæ–‡æœ¬
                        images=[],
                        tables=[]
                    )
                    print(f"ğŸ”§ é¡µé¢ {page_num} æ ‡è®°ä¸ºæœ€ç»ˆå¤±è´¥ï¼Œä½¿ç”¨ç©ºæ•°æ®")
        
        # ğŸ¯ æœ€ç»ˆæ•´ç†ï¼šç¡®ä¿ç»“æœçš„å®Œæ•´æ€§å’Œé¡ºåº
        successful_pages: List[PageData] = [page for page in pages_data if page is not None]
        successful_pages.sort(key=lambda x: x.page_number)
        
        total_pages = len(single_page_files)
        success_count = len(successful_pages)
        final_failed_count = len(current_batch) if current_batch else 0
        
        print(f"ğŸ“Š å¹¶è¡Œå¤„ç†å®Œæˆç»Ÿè®¡:")
        print(f"   âœ… æˆåŠŸé¡µé¢: {success_count}/{total_pages} ({success_count/total_pages*100:.1f}%)")
        print(f"   âŒ æœ€ç»ˆå¤±è´¥: {final_failed_count}/{total_pages} ({final_failed_count/total_pages*100:.1f}%)")
        print(f"   ğŸ”„ æ€»é‡è¯•è½®æ•°: {retry_count-1}")
        
        # ğŸ“ˆ æ€§èƒ½æç¤º
        if self.use_process_pool and success_count > 0:
            theoretical_speedup = min(optimal_workers, len(single_page_files))
            print(f"ğŸš€ ç†è®ºåŠ é€Ÿæ¯”: {theoretical_speedup}xï¼ˆå®é™…å¯èƒ½å› è¿›ç¨‹å¯åŠ¨å¼€é”€ç•¥ä½ï¼‰")
        
        return successful_pages
    
    def _is_page_data_valid(self, page_data: PageData) -> bool:
        """
        æ£€æŸ¥é¡µé¢æ•°æ®è´¨é‡
        
        Args:
            page_data: é¡µé¢æ•°æ®
            
        Returns:
            bool: æ˜¯å¦ä¸ºæœ‰æ•ˆæ•°æ®
        """
        if not page_data:
            return False
        
        # æ£€æŸ¥åŸºæœ¬å±æ€§
        if not hasattr(page_data, 'page_number') or page_data.page_number <= 0:
            print(f"âš ï¸ é¡µé¢æ•°æ®ç¼ºå°‘æœ‰æ•ˆé¡µç ")
            return False
        
        # æ£€æŸ¥æ–‡æœ¬å†…å®¹ï¼ˆè‡³å°‘åº”è¯¥æœ‰ä¸€äº›å†…å®¹ï¼Œé™¤éæ˜¯çº¯å›¾ç‰‡é¡µé¢ï¼‰
        has_text = page_data.raw_text and len(page_data.raw_text.strip()) > 0
        has_media = (hasattr(page_data, 'images') and len(page_data.images) > 0) or \
                   (hasattr(page_data, 'tables') and len(page_data.tables) > 0)
        
        # è‡³å°‘è¦æœ‰æ–‡æœ¬æˆ–åª’ä½“å†…å®¹
        if not has_text and not has_media:
            print(f"âš ï¸ é¡µé¢ {page_data.page_number} æ—¢æ— æ–‡æœ¬ä¹Ÿæ— åª’ä½“å†…å®¹")
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ˜æ˜¾çš„é”™è¯¯ä¿¡æ¯
        if has_text and ("å¤„ç†å¤±è´¥" in page_data.raw_text or "connection" in page_data.raw_text.lower()):
            print(f"âš ï¸ é¡µé¢ {page_data.page_number} æ–‡æœ¬åŒ…å«é”™è¯¯ä¿¡æ¯")
            return False
        
        return True
    
    def _process_pages_sequential(self, 
                                 single_page_files: List[Tuple[int, str]], 
                                 output_dir: str) -> List[PageData]:
        """é¡ºåºå¤„ç†æ‰€æœ‰é¡µé¢"""
        print("ğŸ”„ ä½¿ç”¨é¡ºåºå¤„ç†æ¨¡å¼")
        
        pages_data: List[PageData] = []
        for page_num, single_page_path in single_page_files:
            try:
                page_data = self._process_single_page(page_num, single_page_path, output_dir)
                pages_data.append(page_data)
                print(f"âœ… é¡µé¢ {page_num} å¤„ç†å®Œæˆ")
            except Exception as e:
                print(f"âŒ é¡µé¢ {page_num} å¤„ç†å¤±è´¥: {e}")
                # âœ… ä¿®å¤ï¼šå¤±è´¥é¡µé¢ä½¿ç”¨ç©ºæ–‡æœ¬ï¼Œä¸ä¿å­˜é”™è¯¯ä¿¡æ¯
                pages_data.append(PageData(
                    page_number=page_num,
                    raw_text="",  # ç©ºæ–‡æœ¬è€Œä¸æ˜¯é”™è¯¯ä¿¡æ¯
                    images=[],
                    tables=[]
                ))
        
        return pages_data
    
    def _process_single_page(self, 
                            page_num: int, 
                            single_page_path: str, 
                            output_dir: str) -> PageData:
        """
        å¤„ç†å•é¡µPDF
        
        Args:
            page_num: é¡µç 
            single_page_path: å•é¡µPDFæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            PageData: é¡µé¢æ•°æ®
        """
        if self.doc_converter is None:
            raise RuntimeError("Doclingè½¬æ¢å™¨æœªåˆå§‹åŒ–")
            
        # åˆ›å»ºé¡µé¢ä¸“ç”¨çš„è¾“å‡ºç›®å½•
        page_output_dir = os.path.join(output_dir, f"page_{page_num}")
        os.makedirs(page_output_dir, exist_ok=True)
        
        # ä½¿ç”¨Doclingå¤„ç†å•é¡µPDF
        raw_result = self.doc_converter.convert(Path(single_page_path))
        
        # æå–é¡µé¢æ–‡æœ¬
        page_text = self._extract_page_text(raw_result)
        
        # åˆ›å»ºé¡µé¢æ•°æ®
        page_data = PageData(
            page_number=page_num,
            raw_text=page_text,
            images=[],
            tables=[]
        )
        
        # æå–å›¾ç‰‡å’Œè¡¨æ ¼ - ç”±äºæ˜¯å•é¡µPDFï¼Œæ‰€æœ‰åª’ä½“éƒ½å±äºå½“å‰é¡µ
        self._extract_media_for_single_page(raw_result, page_data, page_output_dir)
        
        return page_data
    
    @staticmethod
    def _extract_smart_context(page_text: str, media_type: str, media_index: int) -> str:
        """
        æ™ºèƒ½æå–åª’ä½“å…ƒç´ çš„ä¸Šä¸‹æ–‡
        
        æ ¹æ®é¡µé¢æ–‡æœ¬çš„é•¿åº¦å’Œå†…å®¹ï¼Œæä¾›åˆé€‚çš„ä¸Šä¸‹æ–‡ï¼š
        - çŸ­é¡µé¢ï¼ˆ<200å­—ç¬¦ï¼‰ï¼šè¿”å›å®Œæ•´é¡µé¢æ–‡æœ¬
        - ä¸­ç­‰é¡µé¢ï¼ˆ200-500å­—ç¬¦ï¼‰ï¼šè¿”å›é¡µé¢æ–‡æœ¬ä½†é™åˆ¶é•¿åº¦
        - é•¿é¡µé¢ï¼ˆ>500å­—ç¬¦ï¼‰ï¼šè¿”å›å…³é”®æ®µè½æˆ–å‰åæ–‡æœ¬ç‰‡æ®µ
        
        Args:
            page_text: é¡µé¢å®Œæ•´æ–‡æœ¬
            media_type: åª’ä½“ç±»å‹ï¼ˆ"image"æˆ–"table"ï¼‰
            media_index: åª’ä½“ç´¢å¼•
            
        Returns:
            str: æ™ºèƒ½æå–çš„ä¸Šä¸‹æ–‡
        """
        if not page_text or not page_text.strip():
            return ""
        
        text = page_text.strip()
        
        # çŸ­é¡µé¢ï¼šç›´æ¥è¿”å›å®Œæ•´æ–‡æœ¬
        if len(text) <= 200:
            return text
        
        # ä¸­ç­‰é¡µé¢ï¼šè¿”å›å‰300å­—ç¬¦
        elif len(text) <= 500:
            return text[:300] + "..." if len(text) > 300 else text
        
        # é•¿é¡µé¢ï¼šå°è¯•æ‰¾åˆ°æœ‰æ„ä¹‰çš„æ®µè½
        else:
            # æŒ‰æ®µè½åˆ†å‰²
            paragraphs = [p.strip() for p in text.split('\n') if p.strip()]
            
            if not paragraphs:
                return text[:300] + "..."
            
            # å¦‚æœæœ‰å¤šä¸ªæ®µè½ï¼Œé€‰æ‹©å‰å‡ ä¸ªæ®µè½
            context_parts = []
            current_length = 0
            target_length = 300
            
            for para in paragraphs:
                if current_length + len(para) > target_length and context_parts:
                    break
                context_parts.append(para)
                current_length += len(para)
            
            if context_parts:
                return '\n'.join(context_parts)
            else:
                return text[:300] + "..."
    
    def _extract_page_text(self, raw_result: Any) -> str:
        """ä»å•é¡µPDFçš„doclingç»“æœä¸­æå–æ–‡æœ¬ - å¢å¼ºå¤šç­–ç•¥æå–"""
        try:
            page_text = ""
            extraction_methods = []
            
            # ğŸ” é¢„æ£€æŸ¥ï¼šæ‰“å°æ–‡æ¡£ç»“æ„ä¿¡æ¯
            print(f"ğŸ” å¼€å§‹æ–‡æœ¬æå–ï¼ŒDocumentç±»å‹: {type(raw_result.document)}")
            if hasattr(raw_result.document, 'texts'):
                print(f"ğŸ” textsé›†åˆé•¿åº¦: {len(raw_result.document.texts) if raw_result.document.texts else 0}")
            
            # ğŸ¯ ç­–ç•¥1ï¼šä¼˜å…ˆå°è¯•export_to_text()æ–¹æ³•ï¼ˆæœ€ç›´æ¥çš„æ–‡æœ¬æå–ï¼‰
            if hasattr(raw_result.document, 'export_to_text'):
                try:
                    page_text = raw_result.document.export_to_text()
                    if page_text and page_text.strip():
                        extraction_methods.append("export_to_text")
                        print(f"âœ… ä½¿ç”¨export_to_text()æˆåŠŸæå–æ–‡æœ¬: {len(page_text)}å­—ç¬¦")
                        return page_text.strip()
                    else:
                        print(f"âš ï¸ export_to_text()è¿”å›ç©ºå†…å®¹")
                except Exception as e:
                    print(f"âš ï¸ export_to_text()æ–¹æ³•å¤±è´¥: {e}")
            else:
                print(f"âš ï¸ documentæ²¡æœ‰export_to_text()æ–¹æ³•")
            
            # ğŸ¯ ç­–ç•¥2ï¼šç›´æ¥éå†document.textsé›†åˆï¼ˆæ¨èæ–¹æ³•ï¼‰
            if hasattr(raw_result.document, 'texts') and raw_result.document.texts:
                try:
                    text_parts = []
                    for i, text_item in enumerate(raw_result.document.texts):
                        if hasattr(text_item, 'text') and text_item.text:
                            text_parts.append(text_item.text)
                            print(f"ğŸ“ æ–‡æœ¬ç‰‡æ®µ{i+1}: {len(text_item.text)}å­—ç¬¦ - {text_item.text[:50]}...")
                    
                    if text_parts:
                        page_text = '\n'.join(text_parts)
                        extraction_methods.append("texts_collection")
                        print(f"âœ… ä»textsé›†åˆæå–æ–‡æœ¬: {len(text_parts)}ä¸ªæ–‡æœ¬é¡¹, {len(page_text)}å­—ç¬¦")
                        return page_text.strip()
                    else:
                        print(f"âš ï¸ textsé›†åˆä¸­æ— æœ‰æ•ˆæ–‡æœ¬å†…å®¹")
                except Exception as e:
                    print(f"âš ï¸ éå†textsé›†åˆå¤±è´¥: {e}")
            else:
                print(f"âš ï¸ documentæ²¡æœ‰textsé›†åˆæˆ–ä¸ºç©º")
            
            # ğŸ¯ ç­–ç•¥3ï¼šå°è¯•ä»å„ç§å¯èƒ½çš„æ–‡æœ¬å±æ€§ä¸­æå–
            text_attributes = ['content', 'text_content', 'body', 'main_text']
            for attr in text_attributes:
                if hasattr(raw_result.document, attr):
                    try:
                        text_val = getattr(raw_result.document, attr)
                        if text_val and str(text_val).strip():
                            extraction_methods.append(f"attribute_{attr}")
                            print(f"âœ… ä»{attr}å±æ€§æå–æ–‡æœ¬: {len(str(text_val))}å­—ç¬¦")
                            return str(text_val).strip()
                    except Exception as e:
                        print(f"âš ï¸ ä»{attr}å±æ€§æå–å¤±è´¥: {e}")
            
            # ğŸ¯ ç­–ç•¥4ï¼šä½¿ç”¨export_to_markdown()ä½œä¸ºå¤‡é€‰
            if hasattr(raw_result.document, 'export_to_markdown'):
                try:
                    raw_markdown = raw_result.document.export_to_markdown()
                    if raw_markdown and raw_markdown.strip():
                        # æ¸…ç†markdownå†…å®¹
                        import re
                        markdown_clean_pattern = re.compile(r"<!--[\s\S]*?-->")
                        cleaned_text = markdown_clean_pattern.sub("", raw_markdown)
                        
                        if cleaned_text and cleaned_text.strip():
                            extraction_methods.append("export_to_markdown")
                            print(f"âœ… ä½¿ç”¨export_to_markdown()æå–æ–‡æœ¬: {len(cleaned_text)}å­—ç¬¦")
                            return cleaned_text.strip()
                    else:
                        print(f"âš ï¸ export_to_markdown()è¿”å›ç©ºå†…å®¹")
                except Exception as e:
                    print(f"âš ï¸ export_to_markdown()æ–¹æ³•å¤±è´¥: {e}")
            else:
                print(f"âš ï¸ documentæ²¡æœ‰export_to_markdown()æ–¹æ³•")
            
            # ğŸ¯ ç­–ç•¥5ï¼šå°è¯•ä»é¡µé¢çº§åˆ«æå–ï¼ˆå¦‚æœæœ‰pagesï¼‰
            if hasattr(raw_result.document, 'pages') and raw_result.document.pages:
                try:
                    page_texts = []
                    for page in raw_result.document.pages:
                        if hasattr(page, 'text') and page.text:
                            page_texts.append(page.text)
                    
                    if page_texts:
                        combined_text = '\n'.join(page_texts)
                        extraction_methods.append("pages_text")
                        print(f"âœ… ä»pagesæå–æ–‡æœ¬: {len(page_texts)}é¡µ, {len(combined_text)}å­—ç¬¦")
                        return combined_text.strip()
                except Exception as e:
                    print(f"âš ï¸ ä»pagesæå–æ–‡æœ¬å¤±è´¥: {e}")
            
            # ğŸ¯ ç­–ç•¥6ï¼šæœ€åçš„è¯Šæ–­å’Œå…œåº•
            all_attrs = [attr for attr in dir(raw_result.document) if not attr.startswith('_')]
            text_like_attrs = [attr for attr in all_attrs if 'text' in attr.lower() or 'content' in attr.lower()]
            
            print("ğŸ” æ–‡æ¡£å¯ç”¨å±æ€§:", all_attrs[:10], "..." if len(all_attrs) > 10 else "")
            print("ğŸ” ç–‘ä¼¼æ–‡æœ¬ç›¸å…³å±æ€§:", text_like_attrs)
            
            # æœ€åå°è¯•ï¼šç›´æ¥æ£€æŸ¥documentçš„__dict__
            if hasattr(raw_result.document, '__dict__'):
                doc_dict = raw_result.document.__dict__
                for key, value in doc_dict.items():
                    if isinstance(value, str) and len(value) > 10:
                        extraction_methods.append(f"dict_{key}")
                        print(f"âœ… ä»__dict__[{key}]æå–æ–‡æœ¬: {len(value)}å­—ç¬¦")
                        return value.strip()
            
            print(f"âŒ æ‰€æœ‰{len(extraction_methods) + 6}ç§æ–‡æœ¬æå–ç­–ç•¥éƒ½å¤±è´¥")
            print(f"ğŸ” å°è¯•è¿‡çš„æ–¹æ³•: {extraction_methods}")
            return ""
            
        except Exception as e:
            print(f"âŒ é¡µé¢æ–‡æœ¬æå–è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸: {e}")
            import traceback
            print(f"ğŸ” å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")
            return ""
    
    def _extract_media_for_single_page(self, 
                                      raw_result: Any, 
                                      page_data: PageData, 
                                      page_output_dir: str):
        """ä¸ºå•é¡µPDFæå–å›¾ç‰‡å’Œè¡¨æ ¼"""
        try:
            # ç”±äºæ˜¯å•é¡µPDFï¼Œæ‰€æœ‰åª’ä½“éƒ½å±äºå½“å‰é¡µ
            image_counter = 0
            table_counter = 0
            
            # æå–å›¾ç‰‡
            for picture in raw_result.document.pictures:
                image_counter += 1
                try:
                    # è·å–å›¾ç‰‡
                    picture_image = picture.get_image(raw_result.document)
                    if picture_image is None:
                        continue
                    
                    # ä¿å­˜å›¾ç‰‡
                    image_filename = f"picture-{image_counter}.png"
                    image_path = os.path.join(page_output_dir, image_filename)
                    with open(image_path, "wb") as fp:
                        picture_image.save(fp, "PNG")
                    
                    # è·å–å›¾ç‰‡ä¿¡æ¯
                    from PIL import Image
                    image_img = Image.open(image_path)
                    caption = picture.caption_text(raw_result.document) if hasattr(picture, 'caption_text') else ""
                    
                    # åˆ›å»ºImageWithContextå¯¹è±¡ - ä½¿ç”¨æ™ºèƒ½ä¸Šä¸‹æ–‡æå–
                    smart_context = Stage1DoclingProcessor._extract_smart_context(page_data.raw_text, "image", image_counter)
                    image_with_context = ImageWithContext(
                        image_path=image_path,
                        page_number=page_data.page_number,  # ğŸŒŸ ç¡®ä¿é¡µç å‡†ç¡®
                        page_context=smart_context,         # ğŸŒŸ ä½¿ç”¨æ™ºèƒ½ä¸Šä¸‹æ–‡è€Œä¸æ˜¯æ•´é¡µæ–‡æœ¬
                        caption=caption or f"å›¾ç‰‡ {image_counter}",
                        metadata={
                            'width': image_img.width,
                            'height': image_img.height,
                            'size': image_img.width * image_img.height,
                            'aspect_ratio': image_img.width / image_img.height
                        }
                    )
                    
                    page_data.images.append(image_with_context)
                    
                except Exception as e:
                    print(f"âŒ é¡µé¢ {page_data.page_number} å›¾ç‰‡ {image_counter} å¤„ç†å¤±è´¥: {e}")
            
            # æå–è¡¨æ ¼
            for table in raw_result.document.tables:
                table_counter += 1
                try:
                    # è·å–è¡¨æ ¼å›¾ç‰‡
                    table_image = table.get_image(raw_result.document)
                    if table_image is None:
                        continue
                    
                    # ä¿å­˜è¡¨æ ¼å›¾ç‰‡
                    table_filename = f"table-{table_counter}.png"
                    table_path = os.path.join(page_output_dir, table_filename)
                    with open(table_path, "wb") as fp:
                        table_image.save(fp, "PNG")
                    
                    # è·å–è¡¨æ ¼ä¿¡æ¯
                    from PIL import Image
                    table_img = Image.open(table_path)
                    caption = table.caption_text(raw_result.document) if hasattr(table, 'caption_text') else ""
                    
                    # åˆ›å»ºTableWithContextå¯¹è±¡ - ä½¿ç”¨æ™ºèƒ½ä¸Šä¸‹æ–‡æå–
                    smart_context = Stage1DoclingProcessor._extract_smart_context(page_data.raw_text, "table", table_counter)
                    table_with_context = TableWithContext(
                        table_path=table_path,
                        page_number=page_data.page_number,  # ğŸŒŸ ç¡®ä¿é¡µç å‡†ç¡®
                        page_context=smart_context,         # ğŸŒŸ ä½¿ç”¨æ™ºèƒ½ä¸Šä¸‹æ–‡è€Œä¸æ˜¯æ•´é¡µæ–‡æœ¬
                        caption=caption or f"è¡¨æ ¼ {table_counter}",
                        metadata={
                            'width': table_img.width,
                            'height': table_img.height,
                            'size': table_img.width * table_img.height,
                            'aspect_ratio': table_img.width / table_img.height
                        }
                    )
                    
                    page_data.tables.append(table_with_context)
                    
                except Exception as e:
                    print(f"âŒ é¡µé¢ {page_data.page_number} è¡¨æ ¼ {table_counter} å¤„ç†å¤±è´¥: {e}")
            
            print(f"ğŸ“Š é¡µé¢ {page_data.page_number}: {len(page_data.images)} ä¸ªå›¾ç‰‡, {len(page_data.tables)} ä¸ªè¡¨æ ¼")
            
        except Exception as e:
            print(f"âŒ é¡µé¢ {page_data.page_number} åª’ä½“æå–å¤±è´¥: {e}")
    
    def _cleanup_temp_files(self, single_page_files: List[Tuple[int, str]]):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        if not single_page_files:
            return
        
        # è·å–ä¸´æ—¶ç›®å½•
        temp_dir = os.path.dirname(single_page_files[0][1])
        
        try:
            shutil.rmtree(temp_dir)
            print(f"ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {temp_dir}")
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
    
    def _generate_full_raw_text_with_media_markers(self, pages: List[PageData]) -> Tuple[str, Dict[str, str]]:
        """
        ç”ŸæˆåŒ…å«åª’ä½“æ ‡è®°çš„å®Œæ•´æ–‡æœ¬ï¼ŒåŒæ—¶ä¿å­˜æ¯é¡µçš„åŸå§‹æ–‡æœ¬
        
        åŸºäºé¡µé¢æ•°æ®ç”Ÿæˆfull_raw_textï¼Œå¹¶åœ¨é€‚å½“ä½ç½®æ’å…¥å›¾ç‰‡è¡¨æ ¼æ ‡è®°
        åŒæ—¶ä¿å­˜æ¯é¡µçš„å®Œæ•´åŸå§‹æ–‡æœ¬ç”¨äºç²¾ç¡®çš„ä¸Šä¸‹æ–‡æå–
        
        Args:
            pages: é¡µé¢æ•°æ®åˆ—è¡¨ (PageData objects)
            
        Returns:
            Tuple[str, Dict[str, str]]: (åŒ…å«åª’ä½“æ ‡è®°çš„å®Œæ•´åŸå§‹æ–‡æœ¬, é¡µç ->é¡µé¢åŸå§‹æ–‡æœ¬çš„å­—å…¸)
        """
        full_text_parts = []
        page_texts = {}  # é¡µç  -> å®Œæ•´é¡µé¢åŸå§‹æ–‡æœ¬ï¼Œç”¨äºç²¾ç¡®çš„ä¸Šä¸‹æ–‡æå–
        
        # å…¨å±€è®¡æ•°å™¨ï¼Œç¡®ä¿content_idå”¯ä¸€
        img_counter = 1
        table_counter = 1
        
        for page in pages:
            page_num = page.page_number
            page_text = page.raw_text or ""
            
            # ä¿å­˜åŸå§‹é¡µé¢æ–‡æœ¬ï¼ˆä¸å«åª’ä½“æ ‡è®°ï¼Œç”¨äºç²¾ç¡®çš„ä¸Šä¸‹æ–‡æå–ï¼‰
            page_texts[str(page_num)] = page_text
            
            # åœ¨é¡µé¢æ–‡æœ¬ä¸­æ’å…¥åª’ä½“æ ‡è®° (ä½¿ç”¨content_id)
            text_with_markers = page_text
            
            # æ’å…¥å›¾ç‰‡æ ‡è®° - ä½¿ç”¨content_idè€Œä¸æ˜¯path
            for img in page.images:
                img_marker = f"[IMAGE:{img_counter}]"  # ç®€åŒ–æ ¼å¼ï¼Œåªç”¨ID
                text_with_markers += f"\n{img_marker}\n"
                img_counter += 1
            
            # æ’å…¥è¡¨æ ¼æ ‡è®° - ä½¿ç”¨content_idè€Œä¸æ˜¯path  
            for table in page.tables:
                table_marker = f"[TABLE:{table_counter}]"  # ç®€åŒ–æ ¼å¼ï¼Œåªç”¨ID
                text_with_markers += f"\n{table_marker}\n"
                table_counter += 1
            
            # ç›´æ¥æ·»åŠ é¡µé¢æ–‡æœ¬ï¼Œä¸æ·»åŠ é¡µé¢æ ‡è®°å™ªéŸ³
            full_text_parts.append(text_with_markers)
        
        # ç”¨ç©ºè¡Œåˆ†éš”é¡µé¢ï¼Œé¿å…é¡µé¢æ ‡è®°å™ªéŸ³
        full_raw_text = "\n\n".join(full_text_parts)
        
        print(f"ğŸ“„ ç”Ÿæˆfull_raw_text: {len(full_raw_text)} å­—ç¬¦ (æ— é¡µé¢å™ªéŸ³)")
        print(f"ğŸ“„ ä¿å­˜page_texts: {len(page_texts)} é¡µ")
        return full_raw_text, page_texts
    
    def _populate_final_schema(self, 
                              final_schema: FinalMetadataSchema,
                              pdf_path: str,
                              full_raw_text: str,
                              page_texts: Dict[str, str],
                              pages: List[PageData],
                              stage_start_time: float):
        """
        å¡«å……Final Schemaçš„åŸºç¡€ä¿¡æ¯
        
        Args:
            final_schema: Final Schemaå¯¹è±¡
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            full_raw_text: å®Œæ•´åŸå§‹æ–‡æœ¬
            page_texts: é¡µç ->é¡µé¢åŸå§‹æ–‡æœ¬çš„å­—å…¸
            pages: é¡µé¢æ•°æ®åˆ—è¡¨ (PageData objects)
            stage_start_time: é˜¶æ®µå¼€å§‹æ—¶é—´
        """
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_images = sum(len(page.images) for page in pages)
        total_tables = sum(len(page.tables) for page in pages)
        total_pages = len(pages)
        
        # å¡«å……DocumentSummary
        doc_id = final_schema.document_id
        final_schema.document_summary = DocumentSummary(
            content_id=f"{doc_id}_document_summary_1",
            document_id=doc_id,
            full_raw_text=full_raw_text,  # ğŸŒŸ å…³é”®ï¼šä¿å­˜full_raw_text
            page_texts=page_texts,  # ğŸŒŸ æ–°å¢ï¼šä¿å­˜æ¯é¡µçš„å®Œæ•´åŸå§‹æ–‡æœ¬
            source_file_path=pdf_path,
            file_name=os.path.basename(pdf_path),
            file_size=os.path.getsize(pdf_path),
            total_pages=total_pages,
            image_count=total_images,
            table_count=total_tables,
            processing_time=time.time() - stage_start_time
        )
        
        # å¡«å……ImageChunks
        img_counter = 1
        for page in pages:
            for img in page.images:
                image_chunk = ImageChunk(
                    content_id=f"{doc_id}_image_{img_counter}",
                    document_id=doc_id,
                    image_path=img.image_path,
                    page_number=img.page_number,  # ğŸŒŸ é¡µç ç°åœ¨æ˜¯å‡†ç¡®çš„
                    caption=img.caption or "",
                    page_context=img.page_context,  # ğŸŒŸ ä¸Šä¸‹æ–‡ç°åœ¨æ˜¯å‡†ç¡®çš„
                    width=img.metadata.get("width", 0),
                    height=img.metadata.get("height", 0),
                    size=img.metadata.get("size", 0),
                    aspect_ratio=img.metadata.get("aspect_ratio", 0.0)
                    # ai_description å’Œ chapter_id ç•™ç©ºï¼Œç­‰å¾…åç»­é˜¶æ®µå¡«å……
                )
                final_schema.image_chunks.append(image_chunk)
                img_counter += 1
        
        # å¡«å……TableChunks  
        table_counter = 1
        for page in pages:
            for table in page.tables:
                table_chunk = TableChunk(
                    content_id=f"{doc_id}_table_{table_counter}",
                    document_id=doc_id,
                    table_path=table.table_path,
                    page_number=table.page_number,  # ğŸŒŸ é¡µç ç°åœ¨æ˜¯å‡†ç¡®çš„
                    caption=table.caption or "",
                    page_context=table.page_context,  # ğŸŒŸ ä¸Šä¸‹æ–‡ç°åœ¨æ˜¯å‡†ç¡®çš„
                    width=table.metadata.get("width", 0),
                    height=table.metadata.get("height", 0),
                    size=table.metadata.get("size", 0),
                    aspect_ratio=table.metadata.get("aspect_ratio", 0.0)
                    # ai_description å’Œ chapter_id ç•™ç©ºï¼Œç­‰å¾…åç»­é˜¶æ®µå¡«å……
                )
                final_schema.table_chunks.append(table_chunk)
                table_counter += 1
        
        print(f"ğŸ“Š å¡«å……å®Œæˆ:")
        print(f"   ğŸ“„ æ–‡æ¡£æ‘˜è¦: 1ä¸ª (åŒ…å«full_raw_text)")
        print(f"   ğŸ–¼ï¸ å›¾ç‰‡chunks: {len(final_schema.image_chunks)}ä¸ª")
        print(f"   ğŸ“‹ è¡¨æ ¼chunks: {len(final_schema.table_chunks)}ä¸ª")
        print(f"   âœ… æ‰€æœ‰é¡µç å’Œä¸Šä¸‹æ–‡éƒ½å·²å‡†ç¡®æ ‡æ³¨")
    
    def can_resume(self, output_dir: str) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦å¯ä»¥è·³è¿‡é˜¶æ®µ1ï¼ˆå·²ç»å®Œæˆï¼‰
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            bool: æ˜¯å¦å¯ä»¥è·³è¿‡
        """
        final_metadata_path = os.path.join(output_dir, "final_metadata.json")
        
        if not os.path.exists(final_metadata_path):
            return False
            
        try:
            final_schema = FinalMetadataSchema.load(final_metadata_path)
            return final_schema.is_stage_complete("stage1")
        except Exception:
            return False
    
    def resume_from_existing(self, output_dir: str) -> Tuple[FinalMetadataSchema, str]:
        """
        ä»ç°æœ‰çš„final_metadata.jsonæ¢å¤
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            Tuple[FinalMetadataSchema, str]: (final_schemaå¯¹è±¡, final_metadata.jsonæ–‡ä»¶è·¯å¾„)
        """
        final_metadata_path = os.path.join(output_dir, "final_metadata.json")
        
        if not self.can_resume(output_dir):
            raise ValueError(f"æ— æ³•ä» {final_metadata_path} æ¢å¤ï¼Œé˜¶æ®µ1æœªå®Œæˆ")
        
        final_schema = FinalMetadataSchema.load(final_metadata_path)
        print(f"âœ… ä»ç°æœ‰æ–‡ä»¶æ¢å¤é˜¶æ®µ1ç»“æœ: {final_metadata_path}")
        print(f"ğŸ“Š å½“å‰å®Œæˆåº¦: {final_schema.get_completion_percentage()}%")
        
        return final_schema, final_metadata_path


def _process_single_page_static(page_num: int, 
                               single_page_path: str, 
                               output_dir: str,
                               config: PDFProcessingConfig) -> PageData:
    """
    å¤„ç†å•é¡µPDFçš„é™æ€å‡½æ•°ï¼ˆç”¨äºè¿›ç¨‹æ± ï¼‰
    
    Args:
        page_num: é¡µç 
        single_page_path: å•é¡µPDFæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        config: PDFå¤„ç†é…ç½®
        
    Returns:
        PageData: é¡µé¢æ•°æ®
    """
    try:
        # ğŸ”§ é…ç½®ç¦»çº¿æ¨¡å¼ï¼Œé¿å…ç½‘ç»œè¿æ¥é—®é¢˜ï¼ˆå­è¿›ç¨‹ï¼‰
        import os
        os.environ['HF_HUB_OFFLINE'] = '1'  # å¼ºåˆ¶HuggingFace Hubç¦»çº¿æ¨¡å¼
        os.environ['TRANSFORMERS_OFFLINE'] = '1'  # Transformersç¦»çº¿æ¨¡å¼
        
        # åœ¨è¿›ç¨‹å†…åˆå§‹åŒ–doclingè½¬æ¢å™¨
        from docling.datamodel.base_models import InputFormat
        from docling.datamodel.pipeline_options import PdfPipelineOptions, EasyOcrOptions
        from docling.document_converter import DocumentConverter, PdfFormatOption
        
        # è®¾ç½®æœ¬åœ°æ¨¡å‹è·¯å¾„
        models_cache_dir = Path("models_cache")
        artifacts_path = None
        if models_cache_dir.exists():
            artifacts_path = str(models_cache_dir.absolute())
        elif config.docling.artifacts_path:
            artifacts_path = config.docling.artifacts_path
        
        # åˆ›å»ºç®¡é“é€‰é¡¹
        if config.docling.ocr_enabled:
            # ğŸ”¥ å¢å¼ºOCRé…ç½®ï¼Œç¡®ä¿æ–‡å­—æå–æˆåŠŸ
            ocr_options = EasyOcrOptions()
            try:
                # æ ¸å¿ƒOCRè®¾ç½®
                ocr_options.force_full_page_ocr = True  # å¼ºåˆ¶å…¨é¡µOCR
                
                # ğŸš€ æ·»åŠ æ›´å¤šOCRä¼˜åŒ–è®¾ç½®ï¼ˆåªä½¿ç”¨å®é™…å­˜åœ¨çš„å±æ€§ï¼‰
                if hasattr(ocr_options, 'use_gpu'):
                    ocr_options.use_gpu = False  # å¼ºåˆ¶ä½¿ç”¨CPUï¼Œé¿å…GPUå…¼å®¹æ€§é—®é¢˜
                
                if hasattr(ocr_options, 'lang'):
                    ocr_options.lang = ['ch_sim', 'en']  # æ”¯æŒä¸­è‹±æ–‡
                
                if hasattr(ocr_options, 'confidence_threshold'):
                    ocr_options.confidence_threshold = 0.3  # é™ä½ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œæé«˜æ£€å‡ºç‡
                
                if hasattr(ocr_options, 'bitmap_area_threshold'):
                    ocr_options.bitmap_area_threshold = 0.01  # é™ä½åŒºåŸŸé˜ˆå€¼ï¼Œæ£€æµ‹æ›´å°æ–‡å­—
                
                print("âœ… åº”ç”¨å¢å¼ºOCRè®¾ç½®ï¼š")
                print("   - force_full_page_ocr=Trueï¼ˆå¼ºåˆ¶å…¨é¡µOCRï¼‰")
                print("   - use_gpu=Falseï¼ˆCPUæ¨¡å¼ï¼Œé¿å…å…¼å®¹æ€§é—®é¢˜ï¼‰")
                print("   - lang=['ch_sim', 'en']ï¼ˆä¸­è‹±æ–‡æ”¯æŒï¼‰")
                print("   - confidence_threshold=0.3ï¼ˆé™ä½ç½®ä¿¡åº¦é˜ˆå€¼ï¼‰")
                print("   - bitmap_area_threshold=0.01ï¼ˆæ£€æµ‹æ›´å°æ–‡å­—ï¼‰")
                
            except Exception as e:
                print(f"âš ï¸ åº”ç”¨å¢å¼ºOCRè®¾ç½®å¤±è´¥: {e}")
            
            pipeline_options = PdfPipelineOptions(
                ocr_options=ocr_options,
                artifacts_path=artifacts_path
            )
        else:
            print("âš ï¸ OCRå·²ç¦ç”¨ï¼Œå¯èƒ½å¯¼è‡´æ–‡å­—æå–ä¸å®Œæ•´")
            pipeline_options = PdfPipelineOptions(
                artifacts_path=artifacts_path
            )
        
        # è®¾ç½®è§£æé€‰é¡¹
        pipeline_options.images_scale = config.docling.images_scale
        pipeline_options.generate_page_images = config.docling.generate_page_images
        pipeline_options.generate_picture_images = config.docling.generate_picture_images
        
        # ğŸš€ æ·»åŠ æ›´å¤šè§£æé€‰é¡¹
        if hasattr(pipeline_options, 'do_ocr'):
            pipeline_options.do_ocr = True  # ç¡®ä¿OCRæ‰§è¡Œ
        
        if hasattr(pipeline_options, 'do_table_structure'):
            pipeline_options.do_table_structure = True  # è¡¨æ ¼ç»“æ„è¯†åˆ«
        
        # åˆ›å»ºæ–‡æ¡£è½¬æ¢å™¨
        doc_converter = DocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
            }
        )
        
        # åˆ›å»ºé¡µé¢ä¸“ç”¨çš„è¾“å‡ºç›®å½•
        page_output_dir = os.path.join(output_dir, f"page_{page_num}")
        os.makedirs(page_output_dir, exist_ok=True)
        
        # ä½¿ç”¨Doclingå¤„ç†å•é¡µPDF
        raw_result = doc_converter.convert(Path(single_page_path))
        
        # æå–é¡µé¢æ–‡æœ¬ - å¤šç­–ç•¥æå–ï¼ˆä¸å®ä¾‹æ–¹æ³•ä¿æŒä¸€è‡´ï¼‰
        try:
            page_text = ""
            
            # ç­–ç•¥1ï¼šä¼˜å…ˆå°è¯•export_to_text()æ–¹æ³•
            if hasattr(raw_result.document, 'export_to_text'):
                try:
                    page_text = raw_result.document.export_to_text()
                    if page_text and page_text.strip():
                        page_text = page_text.strip()
                except Exception:
                    pass
            
            # ç­–ç•¥2ï¼šç›´æ¥éå†document.textsé›†åˆ
            if not page_text and hasattr(raw_result.document, 'texts') and raw_result.document.texts:
                try:
                    text_parts = []
                    for text_item in raw_result.document.texts:
                        if hasattr(text_item, 'text') and text_item.text:
                            text_parts.append(text_item.text)
                    
                    if text_parts:
                        page_text = '\n'.join(text_parts).strip()
                except Exception:
                    pass
            
            # ç­–ç•¥3ï¼šä½¿ç”¨export_to_markdown()ä½œä¸ºå¤‡é€‰
            if not page_text and hasattr(raw_result.document, 'export_to_markdown'):
                try:
                    raw_markdown = raw_result.document.export_to_markdown()
                    import re
                    markdown_clean_pattern = re.compile(r"<!--[\s\S]*?-->")
                    page_text = markdown_clean_pattern.sub("", raw_markdown).strip()
                except Exception:
                    pass
            
            # ç­–ç•¥4ï¼šå°è¯•ä»documentå±æ€§ä¸­ç›´æ¥æå–
            if not page_text and hasattr(raw_result.document, 'text'):
                try:
                    doc_text = getattr(raw_result.document, 'text', None)
                    page_text = doc_text.strip() if doc_text else ""
                except Exception:
                    pass
            
            # å¦‚æœæ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥
            if not page_text:
                page_text = f"é¡µé¢ {page_num} æ–‡æœ¬æå–å¤±è´¥ï¼ˆæ‰€æœ‰ç­–ç•¥å¤±è´¥ï¼‰"
                
        except Exception as e:
            print(f"âš ï¸ é¡µé¢ {page_num} æ–‡æœ¬æå–å¤±è´¥: {e}")
            page_text = f"é¡µé¢ {page_num} æ–‡æœ¬æå–å¤±è´¥: {str(e)}"
        
        # åˆ›å»ºé¡µé¢æ•°æ®
        page_data = PageData(
            page_number=page_num,
            raw_text=page_text,
            images=[],
            tables=[]
        )
        
        # æå–åª’ä½“ï¼ˆè¿›ç¨‹æ± ç‰ˆæœ¬ï¼‰
        try:
            # æå–å›¾ç‰‡
            if hasattr(raw_result.document, 'pictures'):
                for i, picture in enumerate(raw_result.document.pictures):
                    try:
                        # è·å–å›¾ç‰‡
                        picture_image = picture.get_image(raw_result.document)
                        if picture_image is None:
                            continue
                        
                        # ä¿å­˜å›¾ç‰‡
                        image_filename = f"picture-{i+1}.png"
                        image_path = os.path.join(page_output_dir, image_filename)
                        with open(image_path, "wb") as fp:
                            picture_image.save(fp, "PNG")
                        
                        # è·å–å›¾ç‰‡ä¿¡æ¯
                        from PIL import Image
                        image_img = Image.open(image_path)
                        caption = picture.caption_text(raw_result.document) if hasattr(picture, 'caption_text') else ""
                        
                        # åˆ›å»ºImageWithContextå¯¹è±¡ - ä½¿ç”¨æ™ºèƒ½ä¸Šä¸‹æ–‡æå–
                        smart_context = Stage1DoclingProcessor._extract_smart_context(page_text, "image", i+1)
                        image_with_context = ImageWithContext(
                            image_path=image_path,
                            page_number=page_num,  # ğŸŒŸ ç¡®ä¿é¡µç å‡†ç¡®
                            page_context=smart_context,  # ğŸŒŸ ä½¿ç”¨æ™ºèƒ½ä¸Šä¸‹æ–‡è€Œä¸æ˜¯æ•´é¡µæ–‡æœ¬
                            caption=caption or f"å›¾ç‰‡ {i+1}",
                            metadata={
                                'width': image_img.width,
                                'height': image_img.height,
                                'size': image_img.width * image_img.height,
                                'aspect_ratio': image_img.width / image_img.height
                            }
                        )
                        
                        page_data.images.append(image_with_context)
                    except Exception as e:
                        print(f"âš ï¸ é¡µé¢ {page_num} å›¾ç‰‡ {i+1} å¤„ç†å¤±è´¥: {e}")
            
            # æå–è¡¨æ ¼
            if hasattr(raw_result.document, 'tables'):
                for i, table in enumerate(raw_result.document.tables):
                    try:
                        # è·å–è¡¨æ ¼å›¾ç‰‡
                        table_image = table.get_image(raw_result.document)
                        if table_image is None:
                            continue
                        
                        # ä¿å­˜è¡¨æ ¼å›¾ç‰‡
                        table_filename = f"table-{i+1}.png"
                        table_path = os.path.join(page_output_dir, table_filename)
                        with open(table_path, "wb") as fp:
                            table_image.save(fp, "PNG")
                        
                        # è·å–è¡¨æ ¼ä¿¡æ¯
                        from PIL import Image
                        table_img = Image.open(table_path)
                        caption = table.caption_text(raw_result.document) if hasattr(table, 'caption_text') else ""
                        
                        # åˆ›å»ºTableWithContextå¯¹è±¡ - ä½¿ç”¨æ™ºèƒ½ä¸Šä¸‹æ–‡æå–
                        smart_context = Stage1DoclingProcessor._extract_smart_context(page_text, "table", i+1)
                        table_with_context = TableWithContext(
                            table_path=table_path,
                            page_number=page_num,  # ğŸŒŸ ç¡®ä¿é¡µç å‡†ç¡®
                            page_context=smart_context,  # ğŸŒŸ ä½¿ç”¨æ™ºèƒ½ä¸Šä¸‹æ–‡è€Œä¸æ˜¯æ•´é¡µæ–‡æœ¬
                            caption=caption or f"è¡¨æ ¼ {i+1}",
                            metadata={
                                'width': table_img.width,
                                'height': table_img.height,
                                'size': table_img.width * table_img.height,
                                'aspect_ratio': table_img.width / table_img.height
                            }
                        )
                        
                        page_data.tables.append(table_with_context)
                    except Exception as e:
                        print(f"âš ï¸ é¡µé¢ {page_num} è¡¨æ ¼ {i+1} å¤„ç†å¤±è´¥: {e}")
                        
        except Exception as e:
            print(f"âš ï¸ é¡µé¢ {page_num} åª’ä½“æå–å¤±è´¥: {e}")
        
        return page_data
        
    except Exception as e:
        print(f"âŒ é™æ€å‡½æ•°å¤„ç†é¡µé¢ {page_num} å¤±è´¥: {e}")
        # âœ… ä¿®å¤ï¼šå¤±è´¥é¡µé¢ä½¿ç”¨ç©ºæ–‡æœ¬ï¼Œä¸ä¿å­˜é”™è¯¯ä¿¡æ¯
        return PageData(
            page_number=page_num,
            raw_text="",  # ç©ºæ–‡æœ¬è€Œä¸æ˜¯é”™è¯¯ä¿¡æ¯
            images=[],
            tables=[]
        ) 