#!/usr/bin/env python3
"""
Stage 1 Processor: Doclingè§£æ + åˆå§‹Schemaå¡«å……

è´Ÿè´£ä½¿ç”¨Doclingè§£æPDFï¼Œæå–æ–‡æœ¬ã€å›¾ç‰‡ã€è¡¨æ ¼ï¼Œå¹¶åˆå§‹åŒ–Final Schema
è¿™æ˜¯æ•´ä¸ªé‡æ„pipelineçš„ç¬¬ä¸€é˜¶æ®µï¼Œä¸ºåç»­é˜¶æ®µæä¾›åŸºç¡€æ•°æ®

V2ç‰ˆæœ¬ï¼šé‡‡ç”¨é¡µé¢åˆ‡å‰²æ–¹å¼ï¼Œç¡®ä¿é¡µç å‡†ç¡®ï¼Œæ”¯æŒå¹¶è¡Œå¤„ç†
"""

import os
import sys
import time
import tempfile
import shutil
import multiprocessing
from typing import Dict, List, Tuple, Optional, Any
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed

# å¯¼å…¥PDFå¤„ç†åº“
try:
    import fitz  # PyMuPDF
    FITZ_AVAILABLE = True
    print("âœ… PyMuPDF (fitz) å¯ç”¨")
except ImportError:
    FITZ_AVAILABLE = False
    print("âŒ PyMuPDF (fitz) ä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install PyMuPDF")

# å¯¼å…¥doclingç»„ä»¶
try:
    from docling.datamodel.base_models import InputFormat
    from docling.datamodel.pipeline_options import PdfPipelineOptions, EasyOcrOptions
    from docling.document_converter import DocumentConverter, PdfFormatOption
    DOCLING_AVAILABLE = True
    print("âœ… Doclingç»„ä»¶å¯ç”¨")
except ImportError as e:
    DOCLING_AVAILABLE = False
    print(f"âŒ Doclingç»„ä»¶ä¸å¯ç”¨: {e}")

# å¯¼å…¥V1ç‰ˆæœ¬çš„é…ç½®å’Œæ•°æ®æ¨¡å‹
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from pdf_processing.config import PDFProcessingConfig
from pdf_processing.data_models import PageData, ImageWithContext, TableWithContext

# å¯¼å…¥V2ç‰ˆæœ¬çš„Schema
from .final_schema import FinalMetadataSchema, DocumentSummaryChunk, ImageChunk, TableChunk


class Stage1DoclingProcessor:
    """
    é˜¶æ®µ1å¤„ç†å™¨ï¼šDoclingè§£æ + åˆå§‹Schemaå¡«å……
    
    V2ç‰ˆæœ¬ä¸»è¦åŠŸèƒ½ï¼š
    1. å°†PDFæŒ‰é¡µé¢åˆ‡å‰²ä¸ºå•é¡µPDFæ–‡ä»¶
    2. å¹¶è¡Œå¤„ç†æ¯é¡µï¼Œç¡®ä¿é¡µç å‡†ç¡®
    3. ç”Ÿæˆfull_raw_textå¹¶æ’å…¥å›¾ç‰‡è¡¨æ ¼æ ‡è®°
    4. åˆå§‹åŒ–FinalMetadataSchemaå¹¶å¡«å……åŸºç¡€ä¿¡æ¯
    5. ä¿å­˜åˆå§‹åŒ–çš„final_metadata.json
    """
    
    def __init__(self, config: Optional[PDFProcessingConfig] = None, use_process_pool: bool = True):
        """
        åˆå§‹åŒ–å¤„ç†å™¨
        
        Args:
            config: PDFå¤„ç†é…ç½®ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
            use_process_pool: æ˜¯å¦ä½¿ç”¨è¿›ç¨‹æ± ï¼ˆæ¨èç”¨äºCPUå¯†é›†å‹ä»»åŠ¡ï¼‰
            
        ğŸ›ï¸ å…³äº use_process_pool å‚æ•°çš„é€‰æ‹©æŒ‡å—ï¼š
        
        âœ… use_process_pool=True (æ¨èï¼Œé»˜è®¤é€‰æ‹©)ï¼š
        ğŸ“ˆ æ€§èƒ½ä¼˜åŠ¿ï¼š
        - çœŸæ­£çš„å¹¶è¡Œè®¡ç®—ï¼Œå¯ä»¥100%åˆ©ç”¨å¤šæ ¸CPU
        - ç»•è¿‡Python GILé™åˆ¶ï¼Œæ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹è¿è¡Œ
        - ç†è®ºåŠ é€Ÿæ¯”æ¥è¿‘è¿›ç¨‹æ•°ï¼ˆ5è¿›ç¨‹â‰ˆ5å€é€Ÿåº¦ï¼‰
        
        ğŸ›¡ï¸ ç¨³å®šæ€§ä¼˜åŠ¿ï¼š
        - è¿›ç¨‹éš”ç¦»ï¼šä¸€ä¸ªé¡µé¢å´©æºƒä¸å½±å“å…¶ä»–é¡µé¢
        - å†…å­˜ç‹¬ç«‹ï¼šé¿å…å†…å­˜æ³„æ¼ç´¯ç§¯
        - æ•…éšœæ¢å¤ï¼šå•ä¸ªè¿›ç¨‹å¤±è´¥åä¼šè‡ªåŠ¨é‡å¯
        
        âš ï¸ æ³¨æ„äº‹é¡¹ï¼š
        - è¿›ç¨‹å¯åŠ¨æœ‰å¼€é”€ï¼ˆçº¦1-2ç§’ï¼‰
        - å†…å­˜ä½¿ç”¨ç¨é«˜ï¼ˆæ¯è¿›ç¨‹ç‹¬ç«‹å†…å­˜ç©ºé—´ï¼‰
        - éœ€è¦è¶³å¤Ÿçš„ç³»ç»Ÿèµ„æº
        
        ğŸ§µ use_process_pool=False (çº¿ç¨‹æ± æ¨¡å¼)ï¼š
        ğŸ“‹ é€‚ç”¨åœºæ™¯ï¼š
        - ç³»ç»Ÿèµ„æºå—é™ï¼ˆå†…å­˜ä¸è¶³ã€CPUæ ¸å¿ƒå°‘ï¼‰
        - éœ€è¦é¢‘ç¹å…±äº«æ•°æ®
        - ä»»åŠ¡è¾ƒè½»é‡ï¼Œè¿›ç¨‹å¯åŠ¨å¼€é”€å¤ªå¤§
        
        âš ï¸ æ€§èƒ½é™åˆ¶ï¼š
        - å—Python GILé™åˆ¶ï¼ŒCPUå¯†é›†å‹ä»»åŠ¡å¯èƒ½æ— æ³•å¹¶è¡Œ
        - ä½†doclingæœ‰Cæ‰©å±•ï¼Œéƒ¨åˆ†æ“ä½œå¯ä»¥é‡Šæ”¾GIL
        - é€‚åˆI/Oå¯†é›†å‹ä»»åŠ¡ï¼ˆæ–‡ä»¶è¯»å†™ã€ç½‘ç»œè¯·æ±‚ï¼‰
        
        ğŸ¯ å¦‚ä½•é€‰æ‹©ï¼Ÿ
        - å¦‚æœä½ çš„ç”µè„‘æœ‰4æ ¸ä»¥ä¸ŠCPUï¼šé€‰æ‹©è¿›ç¨‹æ± 
        - å¦‚æœå¤„ç†å¤§å‹PDFï¼ˆ>20é¡µï¼‰ï¼šé€‰æ‹©è¿›ç¨‹æ± 
        - å¦‚æœç”µè„‘æ€§èƒ½è¾ƒå¼±æˆ–å†…å­˜ç´§å¼ ï¼šé€‰æ‹©çº¿ç¨‹æ± 
        - å¦‚æœä¸ç¡®å®šï¼šä½¿ç”¨é»˜è®¤çš„è¿›ç¨‹æ± æ¨¡å¼
        """
        self.config = config or PDFProcessingConfig.from_env()
        self.use_process_pool = use_process_pool
        self.doc_converter = None
        
        if not FITZ_AVAILABLE:
            raise RuntimeError("PyMuPDFä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œé¡µé¢åˆ†å‰²")
        
        if not DOCLING_AVAILABLE:
            raise RuntimeError("Doclingä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œé¡µé¢å¤„ç†")
        
        self._init_docling_converter()
        
        # ğŸ’¡ æ™ºèƒ½æ¨¡å¼æç¤º
        cpu_cores = multiprocessing.cpu_count()
        if use_process_pool:
            print(f"ğŸ­ å·²é€‰æ‹©è¿›ç¨‹æ± æ¨¡å¼ï¼ˆé€‚åˆ{cpu_cores}æ ¸CPUçš„å¹¶è¡Œè®¡ç®—ï¼‰")
            if cpu_cores < 4:
                print(f"ğŸ’¡ æç¤ºï¼šä½ çš„CPUåªæœ‰{cpu_cores}æ ¸ï¼Œè€ƒè™‘ä½¿ç”¨çº¿ç¨‹æ± å¯èƒ½æ›´åˆé€‚")
        else:
            print(f"ğŸ§µ å·²é€‰æ‹©çº¿ç¨‹æ± æ¨¡å¼ï¼ˆè½»é‡çº§å¹¶å‘ï¼‰")
            if cpu_cores >= 8:
                print(f"ğŸ’¡ æç¤ºï¼šä½ çš„CPUæœ‰{cpu_cores}æ ¸ï¼Œä½¿ç”¨è¿›ç¨‹æ± å¯èƒ½è·å¾—æ›´å¥½æ€§èƒ½")
                
        print("âœ… Stage1DoclingProcessor V2 åˆå§‹åŒ–å®Œæˆ")
    
    def _init_docling_converter(self) -> None:
        """åˆå§‹åŒ–Doclingè½¬æ¢å™¨"""
        try:
            # è®¾ç½®æœ¬åœ°æ¨¡å‹è·¯å¾„
            models_cache_dir = Path("models_cache")
            artifacts_path = None
            if models_cache_dir.exists():
                artifacts_path = str(models_cache_dir.absolute())
            elif self.config.docling.artifacts_path:
                artifacts_path = self.config.docling.artifacts_path
            
            # åˆ›å»ºç®¡é“é€‰é¡¹
            if self.config.docling.ocr_enabled:
                # ğŸ”¥ ç›´æ¥è®¾ç½®OCRé…ç½®ï¼Œæ ¸å¿ƒè®¾ç½®ï¼šforce_full_page_ocr = True
                ocr_options = EasyOcrOptions()
                try:
                    ocr_options.force_full_page_ocr = True
                    print("âœ… åº”ç”¨å…³é”®OCRè®¾ç½®ï¼šforce_full_page_ocr=Trueï¼ˆä¸»è¿›ç¨‹ï¼‰")
                except Exception as e:
                    print(f"âš ï¸ åº”ç”¨å…³é”®OCRè®¾ç½®å¤±è´¥: {e}")
                
                pipeline_options = PdfPipelineOptions(
                    ocr_options=ocr_options,
                    artifacts_path=artifacts_path
                )
            else:
                pipeline_options = PdfPipelineOptions(
                    artifacts_path=artifacts_path
                )
            
            # è®¾ç½®è§£æé€‰é¡¹
            pipeline_options.images_scale = self.config.docling.images_scale
            pipeline_options.generate_page_images = self.config.docling.generate_page_images
            pipeline_options.generate_picture_images = self.config.docling.generate_picture_images
            
            # åˆ›å»ºæ–‡æ¡£è½¬æ¢å™¨
            self.doc_converter = DocumentConverter(
                format_options={
                    InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
                }
            )
            
            print("âœ… Doclingè½¬æ¢å™¨åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ Doclingè½¬æ¢å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.doc_converter = None
    
    def process(self, pdf_path: str, output_dir: str) -> Tuple[FinalMetadataSchema, str]:
        """
        æ‰§è¡Œé˜¶æ®µ1å¤„ç†
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            Tuple[FinalMetadataSchema, str]: (final_schemaå¯¹è±¡, final_metadata.jsonæ–‡ä»¶è·¯å¾„)
        """
        print(f"ğŸš€ å¼€å§‹é˜¶æ®µ1å¤„ç†ï¼ˆé¡µé¢åˆ‡å‰²ç‰ˆæœ¬ï¼‰: {pdf_path}")
        stage_start_time = time.time()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # åˆå§‹åŒ–Final Schema
        final_schema = FinalMetadataSchema()
        final_schema.update_processing_status("stage1_started", 10)
        
        try:
            # æ­¥éª¤1: åˆ†å‰²PDFä¸ºå•é¡µæ–‡ä»¶
            print("ğŸ“„ æ­¥éª¤1: åˆ†å‰²PDFä¸ºå•é¡µæ–‡ä»¶")
            single_page_files = self._split_pdf_to_pages(pdf_path)
            print(f"ğŸ“„ PDFåˆ†å‰²å®Œæˆï¼Œå…± {len(single_page_files)} é¡µ")
            final_schema.update_processing_status("pdf_split", 15)
            
            # æ­¥éª¤2: å¹¶è¡Œå¤„ç†æ‰€æœ‰é¡µé¢
            print("ğŸ”„ æ­¥éª¤2: å¹¶è¡Œå¤„ç†æ‰€æœ‰é¡µé¢")
            if self.config.media_extractor.parallel_processing:
                pages_data = self._process_pages_parallel(single_page_files, output_dir)
            else:
                pages_data = self._process_pages_sequential(single_page_files, output_dir)
            final_schema.update_processing_status("pages_processed", 25)
            
            # æ­¥éª¤3: æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            print("ğŸ§¹ æ­¥éª¤3: æ¸…ç†ä¸´æ—¶æ–‡ä»¶")
            self._cleanup_temp_files(single_page_files)
            
            # æ­¥éª¤4: ç”Ÿæˆfull_raw_textå¹¶æ’å…¥åª’ä½“æ ‡è®°
            print("ğŸ“ æ­¥éª¤4: ç”Ÿæˆfull_raw_text")
            full_raw_text, page_texts = self._generate_full_raw_text_with_media_markers(pages_data)
            final_schema.update_processing_status("full_text_generated", 27)
            
            # æ­¥éª¤5: å¡«å……Final Schema
            print("ğŸ“‹ æ­¥éª¤5: å¡«å……Final Schema")
            self._populate_final_schema(
                final_schema, pdf_path, full_raw_text, page_texts, pages_data, stage_start_time
            )
            final_schema.update_processing_status("stage1_completed", 30)
            
            # æ­¥éª¤6: ä¿å­˜Final Schema
            final_metadata_path = os.path.join(output_dir, "final_metadata.json")
            final_schema.save(final_metadata_path)
            
            stage_duration = time.time() - stage_start_time
            print(f"âœ… é˜¶æ®µ1å¤„ç†å®Œæˆï¼Œè€—æ—¶: {stage_duration:.2f} ç§’")
            print(f"ğŸ“ ä¿å­˜ä½ç½®: {final_metadata_path}")
            print(f"ğŸ“Š åˆå§‹å¡«å……å®Œæˆåº¦: {final_schema.get_completion_percentage()}%")
            
            return final_schema, final_metadata_path
            
        except Exception as e:
            # ç¡®ä¿æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if 'single_page_files' in locals():
                self._cleanup_temp_files(single_page_files)
            
            error_msg = f"é˜¶æ®µ1å¤„ç†å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            final_schema.update_processing_status("stage1_failed", 0, error_msg)
            raise RuntimeError(error_msg) from e
    
    def _split_pdf_to_pages(self, pdf_path: str) -> List[Tuple[int, str]]:
        """
        å°†PDFåˆ†å‰²ä¸ºå•é¡µæ–‡ä»¶
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            
        Returns:
            List[Tuple[int, str]]: [(é¡µç , å•é¡µPDFæ–‡ä»¶è·¯å¾„), ...]
        """
        doc = fitz.open(pdf_path)
        total_pages = len(doc)
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = tempfile.mkdtemp(prefix="pdf_pages_")
        single_page_files = []
        
        try:
            for page_num in range(total_pages):
                # åˆ›å»ºæ–°çš„PDFæ–‡æ¡£ï¼ŒåªåŒ…å«å½“å‰é¡µ
                single_page_doc = fitz.open()
                single_page_doc.insert_pdf(doc, from_page=page_num, to_page=page_num)
                
                # ä¿å­˜å•é¡µPDF
                single_page_path = os.path.join(temp_dir, f"page_{page_num + 1}.pdf")
                single_page_doc.save(single_page_path)
                single_page_doc.close()
                
                single_page_files.append((page_num + 1, single_page_path))
                
        finally:
            doc.close()
        
        return single_page_files
    
    def _get_optimal_worker_count(self, total_tasks: int) -> int:
        """
        æ ¹æ®ä»»åŠ¡æ•°é‡å’Œç³»ç»Ÿé…ç½®è®¡ç®—æœ€ä¼˜workeræ•°é‡
        
        ğŸ¤” ä»€ä¹ˆæ˜¯è¿›ç¨‹æ± å’Œçº¿ç¨‹æ± ï¼Ÿ
        
        ğŸ“š ç§‘æ™®æ—¶é—´ï¼š
        - è¿›ç¨‹æ±  (ProcessPoolExecutor)ï¼š
          ğŸ­ æƒ³è±¡æˆä¸€ä¸ªå·¥å‚ï¼Œæ¯ä¸ªå·¥äººï¼ˆè¿›ç¨‹ï¼‰éƒ½æœ‰ç‹¬ç«‹çš„å·¥ä½œå°å’Œå·¥å…·
          ğŸ’ª ä¼˜ç‚¹ï¼šæ¯ä¸ªå·¥äººå®Œå…¨ç‹¬ç«‹ï¼Œä¸€ä¸ªå‡ºé—®é¢˜ä¸å½±å“å…¶ä»–äººï¼Œèƒ½å……åˆ†åˆ©ç”¨å¤šæ ¸CPU
          ğŸŒ ç¼ºç‚¹ï¼šå¯åŠ¨å·¥äººéœ€è¦æ—¶é—´ï¼Œå·¥äººä¹‹é—´äº¤æµæ¯”è¾ƒéº»çƒ¦ï¼ˆéœ€è¦é€šè¿‡æ–‡ä»¶/ç®¡é“ï¼‰
          
        - çº¿ç¨‹æ±  (ThreadPoolExecutor)ï¼š
          ğŸ¢ æƒ³è±¡æˆä¸€ä¸ªå¼€æ”¾å¼åŠå…¬å®¤ï¼Œæ‰€æœ‰å‘˜å·¥ï¼ˆçº¿ç¨‹ï¼‰å…±äº«åŠå…¬è®¾å¤‡å’Œèµ„æ–™
          âš¡ ä¼˜ç‚¹ï¼šå¯åŠ¨å¿«ï¼Œå‘˜å·¥ä¹‹é—´äº¤æµæ–¹ä¾¿ï¼ˆå…±äº«å†…å­˜ï¼‰
          âš ï¸ ç¼ºç‚¹ï¼šä¸€ä¸ªå‘˜å·¥å‡ºå¤§é—®é¢˜å¯èƒ½å½±å“æ•´ä¸ªåŠå…¬å®¤ï¼Œå—Python GILé™åˆ¶
        
        ğŸ¯ å¯¹äºæˆ‘ä»¬çš„PDFå¤„ç†ä»»åŠ¡ï¼š
        - Doclingæ˜¯CPUå¯†é›†å‹ä»»åŠ¡ï¼ˆå¤§é‡å›¾åƒè¯†åˆ«ã€æ–‡å­—æå–ï¼‰
        - é€‰æ‹©è¿›ç¨‹æ± å¯ä»¥ç»•è¿‡Pythonçš„GILé™åˆ¶ï¼ŒçœŸæ­£å®ç°å¹¶è¡Œè®¡ç®—
        
        Args:
            total_tasks: æ€»ä»»åŠ¡æ•°é‡
            
        Returns:
            int: æœ€ä¼˜workeræ•°é‡
        """
        if self.use_process_pool:
            # ğŸ”¥ ä¸ºä»€ä¹ˆCPUæ ¸å¿ƒæ•°å¾ˆé‡è¦ï¼Ÿ
            # 
            # ğŸ’» ä½ çš„ç”µè„‘CPUæ ¸å¿ƒæƒ…å†µï¼š
            # - æ¯ä¸ªCPUæ ¸å¿ƒåŒæ—¶åªèƒ½æ‰§è¡Œä¸€ä¸ªè¿›ç¨‹çš„è®¡ç®—
            # - å¦‚æœè¿›ç¨‹æ•° > æ ¸å¿ƒæ•°ï¼Œä¼šå¯¼è‡´"æŠ¢å¤ºèµ„æº"ï¼Œåè€Œå˜æ…¢
            # 
            # âš ï¸ ä¸ºä»€ä¹ˆcpu_count-1ä¼šè®©ç”µè„‘"æ‹‰æ»¡"ï¼Ÿ
            # - å‡è®¾ä½ æœ‰8æ ¸CPUï¼Œå¼€8ä¸ªè¿›ç¨‹æ„å‘³ç€100%å ç”¨æ‰€æœ‰æ ¸å¿ƒ
            # - æ“ä½œç³»ç»Ÿã€æµè§ˆå™¨ã€å…¶ä»–è½¯ä»¶éƒ½æ²¡æœ‰CPUèµ„æºäº†
            # - ç”µè„‘ä¼šå˜å¾—å¡é¡¿ï¼Œé£æ‰‡ç‹‚è½¬ï¼Œæ¸©åº¦é£™å‡
            # 
            # ğŸ¯ åˆç†çš„è®¾ç½®ç­–ç•¥ï¼š
            # - ä½¿ç”¨æ ¸å¿ƒæ•°çš„ä¸€åŠï¼šç»™ç³»ç»Ÿå’Œå…¶ä»–ç¨‹åºç•™ç©ºé—´
            # - è¿™æ ·æ—¢èƒ½äº«å—å¹¶è¡Œå¤„ç†çš„é€Ÿåº¦æå‡ï¼Œåˆä¸ä¼šè®©ç”µè„‘"æ­»æœº"
            
            cpu_count = multiprocessing.cpu_count()
            # ğŸ“Š CPUè´Ÿè½½å½±å“åˆ†æï¼š
            # - 1æ ¸ï¼šé€‚åˆè½»é‡ä»»åŠ¡ï¼Œä½†é€Ÿåº¦æ…¢
            # - æ ¸å¿ƒæ•°/4ï¼šä¿å®ˆè®¾ç½®ï¼Œé€‚åˆåœ¨å·¥ä½œæ—¶åå°è¿è¡Œ
            # - æ ¸å¿ƒæ•°/2ï¼šå¹³è¡¡è®¾ç½®ï¼Œæ—¢å¿«åˆä¸å½±å“å…¶ä»–å·¥ä½œ
            # - æ ¸å¿ƒæ•°-1ï¼šæ¿€è¿›è®¾ç½®ï¼Œå‡ ä¹100%CPUä½¿ç”¨ï¼Œç”µè„‘å¯èƒ½å¡é¡¿
            # - æ ¸å¿ƒæ•°ï¼šå±é™©è®¾ç½®ï¼Œç³»ç»Ÿå¯èƒ½æ— å“åº”
            max_workers = max(1, cpu_count // 2)
            
            print(f"ğŸ§  CPUåˆ†æï¼šä½ çš„ç”µè„‘æœ‰{cpu_count}ä¸ªæ ¸å¿ƒ")
            print(f"âš–ï¸ ä¸ºäº†å¹³è¡¡æ€§èƒ½å’Œç¨³å®šæ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨{max_workers}ä¸ªè¿›ç¨‹")
            print(f"ğŸ“ˆ è¿™æ ·èƒ½æå‡çº¦{max_workers}å€çš„å¤„ç†é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒç”µè„‘å“åº”æµç•…")
            
        else:
            # ğŸ§µ çº¿ç¨‹æ± çš„ç‰¹æ®Šè€ƒè™‘ï¼š
            # 
            # ğŸ Pythonçš„GILï¼ˆå…¨å±€è§£é‡Šå™¨é”ï¼‰é™åˆ¶ï¼š
            # - å³ä½¿å¼€100ä¸ªçº¿ç¨‹ï¼ŒCPUå¯†é›†å‹ä»»åŠ¡å®é™…ä¸Šè¿˜æ˜¯ä¸²è¡Œæ‰§è¡Œ
            # - ä½†å¯¹äºI/Oå¯†é›†å‹ä»»åŠ¡ï¼ˆç½‘ç»œè¯·æ±‚ã€æ–‡ä»¶è¯»å†™ï¼‰ï¼Œçº¿ç¨‹æ± å¾ˆæœ‰ç”¨
            # - Doclingè™½ç„¶æ˜¯CPUå¯†é›†ï¼Œä½†ä¸­é—´æœ‰å¾ˆå¤šæ–‡ä»¶è¯»å†™æ“ä½œ
            # 
            # ğŸ’¡ ä¸ºä»€ä¹ˆé™åˆ¶ä¸º8ä¸ªçº¿ç¨‹ï¼Ÿ
            # - çº¿ç¨‹å¤ªå¤šä¼šå¢åŠ ä¸Šä¸‹æ–‡åˆ‡æ¢çš„å¼€é”€
            # - å¯¹äºæ··åˆå‹ä»»åŠ¡ï¼Œ8ä¸ªçº¿ç¨‹é€šå¸¸æ˜¯ç”œç‚¹
            max_workers = min(8, multiprocessing.cpu_count())
            
            print(f"ğŸ§µ ä½¿ç”¨çº¿ç¨‹æ± æ¨¡å¼ï¼Œåˆ›å»º{max_workers}ä¸ªçº¿ç¨‹")
            print(f"ğŸ’­ æ³¨æ„ï¼šç”±äºPython GILé™åˆ¶ï¼Œå®é™…CPUåˆ©ç”¨ç‡å¯èƒ½ä¸ä¼šæ»¡è½½")
        
        # ğŸ“ æœ€ç»ˆä¼˜åŒ–ï¼šä¸è¦åˆ›å»ºæ¯”ä»»åŠ¡æ›´å¤šçš„worker
        # å¦‚æœåªæœ‰3é¡µPDFï¼Œå¼€10ä¸ªè¿›ç¨‹å°±æ˜¯æµªè´¹èµ„æº
        optimal_workers = min(max_workers, total_tasks)
        
        print(f"ğŸ’» ç³»ç»Ÿé…ç½®: CPUæ ¸å¿ƒæ•°={multiprocessing.cpu_count()}, ä½¿ç”¨{'è¿›ç¨‹æ± ' if self.use_process_pool else 'çº¿ç¨‹æ± '}")
        print(f"âš™ï¸ å·¥ä½œè¿›ç¨‹/çº¿ç¨‹æ•°: {optimal_workers} (æ€»ä»»åŠ¡: {total_tasks})")
        
        # ğŸ“ å°è´´å£«ï¼šå¦‚ä½•è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Ÿ
        if optimal_workers == 1:
            print("ğŸ’¡ æç¤ºï¼šä»»åŠ¡è¾ƒå°‘ï¼Œè€ƒè™‘ä¸²è¡Œå¤„ç†å¯èƒ½æ›´ç®€å•")
        elif self.use_process_pool and optimal_workers >= multiprocessing.cpu_count() * 0.75:
            print("âš ï¸ æç¤ºï¼šä½¿ç”¨äº†è¾ƒå¤šCPUèµ„æºï¼Œå¤„ç†æœŸé—´ç”µè„‘å¯èƒ½ä¼šæ¯”è¾ƒå¡")
        
        # ğŸ›¡ï¸ å®‰å…¨æé†’ï¼šæœ€å¤§è®¾ç½®å»ºè®®
        max_safe_workers = multiprocessing.cpu_count() - 1
        if optimal_workers > max_safe_workers:
            print(f"ğŸš¨ è­¦å‘Šï¼šworkeræ•°é‡({optimal_workers})æ¥è¿‘CPUæ ¸å¿ƒæ•°({multiprocessing.cpu_count()})")
            print(f"ğŸ”§ å»ºè®®ï¼šæœ€å¤§ä¸è¦è¶…è¿‡{max_safe_workers}ä¸ªworkerï¼Œå¦åˆ™ç³»ç»Ÿå¯èƒ½æ— å“åº”")
            print(f"ğŸ’Š è§£å†³æ–¹æ¡ˆï¼šå¦‚éœ€æ›´é«˜æ€§èƒ½ï¼Œè€ƒè™‘ä½¿ç”¨æ›´å¼ºçš„ç¡¬ä»¶æˆ–åˆ†æ‰¹å¤„ç†")
        
        return optimal_workers
    
    def _process_pages_parallel(self, 
                               single_page_files: List[Tuple[int, str]], 
                               output_dir: str) -> List[PageData]:
        """
        å¹¶è¡Œå¤„ç†æ‰€æœ‰é¡µé¢ï¼ˆæ”¯æŒçº¿ç¨‹æ± å’Œè¿›ç¨‹æ± ï¼‰
        
        ğŸ¯ ä¸ºä»€ä¹ˆé€‰æ‹©å¹¶è¡Œå¤„ç†ï¼Ÿ
        
        ğŸ“Š æ€§èƒ½å¯¹æ¯”ï¼ˆä»¥12é¡µPDFä¸ºä¾‹ï¼‰ï¼š
        - ä¸²è¡Œå¤„ç†ï¼š12é¡µ Ã— 10ç§’/é¡µ = 120ç§’
        - 5è¿›ç¨‹å¹¶è¡Œï¼š12é¡µ Ã· 5è¿›ç¨‹ â‰ˆ 25ç§’ï¼ˆæå‡80%ï¼‰
        - ä½†è¦è€ƒè™‘ï¼šè¿›ç¨‹å¯åŠ¨å¼€é”€ + ç³»ç»Ÿç¨³å®šæ€§
        
        ğŸ¤” ä»€ä¹ˆæ—¶å€™ç”¨è¿›ç¨‹æ±  vs çº¿ç¨‹æ± ï¼Ÿ
        
        ğŸ’¡ è¿›ç¨‹æ± é€‚åˆçš„åœºæ™¯ï¼š
        - CPUå¯†é›†å‹ä»»åŠ¡ï¼ˆå›¾åƒå¤„ç†ã€æœºå™¨å­¦ä¹ æ¨ç†ï¼‰
        - ä»»åŠ¡ä¹‹é—´ç›¸äº’ç‹¬ç«‹
        - ä¸éœ€è¦é¢‘ç¹å…±äº«æ•°æ®
        - æˆ‘ä»¬çš„PDFå¤„ç†å®Œç¾ç¬¦åˆè¿™äº›æ¡ä»¶ï¼
        
        ğŸ§µ çº¿ç¨‹æ± é€‚åˆçš„åœºæ™¯ï¼š
        - I/Oå¯†é›†å‹ä»»åŠ¡ï¼ˆç½‘ç»œè¯·æ±‚ã€æ–‡ä»¶ä¸‹è½½ï¼‰
        - éœ€è¦é¢‘ç¹å…±äº«æ•°æ®
        - ä»»åŠ¡å¯åŠ¨/ç»“æŸå¾ˆé¢‘ç¹
        """
        
        # è®¡ç®—æœ€ä¼˜workeræ•°é‡
        optimal_workers = self._get_optimal_worker_count(len(single_page_files))
        
        print(f"âš¡ å¯ç”¨å¹¶è¡Œå¤„ç†æ¨¡å¼: {'è¿›ç¨‹æ± ' if self.use_process_pool else 'çº¿ç¨‹æ± '}")
        print(f"ğŸ”§ å·¥ä½œè¿›ç¨‹/çº¿ç¨‹æ•°: {optimal_workers}")
        
        # ğŸ­ ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªå¤æ‚çš„åˆ—è¡¨ï¼Ÿ
        # - å¹¶è¡Œå¤„ç†çš„ç»“æœå¯èƒ½ä¹±åºè¿”å›ï¼ˆé¡µé¢2å¯èƒ½æ¯”é¡µé¢1å…ˆå®Œæˆï¼‰
        # - æˆ‘ä»¬éœ€è¦æŒ‰é¡µç é¡ºåºé‡æ–°æ’åˆ—ç»“æœ
        # - é¢„åˆ†é…åˆ—è¡¨ç¡®ä¿æ¯ä¸ªé¡µé¢éƒ½æœ‰å›ºå®šä½ç½®
        pages_data: List[Optional[PageData]] = [None] * len(single_page_files)
        
        # ğŸ­ é€‰æ‹©åˆé€‚çš„"å·¥å‚"ç±»å‹
        executor_class = ProcessPoolExecutor if self.use_process_pool else ThreadPoolExecutor
        
        try:
            # ğŸª å¼€å§‹å¹¶è¡Œå¤„ç†çš„"é©¬æˆå›¢è¡¨æ¼”"
            with executor_class(max_workers=optimal_workers) as executor:
                # ğŸ“‹ ä»»åŠ¡åˆ†å‘ï¼šæŠŠå·¥ä½œåˆ†é…ç»™ä¸åŒçš„worker
                if self.use_process_pool:
                    # ğŸ­ è¿›ç¨‹æ± æ¨¡å¼ï¼š
                    # 
                    # âš ï¸ é‡è¦é™åˆ¶ï¼šè¿›ç¨‹ä¹‹é—´ä¸èƒ½ç›´æ¥å…±äº«å¯¹è±¡ï¼
                    # - ä¸èƒ½ä¼ é€’ selfï¼ˆç±»å®ä¾‹ï¼‰
                    # - å¿…é¡»ä½¿ç”¨ç‹¬ç«‹çš„é™æ€å‡½æ•°
                    # - æ‰€æœ‰å‚æ•°éƒ½è¦èƒ½"åºåˆ—åŒ–"ï¼ˆè½¬æ¢æˆäºŒè¿›åˆ¶æ•°æ®ä¼ è¾“ï¼‰
                    # 
                    # ğŸ’¾ æ•°æ®ä¼ è¾“å¼€é”€ï¼š
                    # - æ¯ä¸ªè¿›ç¨‹å¯åŠ¨æ—¶éƒ½è¦ä¼ é€’configå¯¹è±¡
                    # - è¿›ç¨‹é—´é€šä¿¡é€šè¿‡ç®¡é“/å…±äº«å†…å­˜
                    # - è¿™å°±æ˜¯ä¸ºä»€ä¹ˆè¿›ç¨‹å¯åŠ¨æ¯”çº¿ç¨‹æ…¢çš„åŸå› 
                    
                    future_to_page = {
                        executor.submit(
                            _process_single_page_static,  # ğŸ“ è°ƒç”¨ç‹¬ç«‹çš„é™æ€å‡½æ•°
                            page_num, 
                            single_page_path, 
                            output_dir,
                            self.config  # ğŸšš é…ç½®å¯¹è±¡ä¼šè¢«"æ‰“åŒ…"å‘é€ç»™å­è¿›ç¨‹
                        ): page_num
                        for page_num, single_page_path in single_page_files
                    }
                    
                    print(f"ğŸš€ å·²å‘{optimal_workers}ä¸ªç‹¬ç«‹è¿›ç¨‹åˆ†å‘{len(single_page_files)}ä¸ªä»»åŠ¡")
                    print(f"ğŸ’¡ æ¯ä¸ªè¿›ç¨‹éƒ½æ˜¯ç‹¬ç«‹çš„Pythonè§£é‡Šå™¨ï¼Œå¯ä»¥å……åˆ†åˆ©ç”¨CPUæ ¸å¿ƒ")
                    
                else:
                    # ğŸ§µ çº¿ç¨‹æ± æ¨¡å¼ï¼š
                    # 
                    # âœ… ä¾¿åˆ©æ€§ï¼šå¯ä»¥ç›´æ¥è°ƒç”¨ç±»æ–¹æ³•
                    # - æ‰€æœ‰çº¿ç¨‹å…±äº«åŒä¸€ä¸ªå†…å­˜ç©ºé—´
                    # - å¯ä»¥ç›´æ¥è®¿é—® self å’Œæ‰€æœ‰å®ä¾‹å˜é‡
                    # - æ•°æ®ä¼ é€’å‡ ä¹æ²¡æœ‰å¼€é”€
                    # 
                    # âš ï¸ GILé™åˆ¶ï¼š
                    # - Pythonçš„å…¨å±€è§£é‡Šå™¨é”æ„å‘³ç€åŒæ—¶åªæœ‰ä¸€ä¸ªçº¿ç¨‹åœ¨æ‰§è¡ŒPythonä»£ç 
                    # - å¯¹äºçº¯CPUä»»åŠ¡ï¼Œå¤šçº¿ç¨‹å¯èƒ½ä¸ä¼šå¸¦æ¥é€Ÿåº¦æå‡
                    # - ä½†doclingä¸­æœ‰å¾ˆå¤šCæ‰©å±•å’ŒI/Oæ“ä½œï¼Œè¿™äº›å¯ä»¥é‡Šæ”¾GIL
                    
                    future_to_page = {
                        executor.submit(
                            self._process_single_page,  # ğŸ“ ç›´æ¥è°ƒç”¨å®ä¾‹æ–¹æ³•
                            page_num, 
                            single_page_path, 
                            output_dir
                        ): page_num
                        for page_num, single_page_path in single_page_files
                    }
                    
                    print(f"ğŸ§µ å·²å‘{optimal_workers}ä¸ªçº¿ç¨‹åˆ†å‘{len(single_page_files)}ä¸ªä»»åŠ¡")
                    print(f"ğŸ’­ æ‰€æœ‰çº¿ç¨‹å…±äº«å†…å­˜ï¼Œä½†å—Python GILé™åˆ¶å¯èƒ½æ— æ³•æ»¡è½½CPU")
                
                # ğŸ­ æ”¶é›†è¡¨æ¼”ç»“æœï¼šç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                completed_count = 0
                for future in as_completed(future_to_page):
                    page_num = future_to_page[future]
                    try:
                        page_data = future.result()
                        pages_data[page_num - 1] = page_data  # é¡µç ä»1å¼€å§‹ï¼Œç´¢å¼•ä»0å¼€å§‹
                        completed_count += 1
                        print(f"âœ… é¡µé¢ {page_num} å¤„ç†å®Œæˆ ({completed_count}/{len(single_page_files)})")
                    except Exception as e:
                        # ğŸ›¡ï¸ å®¹é”™å¤„ç†ï¼šå•é¡µå¤±è´¥ä¸å½±å“å…¶ä»–é¡µé¢
                        print(f"âŒ é¡µé¢ {page_num} å¤„ç†å¤±è´¥: {e}")
                        # åˆ›å»ºç©ºçš„é¡µé¢æ•°æ®ä½œä¸ºfallback
                        pages_data[page_num - 1] = PageData(
                            page_number=page_num,
                            raw_text=f"é¡µé¢ {page_num} å¤„ç†å¤±è´¥: {str(e)}",
                            images=[],
                            tables=[]
                        )
                        completed_count += 1
                        
        except Exception as e:
            # ğŸš¨ ç³»ç»Ÿçº§é”™è¯¯ï¼šå¹¶è¡Œå¤„ç†å®Œå…¨å¤±è´¥
            print(f"âŒ å¹¶è¡Œå¤„ç†æ‰§è¡Œå™¨åˆ›å»ºå¤±è´¥: {e}")
            print(f"ğŸ’¡ å¯èƒ½åŸå› ï¼šç³»ç»Ÿèµ„æºä¸è¶³ã€æƒé™é—®é¢˜ã€æˆ–è¿›ç¨‹æ± åˆå§‹åŒ–å¤±è´¥")
            # å›é€€åˆ°ä¸²è¡Œå¤„ç†
            print("ğŸ”„ å›é€€åˆ°ä¸²è¡Œå¤„ç†æ¨¡å¼...")
            return self._process_pages_sequential(single_page_files, output_dir)
        
        # ğŸ¯ æœ€ç»ˆæ•´ç†ï¼šç¡®ä¿ç»“æœçš„å®Œæ•´æ€§å’Œé¡ºåº
        successful_pages: List[PageData] = [page for page in pages_data if page is not None]
        successful_pages.sort(key=lambda x: x.page_number)
        
        print(f"ğŸ“Š å¹¶è¡Œå¤„ç†å®Œæˆ: {len(successful_pages)}/{len(single_page_files)} é¡µé¢æˆåŠŸå¤„ç†")
        
        # ğŸ“ˆ æ€§èƒ½æç¤º
        if self.use_process_pool:
            theoretical_speedup = min(optimal_workers, len(single_page_files))
            print(f"ğŸš€ ç†è®ºåŠ é€Ÿæ¯”: {theoretical_speedup}xï¼ˆå®é™…å¯èƒ½å› è¿›ç¨‹å¯åŠ¨å¼€é”€ç•¥ä½ï¼‰")
        
        return successful_pages
    
    def _process_pages_sequential(self, 
                                 single_page_files: List[Tuple[int, str]], 
                                 output_dir: str) -> List[PageData]:
        """é¡ºåºå¤„ç†æ‰€æœ‰é¡µé¢"""
        print("ğŸ”„ ä½¿ç”¨é¡ºåºå¤„ç†æ¨¡å¼")
        
        pages_data: List[PageData] = []
        for page_num, single_page_path in single_page_files:
            try:
                page_data = self._process_single_page(page_num, single_page_path, output_dir)
                pages_data.append(page_data)
                print(f"âœ… é¡µé¢ {page_num} å¤„ç†å®Œæˆ")
            except Exception as e:
                print(f"âŒ é¡µé¢ {page_num} å¤„ç†å¤±è´¥: {e}")
                # åˆ›å»ºç©ºçš„é¡µé¢æ•°æ®
                pages_data.append(PageData(
                    page_number=page_num,
                    raw_text=f"é¡µé¢ {page_num} å¤„ç†å¤±è´¥: {str(e)}",
                    images=[],
                    tables=[]
                ))
        
        return pages_data
    
    def _process_single_page(self, 
                            page_num: int, 
                            single_page_path: str, 
                            output_dir: str) -> PageData:
        """
        å¤„ç†å•é¡µPDF
        
        Args:
            page_num: é¡µç 
            single_page_path: å•é¡µPDFæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            PageData: é¡µé¢æ•°æ®
        """
        if self.doc_converter is None:
            raise RuntimeError("Doclingè½¬æ¢å™¨æœªåˆå§‹åŒ–")
            
        # åˆ›å»ºé¡µé¢ä¸“ç”¨çš„è¾“å‡ºç›®å½•
        page_output_dir = os.path.join(output_dir, f"page_{page_num}")
        os.makedirs(page_output_dir, exist_ok=True)
        
        # ä½¿ç”¨Doclingå¤„ç†å•é¡µPDF
        raw_result = self.doc_converter.convert(Path(single_page_path))
        
        # æå–é¡µé¢æ–‡æœ¬
        page_text = self._extract_page_text(raw_result)
        
        # åˆ›å»ºé¡µé¢æ•°æ®
        page_data = PageData(
            page_number=page_num,
            raw_text=page_text,
            images=[],
            tables=[]
        )
        
        # æå–å›¾ç‰‡å’Œè¡¨æ ¼ - ç”±äºæ˜¯å•é¡µPDFï¼Œæ‰€æœ‰åª’ä½“éƒ½å±äºå½“å‰é¡µ
        self._extract_media_for_single_page(raw_result, page_data, page_output_dir)
        
        return page_data
    
    @staticmethod
    def _extract_smart_context(page_text: str, media_type: str, media_index: int) -> str:
        """
        æ™ºèƒ½æå–åª’ä½“å…ƒç´ çš„ä¸Šä¸‹æ–‡
        
        æ ¹æ®é¡µé¢æ–‡æœ¬çš„é•¿åº¦å’Œå†…å®¹ï¼Œæä¾›åˆé€‚çš„ä¸Šä¸‹æ–‡ï¼š
        - çŸ­é¡µé¢ï¼ˆ<200å­—ç¬¦ï¼‰ï¼šè¿”å›å®Œæ•´é¡µé¢æ–‡æœ¬
        - ä¸­ç­‰é¡µé¢ï¼ˆ200-500å­—ç¬¦ï¼‰ï¼šè¿”å›é¡µé¢æ–‡æœ¬ä½†é™åˆ¶é•¿åº¦
        - é•¿é¡µé¢ï¼ˆ>500å­—ç¬¦ï¼‰ï¼šè¿”å›å…³é”®æ®µè½æˆ–å‰åæ–‡æœ¬ç‰‡æ®µ
        
        Args:
            page_text: é¡µé¢å®Œæ•´æ–‡æœ¬
            media_type: åª’ä½“ç±»å‹ï¼ˆ"image"æˆ–"table"ï¼‰
            media_index: åª’ä½“ç´¢å¼•
            
        Returns:
            str: æ™ºèƒ½æå–çš„ä¸Šä¸‹æ–‡
        """
        if not page_text or not page_text.strip():
            return ""
        
        text = page_text.strip()
        
        # çŸ­é¡µé¢ï¼šç›´æ¥è¿”å›å®Œæ•´æ–‡æœ¬
        if len(text) <= 200:
            return text
        
        # ä¸­ç­‰é¡µé¢ï¼šè¿”å›å‰300å­—ç¬¦
        elif len(text) <= 500:
            return text[:300] + "..." if len(text) > 300 else text
        
        # é•¿é¡µé¢ï¼šå°è¯•æ‰¾åˆ°æœ‰æ„ä¹‰çš„æ®µè½
        else:
            # æŒ‰æ®µè½åˆ†å‰²
            paragraphs = [p.strip() for p in text.split('\n') if p.strip()]
            
            if not paragraphs:
                return text[:300] + "..."
            
            # å¦‚æœæœ‰å¤šä¸ªæ®µè½ï¼Œé€‰æ‹©å‰å‡ ä¸ªæ®µè½
            context_parts = []
            current_length = 0
            target_length = 300
            
            for para in paragraphs:
                if current_length + len(para) > target_length and context_parts:
                    break
                context_parts.append(para)
                current_length += len(para)
            
            if context_parts:
                return '\n'.join(context_parts)
            else:
                return text[:300] + "..."
    
    def _extract_page_text(self, raw_result: Any) -> str:
        """ä»å•é¡µPDFçš„doclingç»“æœä¸­æå–æ–‡æœ¬ - å¤šç­–ç•¥æå–"""
        try:
            page_text = ""
            
            # ğŸ¯ ç­–ç•¥1ï¼šä¼˜å…ˆå°è¯•export_to_text()æ–¹æ³•ï¼ˆæœ€ç›´æ¥çš„æ–‡æœ¬æå–ï¼‰
            if hasattr(raw_result.document, 'export_to_text'):
                try:
                    page_text = raw_result.document.export_to_text()
                    if page_text and page_text.strip():
                        print(f"âœ… ä½¿ç”¨export_to_text()æˆåŠŸæå–æ–‡æœ¬: {len(page_text)}å­—ç¬¦")
                        return page_text.strip()
                except Exception as e:
                    print(f"âš ï¸ export_to_text()æ–¹æ³•å¤±è´¥: {e}")
            
            # ğŸ¯ ç­–ç•¥2ï¼šç›´æ¥éå†document.textsé›†åˆï¼ˆæ¨èæ–¹æ³•ï¼‰
            if hasattr(raw_result.document, 'texts') and raw_result.document.texts:
                try:
                    text_parts = []
                    for text_item in raw_result.document.texts:
                        if hasattr(text_item, 'text') and text_item.text:
                            text_parts.append(text_item.text)
                    
                    if text_parts:
                        page_text = '\n'.join(text_parts)
                        print(f"âœ… ä»textsé›†åˆæå–æ–‡æœ¬: {len(text_parts)}ä¸ªæ–‡æœ¬é¡¹, {len(page_text)}å­—ç¬¦")
                        return page_text.strip()
                except Exception as e:
                    print(f"âš ï¸ éå†textsé›†åˆå¤±è´¥: {e}")
            
            # ğŸ¯ ç­–ç•¥3ï¼šä½¿ç”¨export_to_markdown()ä½œä¸ºå¤‡é€‰
            if hasattr(raw_result.document, 'export_to_markdown'):
                try:
                    raw_markdown = raw_result.document.export_to_markdown()
                    
                    # æ¸…ç†markdownå†…å®¹
                    import re
                    markdown_clean_pattern = re.compile(r"<!--[\s\S]*?-->")
                    cleaned_text = markdown_clean_pattern.sub("", raw_markdown)
                    
                    if cleaned_text and cleaned_text.strip():
                        print(f"âœ… ä½¿ç”¨export_to_markdown()æå–æ–‡æœ¬: {len(cleaned_text)}å­—ç¬¦")
                        return cleaned_text.strip()
                except Exception as e:
                    print(f"âš ï¸ export_to_markdown()æ–¹æ³•å¤±è´¥: {e}")
            
            # ğŸ¯ ç­–ç•¥4ï¼šå°è¯•ä»documentå±æ€§ä¸­ç›´æ¥æå–
            if hasattr(raw_result.document, 'text'):
                try:
                    doc_text = getattr(raw_result.document, 'text', None)
                    if doc_text and doc_text.strip():
                        print(f"âœ… ä»document.textå±æ€§æå–æ–‡æœ¬: {len(doc_text)}å­—ç¬¦")
                        return doc_text.strip()
                except Exception as e:
                    print(f"âš ï¸ document.textå±æ€§è®¿é—®å¤±è´¥: {e}")
            
            # ğŸ¯ ç­–ç•¥5ï¼šæœ€åçš„è¯Šæ–­ä¿¡æ¯
            print("ğŸ” å¯ç”¨çš„documentå±æ€§:", [attr for attr in dir(raw_result.document) if not attr.startswith('_')])
            print("âŒ æ‰€æœ‰æ–‡æœ¬æå–ç­–ç•¥éƒ½å¤±è´¥ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²")
            return ""
            
        except Exception as e:
            print(f"âŒ é¡µé¢æ–‡æœ¬æå–è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸: {e}")
            return ""
    
    def _extract_media_for_single_page(self, 
                                      raw_result: Any, 
                                      page_data: PageData, 
                                      page_output_dir: str):
        """ä¸ºå•é¡µPDFæå–å›¾ç‰‡å’Œè¡¨æ ¼"""
        try:
            # ç”±äºæ˜¯å•é¡µPDFï¼Œæ‰€æœ‰åª’ä½“éƒ½å±äºå½“å‰é¡µ
            image_counter = 0
            table_counter = 0
            
            # æå–å›¾ç‰‡
            for picture in raw_result.document.pictures:
                image_counter += 1
                try:
                    # è·å–å›¾ç‰‡
                    picture_image = picture.get_image(raw_result.document)
                    if picture_image is None:
                        continue
                    
                    # ä¿å­˜å›¾ç‰‡
                    image_filename = f"picture-{image_counter}.png"
                    image_path = os.path.join(page_output_dir, image_filename)
                    with open(image_path, "wb") as fp:
                        picture_image.save(fp, "PNG")
                    
                    # è·å–å›¾ç‰‡ä¿¡æ¯
                    from PIL import Image
                    image_img = Image.open(image_path)
                    caption = picture.caption_text(raw_result.document) if hasattr(picture, 'caption_text') else ""
                    
                    # åˆ›å»ºImageWithContextå¯¹è±¡ - ä½¿ç”¨æ™ºèƒ½ä¸Šä¸‹æ–‡æå–
                    smart_context = Stage1DoclingProcessor._extract_smart_context(page_data.raw_text, "image", image_counter)
                    image_with_context = ImageWithContext(
                        image_path=image_path,
                        page_number=page_data.page_number,  # ğŸŒŸ ç¡®ä¿é¡µç å‡†ç¡®
                        page_context=smart_context,         # ğŸŒŸ ä½¿ç”¨æ™ºèƒ½ä¸Šä¸‹æ–‡è€Œä¸æ˜¯æ•´é¡µæ–‡æœ¬
                        caption=caption or f"å›¾ç‰‡ {image_counter}",
                        metadata={
                            'width': image_img.width,
                            'height': image_img.height,
                            'size': image_img.width * image_img.height,
                            'aspect_ratio': image_img.width / image_img.height
                        }
                    )
                    
                    page_data.images.append(image_with_context)
                    
                except Exception as e:
                    print(f"âŒ é¡µé¢ {page_data.page_number} å›¾ç‰‡ {image_counter} å¤„ç†å¤±è´¥: {e}")
            
            # æå–è¡¨æ ¼
            for table in raw_result.document.tables:
                table_counter += 1
                try:
                    # è·å–è¡¨æ ¼å›¾ç‰‡
                    table_image = table.get_image(raw_result.document)
                    if table_image is None:
                        continue
                    
                    # ä¿å­˜è¡¨æ ¼å›¾ç‰‡
                    table_filename = f"table-{table_counter}.png"
                    table_path = os.path.join(page_output_dir, table_filename)
                    with open(table_path, "wb") as fp:
                        table_image.save(fp, "PNG")
                    
                    # è·å–è¡¨æ ¼ä¿¡æ¯
                    from PIL import Image
                    table_img = Image.open(table_path)
                    caption = table.caption_text(raw_result.document) if hasattr(table, 'caption_text') else ""
                    
                    # åˆ›å»ºTableWithContextå¯¹è±¡ - ä½¿ç”¨æ™ºèƒ½ä¸Šä¸‹æ–‡æå–
                    smart_context = Stage1DoclingProcessor._extract_smart_context(page_data.raw_text, "table", table_counter)
                    table_with_context = TableWithContext(
                        table_path=table_path,
                        page_number=page_data.page_number,  # ğŸŒŸ ç¡®ä¿é¡µç å‡†ç¡®
                        page_context=smart_context,         # ğŸŒŸ ä½¿ç”¨æ™ºèƒ½ä¸Šä¸‹æ–‡è€Œä¸æ˜¯æ•´é¡µæ–‡æœ¬
                        caption=caption or f"è¡¨æ ¼ {table_counter}",
                        metadata={
                            'width': table_img.width,
                            'height': table_img.height,
                            'size': table_img.width * table_img.height,
                            'aspect_ratio': table_img.width / table_img.height
                        }
                    )
                    
                    page_data.tables.append(table_with_context)
                    
                except Exception as e:
                    print(f"âŒ é¡µé¢ {page_data.page_number} è¡¨æ ¼ {table_counter} å¤„ç†å¤±è´¥: {e}")
            
            print(f"ğŸ“Š é¡µé¢ {page_data.page_number}: {len(page_data.images)} ä¸ªå›¾ç‰‡, {len(page_data.tables)} ä¸ªè¡¨æ ¼")
            
        except Exception as e:
            print(f"âŒ é¡µé¢ {page_data.page_number} åª’ä½“æå–å¤±è´¥: {e}")
    
    def _cleanup_temp_files(self, single_page_files: List[Tuple[int, str]]):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        if not single_page_files:
            return
        
        # è·å–ä¸´æ—¶ç›®å½•
        temp_dir = os.path.dirname(single_page_files[0][1])
        
        try:
            shutil.rmtree(temp_dir)
            print(f"ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {temp_dir}")
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
    
    def _generate_full_raw_text_with_media_markers(self, pages: List[PageData]) -> Tuple[str, Dict[str, str]]:
        """
        ç”ŸæˆåŒ…å«åª’ä½“æ ‡è®°çš„å®Œæ•´æ–‡æœ¬ï¼ŒåŒæ—¶ä¿å­˜æ¯é¡µçš„åŸå§‹æ–‡æœ¬
        
        åŸºäºé¡µé¢æ•°æ®ç”Ÿæˆfull_raw_textï¼Œå¹¶åœ¨é€‚å½“ä½ç½®æ’å…¥å›¾ç‰‡è¡¨æ ¼æ ‡è®°
        åŒæ—¶ä¿å­˜æ¯é¡µçš„å®Œæ•´åŸå§‹æ–‡æœ¬ç”¨äºç²¾ç¡®çš„ä¸Šä¸‹æ–‡æå–
        
        Args:
            pages: é¡µé¢æ•°æ®åˆ—è¡¨ (PageData objects)
            
        Returns:
            Tuple[str, Dict[str, str]]: (åŒ…å«åª’ä½“æ ‡è®°çš„å®Œæ•´åŸå§‹æ–‡æœ¬, é¡µç ->é¡µé¢åŸå§‹æ–‡æœ¬çš„å­—å…¸)
        """
        full_text_parts = []
        page_texts = {}  # é¡µç  -> å®Œæ•´é¡µé¢åŸå§‹æ–‡æœ¬ï¼Œç”¨äºç²¾ç¡®çš„ä¸Šä¸‹æ–‡æå–
        
        for page in pages:
            page_num = page.page_number
            page_text = page.raw_text or ""
            
            # ä¿å­˜åŸå§‹é¡µé¢æ–‡æœ¬ï¼ˆä¸å«åª’ä½“æ ‡è®°ï¼Œç”¨äºç²¾ç¡®çš„ä¸Šä¸‹æ–‡æå–ï¼‰
            page_texts[str(page_num)] = page_text
            
            # åœ¨é¡µé¢æ–‡æœ¬ä¸­æ’å…¥åª’ä½“æ ‡è®°
            text_with_markers = page_text
            
            # æ’å…¥å›¾ç‰‡æ ‡è®°
            for img in page.images:
                img_marker = f"\n[IMAGE:{img.image_path}:CAPTION:{img.caption or 'No caption'}]\n"
                text_with_markers += img_marker
            
            # æ’å…¥è¡¨æ ¼æ ‡è®°
            for table in page.tables:
                table_marker = f"\n[TABLE:{table.table_path}:CAPTION:{table.caption or 'No caption'}]\n"
                text_with_markers += table_marker
            
            full_text_parts.append(f"=== Page {page_num} ===\n{text_with_markers}\n")
        
        full_raw_text = "\n".join(full_text_parts)
        
        print(f"ğŸ“„ ç”Ÿæˆfull_raw_text: {len(full_raw_text)} å­—ç¬¦")
        print(f"ğŸ“„ ä¿å­˜page_texts: {len(page_texts)} é¡µ")
        return full_raw_text, page_texts
    
    def _populate_final_schema(self, 
                              final_schema: FinalMetadataSchema,
                              pdf_path: str,
                              full_raw_text: str,
                              page_texts: Dict[str, str],
                              pages: List[PageData],
                              stage_start_time: float):
        """
        å¡«å……Final Schemaçš„åŸºç¡€ä¿¡æ¯
        
        Args:
            final_schema: Final Schemaå¯¹è±¡
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            full_raw_text: å®Œæ•´åŸå§‹æ–‡æœ¬
            page_texts: é¡µç ->é¡µé¢åŸå§‹æ–‡æœ¬çš„å­—å…¸
            pages: é¡µé¢æ•°æ®åˆ—è¡¨ (PageData objects)
            stage_start_time: é˜¶æ®µå¼€å§‹æ—¶é—´
        """
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_images = sum(len(page.images) for page in pages)
        total_tables = sum(len(page.tables) for page in pages)
        total_pages = len(pages)
        
        # å¡«å……DocumentSummaryChunk
        doc_id = final_schema.document_id
        final_schema.document_summary = DocumentSummaryChunk(
            content_id=f"{doc_id}_document_summary_1",
            document_id=doc_id,
            content=full_raw_text,  # ğŸŒŸ å…³é”®ï¼šä¿å­˜full_raw_text
            page_texts=page_texts,  # ğŸŒŸ æ–°å¢ï¼šä¿å­˜æ¯é¡µçš„å®Œæ•´åŸå§‹æ–‡æœ¬
            source_file_path=pdf_path,
            file_name=os.path.basename(pdf_path),
            file_size=os.path.getsize(pdf_path),
            total_pages=total_pages,
            image_count=total_images,
            table_count=total_tables,
            processing_time=time.time() - stage_start_time
        )
        
        # å¡«å……ImageChunks
        img_counter = 1
        for page in pages:
            for img in page.images:
                image_chunk = ImageChunk(
                    content_id=f"{doc_id}_image_{img_counter}",
                    document_id=doc_id,
                    image_path=img.image_path,
                    page_number=img.page_number,  # ğŸŒŸ é¡µç ç°åœ¨æ˜¯å‡†ç¡®çš„
                    caption=img.caption or "",
                    page_context=img.page_context,  # ğŸŒŸ ä¸Šä¸‹æ–‡ç°åœ¨æ˜¯å‡†ç¡®çš„
                    width=img.metadata.get("width", 0),
                    height=img.metadata.get("height", 0),
                    size=img.metadata.get("size", 0),
                    aspect_ratio=img.metadata.get("aspect_ratio", 0.0)
                    # ai_description å’Œ chapter_id ç•™ç©ºï¼Œç­‰å¾…åç»­é˜¶æ®µå¡«å……
                )
                final_schema.image_chunks.append(image_chunk)
                img_counter += 1
        
        # å¡«å……TableChunks  
        table_counter = 1
        for page in pages:
            for table in page.tables:
                table_chunk = TableChunk(
                    content_id=f"{doc_id}_table_{table_counter}",
                    document_id=doc_id,
                    table_path=table.table_path,
                    page_number=table.page_number,  # ğŸŒŸ é¡µç ç°åœ¨æ˜¯å‡†ç¡®çš„
                    caption=table.caption or "",
                    page_context=table.page_context,  # ğŸŒŸ ä¸Šä¸‹æ–‡ç°åœ¨æ˜¯å‡†ç¡®çš„
                    width=table.metadata.get("width", 0),
                    height=table.metadata.get("height", 0),
                    size=table.metadata.get("size", 0),
                    aspect_ratio=table.metadata.get("aspect_ratio", 0.0)
                    # ai_description å’Œ chapter_id ç•™ç©ºï¼Œç­‰å¾…åç»­é˜¶æ®µå¡«å……
                )
                final_schema.table_chunks.append(table_chunk)
                table_counter += 1
        
        print(f"ğŸ“Š å¡«å……å®Œæˆ:")
        print(f"   ğŸ“„ æ–‡æ¡£æ‘˜è¦: 1ä¸ª (åŒ…å«full_raw_text)")
        print(f"   ğŸ–¼ï¸ å›¾ç‰‡chunks: {len(final_schema.image_chunks)}ä¸ª")
        print(f"   ğŸ“‹ è¡¨æ ¼chunks: {len(final_schema.table_chunks)}ä¸ª")
        print(f"   âœ… æ‰€æœ‰é¡µç å’Œä¸Šä¸‹æ–‡éƒ½å·²å‡†ç¡®æ ‡æ³¨")
    
    def can_resume(self, output_dir: str) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦å¯ä»¥è·³è¿‡é˜¶æ®µ1ï¼ˆå·²ç»å®Œæˆï¼‰
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            bool: æ˜¯å¦å¯ä»¥è·³è¿‡
        """
        final_metadata_path = os.path.join(output_dir, "final_metadata.json")
        
        if not os.path.exists(final_metadata_path):
            return False
            
        try:
            final_schema = FinalMetadataSchema.load(final_metadata_path)
            return final_schema.is_stage_complete("stage1")
        except Exception:
            return False
    
    def resume_from_existing(self, output_dir: str) -> Tuple[FinalMetadataSchema, str]:
        """
        ä»ç°æœ‰çš„final_metadata.jsonæ¢å¤
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            Tuple[FinalMetadataSchema, str]: (final_schemaå¯¹è±¡, final_metadata.jsonæ–‡ä»¶è·¯å¾„)
        """
        final_metadata_path = os.path.join(output_dir, "final_metadata.json")
        
        if not self.can_resume(output_dir):
            raise ValueError(f"æ— æ³•ä» {final_metadata_path} æ¢å¤ï¼Œé˜¶æ®µ1æœªå®Œæˆ")
        
        final_schema = FinalMetadataSchema.load(final_metadata_path)
        print(f"âœ… ä»ç°æœ‰æ–‡ä»¶æ¢å¤é˜¶æ®µ1ç»“æœ: {final_metadata_path}")
        print(f"ğŸ“Š å½“å‰å®Œæˆåº¦: {final_schema.get_completion_percentage()}%")
        
        return final_schema, final_metadata_path


def _process_single_page_static(page_num: int, 
                               single_page_path: str, 
                               output_dir: str,
                               config: PDFProcessingConfig) -> PageData:
    """
    å¤„ç†å•é¡µPDFçš„é™æ€å‡½æ•°ï¼ˆç”¨äºè¿›ç¨‹æ± ï¼‰
    
    Args:
        page_num: é¡µç 
        single_page_path: å•é¡µPDFæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        config: PDFå¤„ç†é…ç½®
        
    Returns:
        PageData: é¡µé¢æ•°æ®
    """
    try:
        # åœ¨è¿›ç¨‹å†…åˆå§‹åŒ–doclingè½¬æ¢å™¨
        from docling.datamodel.base_models import InputFormat
        from docling.datamodel.pipeline_options import PdfPipelineOptions, EasyOcrOptions
        from docling.document_converter import DocumentConverter, PdfFormatOption
        
        # è®¾ç½®æœ¬åœ°æ¨¡å‹è·¯å¾„
        models_cache_dir = Path("models_cache")
        artifacts_path = None
        if models_cache_dir.exists():
            artifacts_path = str(models_cache_dir.absolute())
        elif config.docling.artifacts_path:
            artifacts_path = config.docling.artifacts_path
        
        # åˆ›å»ºç®¡é“é€‰é¡¹
        if config.docling.ocr_enabled:
            # ğŸ”¥ ç›´æ¥è®¾ç½®OCRé…ç½®ï¼Œæ ¸å¿ƒè®¾ç½®ï¼šforce_full_page_ocr = True
            ocr_options = EasyOcrOptions()
            
            # åº”ç”¨å…³é”®çš„OCRè®¾ç½®
            try:
                ocr_options.force_full_page_ocr = True  # å¼ºåˆ¶å…¨é¡µOCR
                print(f"âœ… è¿›ç¨‹ {page_num}: åº”ç”¨å…³é”®OCRè®¾ç½® (force_full_page_ocr=True)")
            except Exception as e:
                print(f"âš ï¸ è¿›ç¨‹ {page_num}: åº”ç”¨å…³é”®OCRè®¾ç½®å¤±è´¥: {e}")
            
            pipeline_options = PdfPipelineOptions(
                ocr_options=ocr_options,
                artifacts_path=artifacts_path
            )
        else:
            pipeline_options = PdfPipelineOptions(
                artifacts_path=artifacts_path
            )
        
        # è®¾ç½®è§£æé€‰é¡¹
        pipeline_options.images_scale = config.docling.images_scale
        pipeline_options.generate_page_images = config.docling.generate_page_images
        pipeline_options.generate_picture_images = config.docling.generate_picture_images
        
        # åˆ›å»ºæ–‡æ¡£è½¬æ¢å™¨
        doc_converter = DocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
            }
        )
        
        # åˆ›å»ºé¡µé¢ä¸“ç”¨çš„è¾“å‡ºç›®å½•
        page_output_dir = os.path.join(output_dir, f"page_{page_num}")
        os.makedirs(page_output_dir, exist_ok=True)
        
        # ä½¿ç”¨Doclingå¤„ç†å•é¡µPDF
        raw_result = doc_converter.convert(Path(single_page_path))
        
        # æå–é¡µé¢æ–‡æœ¬ - å¤šç­–ç•¥æå–ï¼ˆä¸å®ä¾‹æ–¹æ³•ä¿æŒä¸€è‡´ï¼‰
        try:
            page_text = ""
            
            # ç­–ç•¥1ï¼šä¼˜å…ˆå°è¯•export_to_text()æ–¹æ³•
            if hasattr(raw_result.document, 'export_to_text'):
                try:
                    page_text = raw_result.document.export_to_text()
                    if page_text and page_text.strip():
                        page_text = page_text.strip()
                except Exception:
                    pass
            
            # ç­–ç•¥2ï¼šç›´æ¥éå†document.textsé›†åˆ
            if not page_text and hasattr(raw_result.document, 'texts') and raw_result.document.texts:
                try:
                    text_parts = []
                    for text_item in raw_result.document.texts:
                        if hasattr(text_item, 'text') and text_item.text:
                            text_parts.append(text_item.text)
                    
                    if text_parts:
                        page_text = '\n'.join(text_parts).strip()
                except Exception:
                    pass
            
            # ç­–ç•¥3ï¼šä½¿ç”¨export_to_markdown()ä½œä¸ºå¤‡é€‰
            if not page_text and hasattr(raw_result.document, 'export_to_markdown'):
                try:
                    raw_markdown = raw_result.document.export_to_markdown()
                    import re
                    markdown_clean_pattern = re.compile(r"<!--[\s\S]*?-->")
                    page_text = markdown_clean_pattern.sub("", raw_markdown).strip()
                except Exception:
                    pass
            
            # ç­–ç•¥4ï¼šå°è¯•ä»documentå±æ€§ä¸­ç›´æ¥æå–
            if not page_text and hasattr(raw_result.document, 'text'):
                try:
                    doc_text = getattr(raw_result.document, 'text', None)
                    page_text = doc_text.strip() if doc_text else ""
                except Exception:
                    pass
            
            # å¦‚æœæ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥
            if not page_text:
                page_text = f"é¡µé¢ {page_num} æ–‡æœ¬æå–å¤±è´¥ï¼ˆæ‰€æœ‰ç­–ç•¥å¤±è´¥ï¼‰"
                
        except Exception as e:
            print(f"âš ï¸ é¡µé¢ {page_num} æ–‡æœ¬æå–å¤±è´¥: {e}")
            page_text = f"é¡µé¢ {page_num} æ–‡æœ¬æå–å¤±è´¥: {str(e)}"
        
        # åˆ›å»ºé¡µé¢æ•°æ®
        page_data = PageData(
            page_number=page_num,
            raw_text=page_text,
            images=[],
            tables=[]
        )
        
        # æå–åª’ä½“ï¼ˆè¿›ç¨‹æ± ç‰ˆæœ¬ï¼‰
        try:
            # æå–å›¾ç‰‡
            if hasattr(raw_result.document, 'pictures'):
                for i, picture in enumerate(raw_result.document.pictures):
                    try:
                        # è·å–å›¾ç‰‡
                        picture_image = picture.get_image(raw_result.document)
                        if picture_image is None:
                            continue
                        
                        # ä¿å­˜å›¾ç‰‡
                        image_filename = f"picture-{i+1}.png"
                        image_path = os.path.join(page_output_dir, image_filename)
                        with open(image_path, "wb") as fp:
                            picture_image.save(fp, "PNG")
                        
                        # è·å–å›¾ç‰‡ä¿¡æ¯
                        from PIL import Image
                        image_img = Image.open(image_path)
                        caption = picture.caption_text(raw_result.document) if hasattr(picture, 'caption_text') else ""
                        
                        # åˆ›å»ºImageWithContextå¯¹è±¡ - ä½¿ç”¨æ™ºèƒ½ä¸Šä¸‹æ–‡æå–
                        smart_context = Stage1DoclingProcessor._extract_smart_context(page_text, "image", i+1)
                        image_with_context = ImageWithContext(
                            image_path=image_path,
                            page_number=page_num,  # ğŸŒŸ ç¡®ä¿é¡µç å‡†ç¡®
                            page_context=smart_context,  # ğŸŒŸ ä½¿ç”¨æ™ºèƒ½ä¸Šä¸‹æ–‡è€Œä¸æ˜¯æ•´é¡µæ–‡æœ¬
                            caption=caption or f"å›¾ç‰‡ {i+1}",
                            metadata={
                                'width': image_img.width,
                                'height': image_img.height,
                                'size': image_img.width * image_img.height,
                                'aspect_ratio': image_img.width / image_img.height
                            }
                        )
                        
                        page_data.images.append(image_with_context)
                    except Exception as e:
                        print(f"âš ï¸ é¡µé¢ {page_num} å›¾ç‰‡ {i+1} å¤„ç†å¤±è´¥: {e}")
            
            # æå–è¡¨æ ¼
            if hasattr(raw_result.document, 'tables'):
                for i, table in enumerate(raw_result.document.tables):
                    try:
                        # è·å–è¡¨æ ¼å›¾ç‰‡
                        table_image = table.get_image(raw_result.document)
                        if table_image is None:
                            continue
                        
                        # ä¿å­˜è¡¨æ ¼å›¾ç‰‡
                        table_filename = f"table-{i+1}.png"
                        table_path = os.path.join(page_output_dir, table_filename)
                        with open(table_path, "wb") as fp:
                            table_image.save(fp, "PNG")
                        
                        # è·å–è¡¨æ ¼ä¿¡æ¯
                        from PIL import Image
                        table_img = Image.open(table_path)
                        caption = table.caption_text(raw_result.document) if hasattr(table, 'caption_text') else ""
                        
                        # åˆ›å»ºTableWithContextå¯¹è±¡ - ä½¿ç”¨æ™ºèƒ½ä¸Šä¸‹æ–‡æå–
                        smart_context = Stage1DoclingProcessor._extract_smart_context(page_text, "table", i+1)
                        table_with_context = TableWithContext(
                            table_path=table_path,
                            page_number=page_num,  # ğŸŒŸ ç¡®ä¿é¡µç å‡†ç¡®
                            page_context=smart_context,  # ğŸŒŸ ä½¿ç”¨æ™ºèƒ½ä¸Šä¸‹æ–‡è€Œä¸æ˜¯æ•´é¡µæ–‡æœ¬
                            caption=caption or f"è¡¨æ ¼ {i+1}",
                            metadata={
                                'width': table_img.width,
                                'height': table_img.height,
                                'size': table_img.width * table_img.height,
                                'aspect_ratio': table_img.width / table_img.height
                            }
                        )
                        
                        page_data.tables.append(table_with_context)
                    except Exception as e:
                        print(f"âš ï¸ é¡µé¢ {page_num} è¡¨æ ¼ {i+1} å¤„ç†å¤±è´¥: {e}")
                        
        except Exception as e:
            print(f"âš ï¸ é¡µé¢ {page_num} åª’ä½“æå–å¤±è´¥: {e}")
        
        return page_data
        
    except Exception as e:
        print(f"âŒ é™æ€å‡½æ•°å¤„ç†é¡µé¢ {page_num} å¤±è´¥: {e}")
        # è¿”å›å¤±è´¥çš„é¡µé¢æ•°æ®
        return PageData(
            page_number=page_num,
            raw_text=f"é¡µé¢ {page_num} å¤„ç†å¤±è´¥: {str(e)}",
            images=[],
            tables=[]
        ) 