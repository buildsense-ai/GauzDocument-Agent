#!/usr/bin/env python3
"""
æ–‡æ¡£ç»“æ„åˆ†æå™¨ï¼ˆé‡æ„ç‰ˆï¼‰
åŸºäºDocling HierarchicalChunkerçš„æ— å¹»è§‰æ–‡æ¡£ç»“æ„åˆ†æå’Œåˆ†å—
ä¸“æ³¨äºæ–‡æœ¬å¤„ç†ï¼Œæé€Ÿåˆ†å—
"""

import json
import logging
import os
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path

from .config import PDFProcessingConfig

# å¯¼å…¥Doclingç»„ä»¶
try:
    from docling.document_converter import DocumentConverter
    from docling.chunking import HierarchicalChunker
    from docling.datamodel.base_models import InputFormat
    from docling.datamodel.pipeline_options import PdfPipelineOptions, EasyOcrOptions
    from docling.document_converter import PdfFormatOption
    DOCLING_AVAILABLE = True
    print("âœ… Doclingç»„ä»¶å¯ç”¨ï¼ˆDocumentStructureAnalyzerï¼‰")
except ImportError as e:
    DOCLING_AVAILABLE = False
    print(f"âŒ Doclingç»„ä»¶ä¸å¯ç”¨ï¼ˆDocumentStructureAnalyzerï¼‰: {e}")

logger = logging.getLogger(__name__)

@dataclass
class ChapterInfo:
    """ç« èŠ‚ä¿¡æ¯"""
    chapter_id: str  # å¦‚ "1", "2.1", "3.2.1"
    level: int  # å±‚çº§ï¼š1, 2, 3...
    title: str  # ç« èŠ‚æ ‡é¢˜
    content_summary: str  # ç« èŠ‚å†…å®¹æ¦‚è¦
    
@dataclass
class DocumentStructure:
    """æ–‡æ¡£ç»“æ„æ•°æ®ç±»"""
    toc: List[ChapterInfo]  # ç›®å½•ç»“æ„
    total_pages: int  # æ€»é¡µæ•°ï¼ˆæ¥æºäºåŸºç¡€å¤„ç†ï¼‰
    
@dataclass
class MinimalChunk:
    """æœ€å°é¢—ç²’åº¦åˆ†å—ä¿¡æ¯"""
    chunk_id: int
    content: str  # å®é™…å†…å®¹ï¼ˆæ®µè½ã€åˆ—è¡¨é¡¹ç­‰ï¼‰
    chunk_type: str  # ç±»å‹ï¼šparagraph, list_item, headingç­‰
    belongs_to_chapter: str  # æ‰€å±ç« èŠ‚IDï¼Œå¦‚ "1", "2.1", "3.2.1"
    chapter_title: str  # æ‰€å±ç« èŠ‚æ ‡é¢˜
    chapter_level: int  # æ‰€å±ç« èŠ‚å±‚çº§

class DocumentStructureAnalyzer:
    """
    æ–‡æ¡£ç»“æ„åˆ†æå™¨ï¼ˆé‡æ„ç‰ˆï¼‰
    
    åŸºäºDocling HierarchicalChunkerçš„æ— å¹»è§‰æ–‡æ¡£ç»“æ„åˆ†æå’Œåˆ†å—
    ä¸“æ³¨äºæ–‡æœ¬å¤„ç†ï¼Œæé€Ÿåˆ†å—ï¼ˆ~0.02ç§’ï¼‰
    """
    
    def __init__(self, config: PDFProcessingConfig = None):
        """
        åˆå§‹åŒ–æ–‡æ¡£ç»“æ„åˆ†æå™¨
        
        Args:
            config: PDFå¤„ç†é…ç½®
        """
        self.config = config or PDFProcessingConfig()
        
        # åˆå§‹åŒ–Doclingç»„ä»¶
        self.doc_converter = self._init_docling_converter()
        self.chunker = self._init_chunker()
        
        logger.info("ğŸ“Š æ–‡æ¡£ç»“æ„åˆ†æå™¨å·²åˆå§‹åŒ–")
    
    def _init_docling_converter(self):
        """åˆå§‹åŒ–Doclingè½¬æ¢å™¨"""
        if not DOCLING_AVAILABLE:
            logger.warning("Doclingä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨é™çº§å¤„ç†")
            return None
        
        try:
            # é…ç½®PDFå¤„ç†é€‰é¡¹
            pdf_options = PdfPipelineOptions(
                do_ocr=False,  # ç¦ç”¨OCRä»¥æé«˜é€Ÿåº¦
                do_table_structure=False,  # ç¦ç”¨è¡¨æ ¼ç»“æ„è¯†åˆ«
                ocr_options=EasyOcrOptions(lang=["en"])
            )
            
            # åˆ›å»ºæ–‡æ¡£è½¬æ¢å™¨
            converter = DocumentConverter(
                format_options={
                    InputFormat.PDF: PdfFormatOption(pipeline_options=pdf_options)
                }
            )
            
            logger.info("âœ… Doclingè½¬æ¢å™¨åˆå§‹åŒ–å®Œæˆ")
            return converter
            
        except Exception as e:
            logger.error(f"Doclingè½¬æ¢å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return None
    
    def _init_chunker(self):
        """åˆå§‹åŒ–åˆ†å—å™¨"""
        if not DOCLING_AVAILABLE:
            return None
            
        try:
            # ä½¿ç”¨HierarchicalChunkerè¿›è¡Œå±‚æ¬¡åŒ–åˆ†å—
            chunker = HierarchicalChunker(
                tokenizer=None,  # ä½¿ç”¨é»˜è®¤åˆ†è¯å™¨
                max_tokens=512,  # æœ€å¤§tokenæ•°
                overlap_token_count=50,  # é‡å tokenæ•°
                include_metadata=True  # åŒ…å«å…ƒæ•°æ®
            )
            
            logger.info("âœ… Doclingåˆ†å—å™¨åˆå§‹åŒ–å®Œæˆ")
            return chunker
            
        except Exception as e:
            logger.error(f"åˆ†å—å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return None
    
    def analyze_and_chunk(self, pdf_path: str, output_dir: str) -> Tuple[DocumentStructure, List[MinimalChunk]]:
        """
        åŸºäºDoclingçš„æ–‡æ¡£ç»“æ„åˆ†æå’Œåˆ†å—
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            Tuple[DocumentStructure, List[MinimalChunk]]: æ–‡æ¡£ç»“æ„å’Œæœ€å°åˆ†å—
        """
        logger.info(f"ğŸ“Š å¼€å§‹Doclingæ–‡æ¡£ç»“æ„åˆ†æ: {pdf_path}")
        
        if not self.doc_converter or not self.chunker:
            logger.warning("Doclingç»„ä»¶ä¸å¯ç”¨ï¼Œä½¿ç”¨é™çº§å¤„ç†")
            return self._fallback_analysis(pdf_path, output_dir)
        
        try:
            # 1. è§£æPDFæ–‡æ¡£
            logger.info("ğŸ”„ è§£æPDFæ–‡æ¡£...")
            raw_result = self.doc_converter.convert(Path(pdf_path))
            document = raw_result.document
            
            # 2. ä½¿ç”¨HierarchicalChunkerè¿›è¡Œåˆ†å—
            logger.info("ğŸ”ª ä½¿ç”¨Doclingåˆ†å—å™¨è¿›è¡Œç»“æ„åŒ–åˆ†å—...")
            chunks = list(self.chunker.chunk(document))
            logger.info(f"âœ… å®Œæˆåˆ†å—: {len(chunks)} ä¸ªåˆ†å—")
            
            # 3. æå–æ–‡æ¡£ç»“æ„ï¼ˆTOCï¼‰
            document_structure = self._extract_document_structure(chunks, pdf_path)
            
            # 4. è½¬æ¢ä¸ºMinimalChunkæ ¼å¼
            minimal_chunks = self._convert_to_minimal_chunks(chunks, document_structure)
            
            # 5. ä¿å­˜ç»“æœ
            self._save_results(document_structure, minimal_chunks, output_dir)
            
            logger.info(f"âœ… æ–‡æ¡£ç»“æ„åˆ†æå®Œæˆ: {len(minimal_chunks)} ä¸ªåˆ†å—")
            
            return document_structure, minimal_chunks
            
        except Exception as e:
            logger.error(f"Doclingåˆ†æå¤±è´¥: {e}")
            return self._fallback_analysis(pdf_path, output_dir)
    
    def _extract_document_structure(self, chunks: List, pdf_path: str) -> DocumentStructure:
        """ä»Doclingåˆ†å—ä¸­æå–æ–‡æ¡£ç»“æ„ï¼ˆTOCï¼‰"""
        try:
            toc = []
            
            # ä»åˆ†å—ä¸­æå–æ ‡é¢˜å±‚æ¬¡
            for chunk in chunks:
                if hasattr(chunk, 'meta') and hasattr(chunk.meta, 'headings'):
                    headings = chunk.meta.headings
                    
                    for heading in headings:
                        # headingæ˜¯å­—ç¬¦ä¸²ï¼Œä¸æ˜¯å¯¹è±¡
                        if isinstance(heading, str) and heading.strip():
                            # ç®€å•çš„å±‚çº§åˆ¤æ–­
                            level = 1
                            title = heading.strip()
                            
                            # å°è¯•ä»æ ‡é¢˜ä¸­æå–å±‚çº§
                            import re
                            # åŒ¹é… "1. Introduction" æˆ– "2.1. Task specification" æ ¼å¼
                            number_match = re.match(r'^(\d+(?:\.\d+)*)\.\s*(.+)$', title)
                            if number_match:
                                number_part = number_match.group(1)
                                title_part = number_match.group(2)
                                level = len(number_part.split('.'))  # æ ¹æ®ç‚¹çš„æ•°é‡ç¡®å®šå±‚çº§
                                title = title_part
                            
                            # ç”Ÿæˆç« èŠ‚ID
                            chapter_id = f"section_{len(toc) + 1}"
                            
                            # åˆ›å»ºç« èŠ‚ä¿¡æ¯
                            chapter_info = ChapterInfo(
                                chapter_id=chapter_id,
                                level=level,
                                title=title,
                                content_summary=title[:100] + "..." if len(title) > 100 else title
                            )
                            
                            toc.append(chapter_info)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡é¢˜ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤ç»“æ„
            if not toc:
                toc = [ChapterInfo(
                    chapter_id="section_1",
                    level=1,
                    title="æ–‡æ¡£å†…å®¹",
                    content_summary="æ–‡æ¡£çš„ä¸»è¦å†…å®¹"
                )]
            
            # è·å–æ€»é¡µæ•°ï¼ˆä»æ–‡æ¡£ä¿¡æ¯ä¸­è·å–ï¼‰
            total_pages = getattr(chunks[0].meta, 'page_count', 1) if chunks else 1
            
            return DocumentStructure(
                toc=toc,
                total_pages=total_pages
            )
            
        except Exception as e:
            logger.error(f"æå–æ–‡æ¡£ç»“æ„å¤±è´¥: {e}")
            return DocumentStructure(
                toc=[ChapterInfo(
                    chapter_id="section_1",
                    level=1,
                    title="æ–‡æ¡£å†…å®¹",
                    content_summary="æ–‡æ¡£çš„ä¸»è¦å†…å®¹"
                )],
                total_pages=1
            )
    
    def _convert_to_minimal_chunks(self, chunks: List, document_structure: DocumentStructure) -> List[MinimalChunk]:
        """å°†Doclingåˆ†å—è½¬æ¢ä¸ºMinimalChunkæ ¼å¼"""
        minimal_chunks = []
        
        for i, chunk in enumerate(chunks):
            try:
                # æå–å†…å®¹
                content = chunk.text if hasattr(chunk, 'text') else str(chunk)
                
                # ç¡®å®šåˆ†å—ç±»å‹
                chunk_type = "paragraph"  # é»˜è®¤ç±»å‹
                if hasattr(chunk, 'meta') and hasattr(chunk.meta, 'doc_items'):
                    # æ ¹æ®æ–‡æ¡£é¡¹ç›®ç±»å‹ç¡®å®šåˆ†å—ç±»å‹
                    doc_items = chunk.meta.doc_items
                    if doc_items:
                        first_item = doc_items[0]
                        if hasattr(first_item, 'label'):
                            chunk_type = first_item.label.lower()
                
                # ç¡®å®šæ‰€å±ç« èŠ‚
                belongs_to_chapter = "section_1"  # é»˜è®¤ç« èŠ‚
                chapter_title = "æ–‡æ¡£å†…å®¹"  # é»˜è®¤æ ‡é¢˜
                chapter_level = 1  # é»˜è®¤å±‚çº§
                
                # å°è¯•ä»æ–‡æ¡£ç»“æ„ä¸­æ‰¾åˆ°åŒ¹é…çš„ç« èŠ‚
                if document_structure.toc:
                    # ç®€å•çš„ç« èŠ‚å½’å±ï¼šåŸºäºä½ç½®æ¯”ä¾‹
                    chapter_index = min(i // max(1, len(chunks) // len(document_structure.toc)), len(document_structure.toc) - 1)
                    chapter = document_structure.toc[chapter_index]
                    belongs_to_chapter = chapter.chapter_id
                    chapter_title = chapter.title
                    chapter_level = chapter.level
                
                # åˆ›å»ºMinimalChunk
                minimal_chunk = MinimalChunk(
                    chunk_id=i + 1,
                    content=content,
                    chunk_type=chunk_type,
                    belongs_to_chapter=belongs_to_chapter,
                    chapter_title=chapter_title,
                    chapter_level=chapter_level
                )
                
                minimal_chunks.append(minimal_chunk)
                
            except Exception as e:
                logger.error(f"è½¬æ¢åˆ†å— {i} å¤±è´¥: {e}")
                continue
        
        return minimal_chunks
    
    def _save_results(self, document_structure: DocumentStructure, minimal_chunks: List[MinimalChunk], output_dir: str):
        """ä¿å­˜åˆ†æç»“æœ"""
        try:
            os.makedirs(output_dir, exist_ok=True)
            
            # ä¿å­˜æ–‡æ¡£ç»“æ„
            structure_file = os.path.join(output_dir, "document_structure.json")
            with open(structure_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "total_pages": document_structure.total_pages,
                    "toc": [
                        {
                            "chapter_id": chapter.chapter_id,
                            "level": chapter.level,
                            "title": chapter.title,
                            "content_summary": chapter.content_summary
                        }
                        for chapter in document_structure.toc
                    ]
                }, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜åˆ†å—ç»“æœ
            chunks_file = os.path.join(output_dir, "chunks.json")
            with open(chunks_file, 'w', encoding='utf-8') as f:
                json.dump([
                    {
                        "chunk_id": chunk.chunk_id,
                        "content": chunk.content,
                        "chunk_type": chunk.chunk_type,
                        "belongs_to_chapter": chunk.belongs_to_chapter,
                        "chapter_title": chunk.chapter_title,
                        "chapter_level": chunk.chapter_level
                    }
                    for chunk in minimal_chunks
                ], f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ“„ ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    def _fallback_analysis(self, pdf_path: str, output_dir: str) -> Tuple[DocumentStructure, List[MinimalChunk]]:
        """é™çº§åˆ†æï¼šå½“Doclingä¸å¯ç”¨æ—¶çš„ç®€å•å¤„ç†"""
        logger.info("ğŸ”„ ä½¿ç”¨é™çº§æ–‡æ¡£ç»“æ„åˆ†æ")
        
        try:
            # åˆ›å»ºåŸºæœ¬çš„æ–‡æ¡£ç»“æ„
            document_structure = DocumentStructure(
                toc=[ChapterInfo(
                    chapter_id="section_1",
                    level=1,
                    title="æ–‡æ¡£å†…å®¹",
                    content_summary="æ–‡æ¡£çš„ä¸»è¦å†…å®¹"
                )],
                total_pages=1
            )
            
            # åˆ›å»ºåŸºæœ¬çš„åˆ†å—
            minimal_chunks = [MinimalChunk(
                chunk_id=1,
                content="é™çº§å¤„ç†ï¼šæ— æ³•è¿›è¡Œè¯¦ç»†çš„æ–‡æ¡£ç»“æ„åˆ†æ",
                chunk_type="paragraph",
                belongs_to_chapter="section_1",
                chapter_title="æ–‡æ¡£å†…å®¹",
                chapter_level=1
            )]
            
            # ä¿å­˜ç»“æœ
            self._save_results(document_structure, minimal_chunks, output_dir)
            
            return document_structure, minimal_chunks
            
        except Exception as e:
            logger.error(f"é™çº§åˆ†æå¤±è´¥: {e}")
            return DocumentStructure(toc=[], total_pages=0), [] 