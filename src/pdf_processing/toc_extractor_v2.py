#!/usr/bin/env python3
"""
TOCæå–å™¨ V2 - æ”¹è¿›ç‰ˆæœ¬
è§£å†³é¡µç å™ªéŸ³ã€ç« èŠ‚IDå’Œparent_idé€»è¾‘é—®é¢˜
ä½¿ç”¨qwen_plus thinkingæ¨¡å¼æå–æ–‡æ¡£ç›®å½•ç»“æ„
"""

import json
import os
import sys
import time
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from openai import OpenAI
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

load_dotenv()

@dataclass
class TOCItem:
    """TOCæ¡ç›®"""
    title: str
    level: int
    start_text: str  # ç”¨äºåŒ¹é…çš„å¼€å¤´æ–‡æœ¬
    id: str
    parent_id: Optional[str] = None

class TOCExtractorV2:
    """TOCæå–å™¨ V2"""
    
    def __init__(self, model: str = "qwen-plus"):
        """
        åˆå§‹åŒ–TOCæå–å™¨
        
        Args:
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
        """
        self.client = OpenAI(
            api_key=os.getenv("QWEN_API_KEY"),
            base_url=os.getenv("QWEN_BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1")
        )
        self.model = model
        
        if not self.client.api_key:
            raise ValueError("æœªæ‰¾åˆ°APIå¯†é’¥ï¼Œè¯·è®¾ç½®QWEN_API_KEYç¯å¢ƒå˜é‡")
    
    def stitch_full_text(self, basic_result_path: str) -> str:
        """
        ç¼åˆå®Œæ•´æ–‡æœ¬ï¼ˆä¸åŒ…å«é¡µç æ ‡è®°ï¼‰
        
        Args:
            basic_result_path: åŸºç¡€å¤„ç†ç»“æœæ–‡ä»¶è·¯å¾„
            
        Returns:
            str: ç¼åˆåçš„å®Œæ•´æ–‡æœ¬
        """
        print("ğŸ§µ å¼€å§‹ç¼åˆfull_text...")
        
        # è¯»å–åŸºç¡€å¤„ç†ç»“æœ
        with open(basic_result_path, 'r', encoding='utf-8') as f:
            basic_result = json.load(f)
        
        # æå–é¡µé¢æ•°æ®
        pages = basic_result.get('pages', [])
        if not pages:
            raise ValueError("æœªæ‰¾åˆ°é¡µé¢æ•°æ®")
        
        # æŒ‰é¡µç æ’åº
        pages.sort(key=lambda x: x.get('page_number', 0))
        
        # ç¼åˆæ–‡æœ¬ï¼ˆä¸åŒ…å«é¡µç æ ‡è®°ï¼‰
        full_text_parts = []
        
        for page in pages:
            page_num = page.get('page_number', 0)
            cleaned_text = page.get('cleaned_text', '')
            images = page.get('images', [])
            tables = page.get('tables', [])
            
            # æ·»åŠ é¡µé¢æ–‡æœ¬ï¼ˆä¸æ·»åŠ é¡µç æ ‡è®°ï¼‰
            if cleaned_text.strip():
                full_text_parts.append(cleaned_text.strip())
            
            # æ·»åŠ å›¾ç‰‡ä¿¡æ¯
            for i, image in enumerate(images, 1):
                description = image.get('ai_description', 'å›¾ç‰‡æè¿°')
                full_text_parts.append(f"[å›¾ç‰‡: {description}]")
            
            # æ·»åŠ è¡¨æ ¼ä¿¡æ¯
            for i, table in enumerate(tables, 1):
                description = table.get('ai_description', 'è¡¨æ ¼æè¿°')
                full_text_parts.append(f"[è¡¨æ ¼: {description}]")
        
        # ç”¨åŒæ¢è¡Œè¿æ¥ï¼Œä¿æŒæ®µè½åˆ†éš”
        full_text = "\n\n".join(full_text_parts)
        
        print(f"âœ… æ–‡æœ¬ç¼åˆå®Œæˆï¼Œæ€»é•¿åº¦: {len(full_text)} å­—ç¬¦")
        print(f"ğŸ“„ æ€»é¡µæ•°: {len(pages)}")
        
        return full_text
    
    def extract_toc_with_qwen_plus(self, full_text: str) -> Tuple[List[TOCItem], str]:
        """
        ä½¿ç”¨qwen_plusæ¨ç†æ¨¡å¼æå–TOC
        
        Args:
            full_text: å®Œæ•´æ–‡æ¡£æ–‡æœ¬
            
        Returns:
            Tuple[List[TOCItem], str]: TOCé¡¹ç›®åˆ—è¡¨å’Œthinkingå†…å®¹
        """
        print("ğŸ§  å¼€å§‹ä½¿ç”¨qwen_plusæ¨ç†æ¨¡å¼æå–TOC...")
        
        # æ„å»ºç³»ç»Ÿæç¤º
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£ç»“æ„åˆ†æå¸ˆã€‚è¯·åˆ†æç»™å®šçš„æ–‡æ¡£æ–‡æœ¬ï¼Œæå–å®Œæ•´çš„ç›®å½•ç»“æ„(TOC)ã€‚

è¦æ±‚ï¼š
1. è¯†åˆ«æ‰€æœ‰å±‚çº§çš„ç« èŠ‚æ ‡é¢˜ï¼ˆä¸€çº§ã€äºŒçº§ã€ä¸‰çº§ç­‰ï¼‰
2. ä¸ºæ¯ä¸ªç« èŠ‚æä¾›ç”¨äºåŒ¹é…çš„å¼€å¤´æ–‡æœ¬ç‰‡æ®µï¼ˆ20-40ä¸ªå­—ç¬¦ï¼‰
3. åˆ†æç« èŠ‚çš„å±‚çº§å…³ç³»ï¼Œæ­£ç¡®è®¾ç½®parent_id
4. ä½¿ç”¨æ•°å­—IDæ ¼å¼ï¼ˆ1, 2, 3ç­‰ï¼‰

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{
  "toc": [
    {
      "title": "ç« èŠ‚æ ‡é¢˜",
      "level": 1,
      "start_text": "ç« èŠ‚å¼€å¤´çš„æ–‡æœ¬ç‰‡æ®µï¼Œç”¨äºç²¾ç¡®åŒ¹é…",
      "id": "1",
      "parent_id": null
    },
    {
      "title": "å­ç« èŠ‚æ ‡é¢˜",
      "level": 2,
      "start_text": "å­ç« èŠ‚å¼€å¤´çš„æ–‡æœ¬ç‰‡æ®µ",
      "id": "2",
      "parent_id": "1"
    }
  ]
}

æ³¨æ„ï¼š
- levelä»1å¼€å§‹ï¼ˆ1=ä¸€çº§æ ‡é¢˜ï¼Œ2=äºŒçº§æ ‡é¢˜ï¼Œä»¥æ­¤ç±»æ¨ï¼‰
- start_textåº”è¯¥æ˜¯ç« èŠ‚æ ‡é¢˜åç´§æ¥ç€çš„æ–‡æœ¬ï¼Œç”¨äºç²¾ç¡®åŒ¹é…
- idä½¿ç”¨ç®€å•çš„æ•°å­—æ ¼å¼ï¼š1, 2, 3, 4ç­‰
- parent_idæŒ‡å‘ä¸Šçº§ç« èŠ‚çš„idï¼Œä¸€çº§ç« èŠ‚çš„parent_idä¸ºnull
- ç¡®ä¿å±‚çº§å…³ç³»æ­£ç¡®ï¼ŒäºŒçº§ç« èŠ‚çš„parent_idåº”è¯¥æŒ‡å‘å…¶æ‰€å±çš„ä¸€çº§ç« èŠ‚
"""
        
        # ä¸é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼Œè®©AIå®Œæ•´åˆ†æ
        # limited_text = full_text[:50000] if len(full_text) > 50000 else full_text
        
        # æ„å»ºç”¨æˆ·æç¤º
        user_prompt = f"""è¯·ä»”ç»†åˆ†æä»¥ä¸‹æ–‡æ¡£æ–‡æœ¬ï¼Œæå–å®Œæ•´çš„ç›®å½•ç»“æ„ã€‚

è¯·ä½¿ç”¨æ¨ç†æ¨¡å¼åˆ†ææ–‡æ¡£ç»“æ„ï¼Œè¯†åˆ«æ‰€æœ‰ç« èŠ‚æ ‡é¢˜å’Œå±‚çº§å…³ç³»ï¼š

{full_text}

è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ç»“æœï¼Œç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æ­£ç¡®å¡«å†™ã€‚
"""
        
        try:
            # ä½¿ç”¨æµå¼è¾“å‡ºå’Œthinkingæ¨¡å¼
            print("ğŸ”„ æ­£åœ¨è°ƒç”¨qwen-plusæ¨ç†æ¨¡å¼...")
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1,
                stream=True,  # å¯ç”¨æµå¼è¾“å‡º
                extra_body={
                    "enable_thinking": True  # å¯ç”¨æ¨ç†æ¨¡å¼
                }
            )
            
            # æ”¶é›†å“åº”å†…å®¹å’Œæ¨ç†å†…å®¹
            content_chunks = []
            reasoning_chunks = []
            
            print("ğŸ§  æ­£åœ¨æ¥æ”¶æ¨ç†å†…å®¹...")
            
            for chunk in response:
                if chunk.choices:
                    delta = chunk.choices[0].delta
                    
                    # æ”¶é›†ä¸»è¦å†…å®¹
                    if hasattr(delta, 'content') and delta.content:
                        content_chunks.append(delta.content)
                    
                    # æ”¶é›†æ¨ç†å†…å®¹
                    if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                        reasoning_chunks.append(delta.reasoning_content)
                        print(delta.reasoning_content, end='', flush=True)
            
            # åˆå¹¶å†…å®¹
            content = ''.join(content_chunks)
            reasoning_content = ''.join(reasoning_chunks)
            
            print(f"\nâœ… æ¨ç†å†…å®¹æ”¶é›†å®Œæˆï¼Œå…± {len(reasoning_content)} å­—ç¬¦")
            print(f"ğŸ“ ä¸»è¦å“åº”å†…å®¹ï¼Œå…± {len(content)} å­—ç¬¦")
            
            # å¤„ç†```jsonæ ¼å¼çš„å“åº”
            if content.startswith('```json'):
                # æå–JSONéƒ¨åˆ†
                json_start = content.find('{')
                json_end = content.rfind('}') + 1
                if json_start != -1 and json_end != -1:
                    content = content[json_start:json_end]
            
            # å°è¯•è§£æJSON
            try:
                result = json.loads(content)
                toc_data = result.get('toc', [])
                
                # è½¬æ¢ä¸ºTOCItemå¯¹è±¡
                toc_items = []
                for item in toc_data:
                    toc_item = TOCItem(
                        title=item.get('title', ''),
                        level=item.get('level', 1),
                        start_text=item.get('start_text', ''),
                        id=str(item.get('id', '')),
                        parent_id=item.get('parent_id')
                    )
                    toc_items.append(toc_item)
                
                print(f"âœ… TOCæå–æˆåŠŸï¼Œå…±è¯†åˆ« {len(toc_items)} ä¸ªç« èŠ‚")
                
                # æ˜¾ç¤ºæå–ç»“æœ
                for item in toc_items:
                    indent = "  " * (item.level - 1)
                    parent_info = f" (çˆ¶çº§: {item.parent_id})" if item.parent_id else ""
                    print(f"{indent}{item.level}. {item.title}{parent_info}")
                
                return toc_items, reasoning_content
                
            except json.JSONDecodeError as e:
                print(f"âŒ JSONè§£æå¤±è´¥: {e}")
                print(f"ğŸ” åŸå§‹å“åº”: {content}")
                return [], reasoning_content
                
        except Exception as e:
            print(f"âŒ APIè°ƒç”¨å¤±è´¥: {e}")
            return [], ""
    
    def save_toc_result(self, toc_items: List[TOCItem], reasoning_content: str, output_path: str):
        """
        ä¿å­˜TOCç»“æœ
        
        Args:
            toc_items: TOCé¡¹ç›®åˆ—è¡¨
            reasoning_content: æ¨ç†å†…å®¹
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        toc_dict = {
            "toc": [
                {
                    "title": item.title,
                    "level": item.level,
                    "start_text": item.start_text,
                    "id": item.id,
                    "parent_id": item.parent_id
                }
                for item in toc_items
            ],
            "extraction_time": time.time(),
            "total_chapters": len(toc_items),
            "reasoning_content": reasoning_content  # ä¿å­˜æ¨ç†å†…å®¹
        }
        
        # ä¿å­˜ç»“æœ
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(toc_dict, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“ TOCç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    
    def save_full_text_debug(self, full_text: str, output_path: str):
        """
        ä¿å­˜å®Œæ•´æ–‡æœ¬ç”¨äºè°ƒè¯•
        
        Args:
            full_text: å®Œæ•´æ–‡æœ¬
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(full_text)
        
        print(f"ğŸ“ å®Œæ•´æ–‡æœ¬å·²ä¿å­˜åˆ°: {output_path}")

def main():
    """ä¸»å‡½æ•°"""
    # è¾“å…¥æ–‡ä»¶è·¯å¾„
    input_file = "parser_output/20250714_145102_zpdlfg/basic_processing_result.json"
    
    if not os.path.exists(input_file):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return
    
    # åˆ›å»ºæå–å™¨
    extractor = TOCExtractorV2()
    
    # ç¼åˆå®Œæ•´æ–‡æœ¬
    full_text = extractor.stitch_full_text(input_file)
    
    # ä¿å­˜å®Œæ•´æ–‡æœ¬ç”¨äºè°ƒè¯•
    output_dir = os.path.dirname(input_file)
    full_text_path = os.path.join(output_dir, "full_text_v2.txt")
    extractor.save_full_text_debug(full_text, full_text_path)
    
    # æå–TOC
    toc_items, reasoning_content = extractor.extract_toc_with_qwen_plus(full_text)
    
    # è¾“å‡ºæ¨ç†å†…å®¹
    if reasoning_content:
        print("\n" + "="*50)
        print("ğŸ§  QWEN_PLUS æ¨ç†å†…å®¹:")
        print("="*50)
        print(reasoning_content)
        print("="*50)
    
    # ä¿å­˜ç»“æœ
    if toc_items:
        result_path = os.path.join(output_dir, "toc_extraction_result_v2.json")
        extractor.save_toc_result(toc_items, reasoning_content, result_path)
        print(f"\nğŸ‰ TOCæå–å®Œæˆ! ç»“æœä¿å­˜åœ¨: {result_path}")
    else:
        print("âŒ TOCæå–å¤±è´¥")

if __name__ == "__main__":
    main() 