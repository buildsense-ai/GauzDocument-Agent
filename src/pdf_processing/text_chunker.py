#!/usr/bin/env python3
"""
æ–‡æœ¬åˆ†å—å™¨ - åŸºäºTOCçš„æ™ºèƒ½æ–‡æœ¬åˆ†å—
æ”¯æŒç¬¬ä¸€çº§ç« èŠ‚åˆ‡å‰²å’Œæœ€å°å—åˆ†å‰²
å®ç°"å°å—æ£€ç´¢ï¼Œå¤§å—å–‚å…»"æ¨¡å¼
ä½¿ç”¨AIæ™ºèƒ½åˆ†å—å™¨æ›¿ä»£æ­£åˆ™è¡¨è¾¾å¼æ®µè½åˆ‡å‰²
"""

import json
import re
import os
import asyncio
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path

# å¯¼å…¥AIåˆ†å—å™¨
from .ai_chunker import AIChunker, MinimalChunk as AIMinimalChunk

@dataclass
class ChapterContent:
    """ç¬¬ä¸€çº§ç« èŠ‚å†…å®¹"""
    chapter_id: str
    title: str
    content: str
    start_pos: int
    end_pos: int
    word_count: int
    has_images: bool
    has_tables: bool
    
@dataclass
class MinimalChunk:
    """æœ€å°é¢—ç²’åº¦åˆ†å—"""
    chunk_id: str
    content: str
    chunk_type: str  # title, paragraph, list_item, image_desc, table_desc
    belongs_to_chapter: str
    chapter_title: str
    start_pos: int
    end_pos: int
    word_count: int
    
@dataclass
class ChunkingResult:
    """åˆ†å—ç»“æœ"""
    first_level_chapters: List[ChapterContent]
    minimal_chunks: List[MinimalChunk]
    total_chapters: int
    total_chunks: int
    processing_metadata: Dict[str, Any]
    
    def to_dict(self) -> Dict[str, Any]:
        """
        è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼Œä¾¿äºJSONåºåˆ—åŒ–
        
        Returns:
            Dict[str, Any]: åŒ…å«æ‰€æœ‰åˆ†å—ä¿¡æ¯çš„å­—å…¸
        """
        return {
            "first_level_chapters": [
                {
                    "chapter_id": chapter.chapter_id,
                    "title": chapter.title,
                    "content": chapter.content,
                    "start_pos": chapter.start_pos,
                    "end_pos": chapter.end_pos,
                    "word_count": chapter.word_count,
                    "has_images": chapter.has_images,
                    "has_tables": chapter.has_tables
                }
                for chapter in self.first_level_chapters
            ],
            "minimal_chunks": [
                {
                    "chunk_id": chunk.chunk_id,
                    "content": chunk.content,
                    "chunk_type": chunk.chunk_type,
                    "belongs_to_chapter": chunk.belongs_to_chapter,
                    "chapter_title": chunk.chapter_title,
                    "start_pos": chunk.start_pos,
                    "end_pos": chunk.end_pos,
                    "word_count": chunk.word_count
                }
                for chunk in self.minimal_chunks
            ],
            "total_chapters": self.total_chapters,
            "total_chunks": self.total_chunks,
            "processing_metadata": self.processing_metadata
        }


class TextChunker:
    """æ–‡æœ¬åˆ†å—å™¨"""
    
    def __init__(self, use_ai_chunker: bool = True, ai_model: str = "qwen-turbo"):
        """
        åˆå§‹åŒ–æ–‡æœ¬åˆ†å—å™¨
        
        Args:
            use_ai_chunker: æ˜¯å¦ä½¿ç”¨AIåˆ†å—å™¨
            ai_model: AIåˆ†å—å™¨ä½¿ç”¨çš„æ¨¡å‹
        """
        self.use_ai_chunker = use_ai_chunker
        self.ai_chunker = AIChunker(model=ai_model) if use_ai_chunker else None
        
        self.processing_metadata = {
            "chunking_method": "toc_based_with_ai" if use_ai_chunker else "toc_based",
            "support_retrieval": True,
            "chunk_strategy": "minimal_chunks_with_full_chapters",
            "ai_model": ai_model if use_ai_chunker else None
        }
    
    def chunk_text_with_toc(self, full_text: str, toc_result: Dict[str, Any]) -> ChunkingResult:
        """
        åŸºäºTOCç»“æœè¿›è¡Œæ–‡æœ¬åˆ†å—
        
        Args:
            full_text: å®Œæ•´æ–‡æœ¬
            toc_result: TOCæå–ç»“æœ
            
        Returns:
            ChunkingResult: åˆ†å—ç»“æœ
        """
        print("ğŸ”ª å¼€å§‹åŸºäºTOCçš„æ–‡æœ¬åˆ†å—...")
        
        # è·å–ç¬¬ä¸€çº§ç« èŠ‚
        first_level_toc = [item for item in toc_result['toc'] if item['level'] == 1]
        print(f"ğŸ“– æ‰¾åˆ° {len(first_level_toc)} ä¸ªç¬¬ä¸€çº§ç« èŠ‚")
        
        # 1. åˆ‡å‰²ç¬¬ä¸€çº§ç« èŠ‚
        first_level_chapters = self._cut_first_level_chapters(full_text, first_level_toc)
        print(f"âœ… ç¬¬ä¸€çº§ç« èŠ‚åˆ‡å‰²å®Œæˆ: {len(first_level_chapters)} ä¸ªç« èŠ‚")
        
        # 2. ç”Ÿæˆæœ€å°å—
        minimal_chunks = self._generate_minimal_chunks(first_level_chapters)
        print(f"âœ… æœ€å°å—ç”Ÿæˆå®Œæˆ: {len(minimal_chunks)} ä¸ªå—")
        
        # 3. æ„å»ºç»“æœ
        result = ChunkingResult(
            first_level_chapters=first_level_chapters,
            minimal_chunks=minimal_chunks,
            total_chapters=len(first_level_chapters),
            total_chunks=len(minimal_chunks),
            processing_metadata=self.processing_metadata
        )
        
        return result
    
    def _cut_first_level_chapters(self, full_text: str, first_level_toc: List[Dict]) -> List[ChapterContent]:
        """
        åˆ‡å‰²ç¬¬ä¸€çº§ç« èŠ‚
        
        Args:
            full_text: å®Œæ•´æ–‡æœ¬
            first_level_toc: ç¬¬ä¸€çº§TOCé¡¹ç›®
            
        Returns:
            List[ChapterContent]: ç« èŠ‚å†…å®¹åˆ—è¡¨
        """
        chapters = []
        
        for i, toc_item in enumerate(first_level_toc):
            chapter_id = toc_item['id']
            title = toc_item['title']
            start_text = toc_item['start_text']
            
            # ä½¿ç”¨start_textæŸ¥æ‰¾ç« èŠ‚å¼€å§‹ä½ç½®
            start_pos = self._find_chapter_start(full_text, start_text)
            
            if start_pos == -1:
                print(f"âš ï¸ æ— æ³•æ‰¾åˆ°ç« èŠ‚ '{title}' çš„å¼€å§‹ä½ç½®")
                continue
                
            # ç¡®å®šç« èŠ‚ç»“æŸä½ç½®
            if i + 1 < len(first_level_toc):
                next_start_text = first_level_toc[i + 1]['start_text']
                end_pos = self._find_chapter_start(full_text, next_start_text)
                if end_pos == -1:
                    end_pos = len(full_text)
            else:
                end_pos = len(full_text)
            
            # æå–ç« èŠ‚å†…å®¹
            content = full_text[start_pos:end_pos].strip()
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡å’Œè¡¨æ ¼
            has_images = "[å›¾ç‰‡:" in content
            has_tables = "[è¡¨æ ¼:" in content or "è¡¨" in content
            
            chapter = ChapterContent(
                chapter_id=chapter_id,
                title=title,
                content=content,
                start_pos=start_pos,
                end_pos=end_pos,
                word_count=len(content),
                has_images=has_images,
                has_tables=has_tables
            )
            
            chapters.append(chapter)
            
        return chapters
    
    def _find_chapter_start(self, full_text: str, start_text: str) -> int:
        """
        æŸ¥æ‰¾ç« èŠ‚å¼€å§‹ä½ç½®
        
        Args:
            full_text: å®Œæ•´æ–‡æœ¬
            start_text: å¼€å§‹æ–‡æœ¬ç‰‡æ®µ
            
        Returns:
            int: ä½ç½®ç´¢å¼•ï¼Œ-1è¡¨ç¤ºæœªæ‰¾åˆ°
        """
        # é¢„å¤„ç†start_textï¼Œå»é™¤æ¢è¡Œå’Œå¤šä½™ç©ºæ ¼
        cleaned_start_text = re.sub(r'\s+', ' ', start_text).strip()
        
        # 1. ç²¾ç¡®åŒ¹é…
        pos = full_text.find(cleaned_start_text)
        if pos != -1:
            return pos
            
        # 2. å°è¯•æ¨¡ç³ŠåŒ¹é… - åˆ†è¯åŒ¹é…
        words = cleaned_start_text.split()
        if len(words) > 3:
            # å°è¯•å‰3ä¸ªè¯
            partial_text = ' '.join(words[:3])
            pos = full_text.find(partial_text)
            if pos != -1:
                return pos
        
        # 3. å°è¯•æ›´å®½æ¾çš„åŒ¹é… - å»æ‰æ ‡ç‚¹ç¬¦å·
        import string
        cleaned_text = cleaned_start_text.translate(str.maketrans('', '', string.punctuation))
        pos = full_text.find(cleaned_text)
        if pos != -1:
            return pos
            
        # 4. å°è¯•å…³é”®è¯åŒ¹é…
        # å¦‚æœstart_textåŒ…å«å…³é”®è¯ï¼Œå°è¯•å•ç‹¬åŒ¹é…å…³é”®è¯
        keywords = ['æ€»ç»“', 'æ–¹æ¡ˆ', 'ç›‘æµ‹', 'é¡¹ç›®', 'å·¥ç¨‹', 'åˆ†æ', 'æªæ–½', 'å»ºè®®']
        for keyword in keywords:
            if keyword in cleaned_start_text and keyword in full_text:
                # æ‰¾åˆ°å…³é”®è¯åï¼Œå°è¯•æ‰¾åˆ°åŒ…å«æ›´å¤šä¸Šä¸‹æ–‡çš„ä½ç½®
                keyword_positions = []
                start_pos = 0
                while True:
                    pos = full_text.find(keyword, start_pos)
                    if pos == -1:
                        break
                    keyword_positions.append(pos)
                    start_pos = pos + 1
                
                # å¯¹æ¯ä¸ªå…³é”®è¯ä½ç½®ï¼Œæ£€æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦åŒ¹é…
                for pos in keyword_positions:
                    # å–å…³é”®è¯å‰å20ä¸ªå­—ç¬¦ä½œä¸ºä¸Šä¸‹æ–‡
                    context_start = max(0, pos - 20)
                    context_end = min(len(full_text), pos + len(keyword) + 20)
                    context = full_text[context_start:context_end]
                    
                    # æ£€æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦åŒ…å«start_textä¸­çš„å…¶ä»–è¯
                    word_matches = sum(1 for word in words if word in context)
                    if word_matches >= min(2, len(words) // 2):  # è‡³å°‘åŒ¹é…ä¸€åŠçš„è¯
                        return pos
        
        # 5. æœ€åçš„å›é€€ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œå®½æ¾åŒ¹é…
        try:
            # åªåŒ¹é…å‰10ä¸ªå­—ç¬¦ï¼Œå…è®¸ä¸€äº›å˜åŒ–
            pattern = re.escape(cleaned_start_text[:10])
            match = re.search(pattern, full_text, re.IGNORECASE)
            if match:
                return match.start()
        except:
            pass
            
        return -1
    
    def _generate_minimal_chunks(self, chapters: List[ChapterContent]) -> List[MinimalChunk]:
        """
        ç”Ÿæˆæœ€å°é¢—ç²’åº¦åˆ†å—
        
        Args:
            chapters: ç« èŠ‚å†…å®¹åˆ—è¡¨
            
        Returns:
            List[MinimalChunk]: æœ€å°å—åˆ—è¡¨
        """
        if self.use_ai_chunker and self.ai_chunker:
            # ä½¿ç”¨AIåˆ†å—å™¨è¿›è¡Œå¼‚æ­¥åˆ†å—
            return self._generate_chunks_with_ai(chapters)
        else:
            # ä½¿ç”¨ä¼ ç»Ÿçš„æ­£åˆ™åˆ†å—
            return self._generate_chunks_with_regex(chapters)
    
    def _generate_chunks_with_ai(self, chapters: List[ChapterContent]) -> List[MinimalChunk]:
        """ä½¿ç”¨AIåˆ†å—å™¨ç”Ÿæˆåˆ†å—"""
        print("ğŸ¤– ä½¿ç”¨AIåˆ†å—å™¨è¿›è¡Œæ™ºèƒ½åˆ†å—...")
        
        if not self.ai_chunker:
            print("âŒ AIåˆ†å—å™¨æœªåˆå§‹åŒ–ï¼Œå›é€€åˆ°æ­£åˆ™åˆ†å—")
            return self._generate_chunks_with_regex(chapters)
        
        # å‡†å¤‡ç« èŠ‚æ•°æ®
        chapter_data = []
        for chapter in chapters:
            chapter_data.append({
                'chapter_id': chapter.chapter_id,
                'title': chapter.title,
                'content': chapter.content
            })
        
        # ä½¿ç”¨AIåˆ†å—å™¨è¿›è¡Œæ‰¹é‡å¼‚æ­¥å¤„ç†
        try:
            ai_chunks = asyncio.run(self.ai_chunker.chunk_chapters_batch(chapter_data))
            print(f"âœ… AIåˆ†å—å®Œæˆï¼Œå…±ç”Ÿæˆ {len(ai_chunks)} ä¸ªåˆ†å—")
            
            # è½¬æ¢ä¸ºæœ¬åœ°MinimalChunkæ ¼å¼
            chunks = []
            for ai_chunk in ai_chunks:
                chunk = MinimalChunk(
                    chunk_id=ai_chunk.chunk_id,
                    content=ai_chunk.content,
                    chunk_type=ai_chunk.chunk_type,
                    belongs_to_chapter=ai_chunk.belongs_to_chapter,
                    chapter_title=ai_chunk.chapter_title,
                    start_pos=ai_chunk.start_pos,
                    end_pos=ai_chunk.end_pos,
                    word_count=ai_chunk.word_count
                )
                chunks.append(chunk)
            
            return chunks
        except Exception as e:
            print(f"âŒ AIåˆ†å—å¤±è´¥ï¼Œå›é€€åˆ°æ­£åˆ™åˆ†å—: {e}")
            return self._generate_chunks_with_regex(chapters)
    
    def _generate_chunks_with_regex(self, chapters: List[ChapterContent]) -> List[MinimalChunk]:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç”Ÿæˆåˆ†å—ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        print("ğŸ“ ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œä¼ ç»Ÿåˆ†å—...")
        
        minimal_chunks = []
        chunk_counter = 0
        
        for chapter in chapters:
            # ä¸ºæ¯ä¸ªç« èŠ‚ç”Ÿæˆæœ€å°å—
            chapter_chunks = self._chunk_single_chapter(chapter, chunk_counter)
            minimal_chunks.extend(chapter_chunks)
            chunk_counter += len(chapter_chunks)
            
        return minimal_chunks
    
    def _chunk_single_chapter(self, chapter: ChapterContent, start_chunk_id: int) -> List[MinimalChunk]:
        """
        å¯¹å•ä¸ªç« èŠ‚è¿›è¡Œæœ€å°å—åˆ†å‰²
        
        Args:
            chapter: ç« èŠ‚å†…å®¹
            start_chunk_id: èµ·å§‹å—ID
            
        Returns:
            List[MinimalChunk]: ç« èŠ‚å†…çš„æœ€å°å—åˆ—è¡¨
        """
        chunks = []
        content = chapter.content
        chunk_id = start_chunk_id
        
        # 1. æ ‡é¢˜å— - ç« èŠ‚æ ‡é¢˜ + å¼€å¤´1-2æ®µ
        title_chunk = self._create_title_chunk(chapter, chunk_id)
        if title_chunk:
            chunks.append(title_chunk)
            chunk_id += 1
        
        # 2. æ®µè½å— - å°†å†…å®¹åˆ†æˆæ®µè½
        paragraphs = self._split_into_paragraphs(content)
        
        for paragraph in paragraphs:
            if not paragraph.strip():
                continue
                
            # åˆ¤æ–­æ®µè½ç±»å‹
            chunk_type = self._determine_chunk_type(paragraph)
            
            # åˆ›å»ºæœ€å°å—
            chunk = MinimalChunk(
                chunk_id=f"{chapter.chapter_id}_{chunk_id}",
                content=paragraph.strip(),
                chunk_type=chunk_type,
                belongs_to_chapter=chapter.chapter_id,
                chapter_title=chapter.title,
                start_pos=0,  # ç›¸å¯¹äºç« èŠ‚çš„ä½ç½®
                end_pos=len(paragraph),
                word_count=len(paragraph.strip())
            )
            
            chunks.append(chunk)
            chunk_id += 1
            
        return chunks
    
    def _create_title_chunk(self, chapter: ChapterContent, chunk_id: int) -> Optional[MinimalChunk]:
        """
        åˆ›å»ºæ ‡é¢˜å—
        
        Args:
            chapter: ç« èŠ‚å†…å®¹
            chunk_id: å—ID
            
        Returns:
            Optional[MinimalChunk]: æ ‡é¢˜å—
        """
        # è·å–ç« èŠ‚å¼€å¤´çš„å†…å®¹ï¼ˆæ ‡é¢˜ + å‰1-2æ®µï¼‰
        lines = chapter.content.split('\n')
        title_content_lines = []
        
        # æ·»åŠ æ ‡é¢˜
        title_content_lines.append(chapter.title)
        
        # æ·»åŠ å‰å‡ è¡Œå†…å®¹
        for line in lines[:3]:  # å–å‰3è¡Œ
            if line.strip() and not line.startswith('[å›¾ç‰‡:') and not line.startswith('[è¡¨æ ¼:'):
                title_content_lines.append(line.strip())
        
        title_content = '\n'.join(title_content_lines)
        
        if len(title_content) < 50:  # å¦‚æœå¤ªçŸ­ï¼Œè¿”å›None
            return None
            
        return MinimalChunk(
            chunk_id=f"{chapter.chapter_id}_{chunk_id}",
            content=title_content,
            chunk_type="title",
            belongs_to_chapter=chapter.chapter_id,
            chapter_title=chapter.title,
            start_pos=0,
            end_pos=len(title_content),
            word_count=len(title_content)
        )
    
    def _split_into_paragraphs(self, content: str) -> List[str]:
        """
        å°†å†…å®¹åˆ†å‰²ä¸ºæ®µè½
        
        Args:
            content: å†…å®¹æ–‡æœ¬
            
        Returns:
            List[str]: æ®µè½åˆ—è¡¨
        """
        # æŒ‰åŒæ¢è¡Œåˆ†å‰²æ®µè½
        paragraphs = content.split('\n\n')
        
        # è¿›ä¸€æ­¥å¤„ç†ï¼Œç¡®ä¿æ¯ä¸ªæ®µè½éƒ½æœ‰åˆé€‚çš„é•¿åº¦
        processed_paragraphs = []
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
                
            # å¦‚æœæ®µè½å¤ªé•¿ï¼Œè¿›ä¸€æ­¥åˆ†å‰²
            if len(paragraph) > 1000:
                # æŒ‰å¥å·åˆ†å‰²
                sentences = paragraph.split('ã€‚')
                current_chunk = ""
                
                for sentence in sentences:
                    if len(current_chunk) + len(sentence) > 500:
                        if current_chunk:
                            processed_paragraphs.append(current_chunk + 'ã€‚')
                        current_chunk = sentence
                    else:
                        current_chunk += sentence + 'ã€‚'
                
                if current_chunk:
                    processed_paragraphs.append(current_chunk)
            else:
                processed_paragraphs.append(paragraph)
        
        return processed_paragraphs
    
    def _determine_chunk_type(self, paragraph: str) -> str:
        """
        ç¡®å®šæ®µè½ç±»å‹
        
        Args:
            paragraph: æ®µè½å†…å®¹
            
        Returns:
            str: æ®µè½ç±»å‹
        """
        paragraph = paragraph.strip()
        
        # å›¾ç‰‡æè¿°
        if paragraph.startswith('[å›¾ç‰‡:') or 'Error: An unexpected error occurred while fetching the image description' in paragraph:
            return "image_desc"
            
        # è¡¨æ ¼æè¿°
        if paragraph.startswith('[è¡¨æ ¼:') or 'è¡¨' in paragraph[:50]:
            return "table_desc"
            
        # åˆ—è¡¨é¡¹
        if paragraph.startswith('- ') or paragraph.startswith('â€¢ ') or re.match(r'^\d+\.', paragraph):
            return "list_item"
            
        # æ™®é€šæ®µè½
        return "paragraph"
    
    def save_chunking_result(self, result: ChunkingResult, output_dir: str):
        """
        ä¿å­˜åˆ†å—ç»“æœ
        
        Args:
            result: åˆ†å—ç»“æœ
            output_dir: è¾“å‡ºç›®å½•
        """
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜ç¬¬ä¸€çº§ç« èŠ‚
        chapters_data = {
            "chapters": [
                {
                    "chapter_id": chapter.chapter_id,
                    "title": chapter.title,
                    "content": chapter.content,
                    "start_pos": chapter.start_pos,
                    "end_pos": chapter.end_pos,
                    "word_count": chapter.word_count,
                    "has_images": chapter.has_images,
                    "has_tables": chapter.has_tables
                }
                for chapter in result.first_level_chapters
            ],
            "total_chapters": result.total_chapters,
            "processing_metadata": result.processing_metadata
        }
        
        chapters_file = os.path.join(output_dir, "first_level_chapters.json")
        with open(chapters_file, 'w', encoding='utf-8') as f:
            json.dump(chapters_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æœ€å°å—
        chunks_data = {
            "chunks": [
                {
                    "chunk_id": chunk.chunk_id,
                    "content": chunk.content,
                    "chunk_type": chunk.chunk_type,
                    "belongs_to_chapter": chunk.belongs_to_chapter,
                    "chapter_title": chunk.chapter_title,
                    "start_pos": chunk.start_pos,
                    "end_pos": chunk.end_pos,
                    "word_count": chunk.word_count
                }
                for chunk in result.minimal_chunks
            ],
            "total_chunks": result.total_chunks,
            "processing_metadata": result.processing_metadata
        }
        
        chunks_file = os.path.join(output_dir, "minimal_chunks.json")
        with open(chunks_file, 'w', encoding='utf-8') as f:
            json.dump(chunks_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“ åˆ†å—ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
        print(f"   - ç¬¬ä¸€çº§ç« èŠ‚: {chapters_file}")
        print(f"   - æœ€å°å—: {chunks_file}") 