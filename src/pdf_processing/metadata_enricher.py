#!/usr/bin/env python3
"""
å…ƒæ•°æ®å¢å¼ºå™¨
æ”¯æŒ"å°å—æ£€ç´¢ï¼Œå¤§å—å–‚å…»"æ¨¡å¼çš„å…ƒæ•°æ®å¢å¼º
é‡ç‚¹ï¼šç« èŠ‚å½’å±ä¿¡æ¯å’Œæ£€ç´¢ä¼˜åŒ–çš„å…ƒæ•°æ®
"""

import json
import logging
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime

from .config import PDFProcessingConfig
from .document_structure_analyzer import DocumentStructure, MinimalChunk, ChapterInfo
from .data_models import ProcessingResult, PageData, ImageWithContext, TableWithContext
try:
    from ..deepseek_client import DeepSeekClient
    from ..openrouter_client import OpenRouterClient
    from ..qwen_client import QwenClient
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    import sys
    sys.path.append('..')
    from deepseek_client import DeepSeekClient
    from openrouter_client import OpenRouterClient
    from qwen_client import QwenClient

logger = logging.getLogger(__name__)

@dataclass
class EnrichedChunk:
    """å¢å¼ºçš„æœ€å°åˆ†å—ä¿¡æ¯ - ä¸ºæ£€ç´¢ä¼˜åŒ–"""
    # åŸºç¡€åˆ†å—ä¿¡æ¯
    chunk_id: int
    content: str
    chunk_type: str  # paragraph, list_item, image_desc, table_desc
    
    # ç« èŠ‚å½’å±ä¿¡æ¯ï¼ˆæ ¸å¿ƒï¼‰
    belongs_to_chapter: str  # "1", "2.1", "3.2.1"
    chapter_title: str
    chapter_level: int
    chapter_summary: str  # æ‰€å±ç« èŠ‚çš„æ¦‚è¦
    
    # ç›¸å…³åª’ä½“ä¿¡æ¯
    related_images: List[Dict[str, Any]] = None
    related_tables: List[Dict[str, Any]] = None
    
    # æ£€ç´¢ä¼˜åŒ–
    summary: Optional[str] = None  # å—æ‘˜è¦ï¼ˆå¯é€‰ï¼‰
    hypothetical_questions: List[str] = None  # å‡è®¾é—®é¢˜
    
    def __post_init__(self):
        if self.related_images is None:
            self.related_images = []
        if self.related_tables is None:
            self.related_tables = []
        if self.hypothetical_questions is None:
            self.hypothetical_questions = []

@dataclass
class ChapterSummary:
    """ç« èŠ‚æ‘˜è¦ - ç”¨äº"å¤§å—å–‚å…»" """
    chapter_id: str  # "1", "2.1", "3.2.1"
    chapter_title: str
    level: int
    content_summary: str  # ç« èŠ‚æ¦‚è¦
    total_chunks: int  # åŒ…å«çš„åˆ†å—æ•°
    chunk_ids: List[int]  # åŒ…å«çš„åˆ†å—IDåˆ—è¡¨
    related_media_count: int  # ç›¸å…³åª’ä½“æ•°é‡

@dataclass
class IndexStructure:
    """æ£€ç´¢ä¼˜åŒ–çš„ç´¢å¼•ç»“æ„"""
    # å°å—æ£€ç´¢ç´¢å¼•
    minimal_chunks: List[EnrichedChunk]  # æœ€å°é¢—ç²’åº¦åˆ†å—ï¼Œç”¨äºç²¾ç¡®æ£€ç´¢
    
    # å¤§å—å–‚å…»ç´¢å¼•
    chapter_summaries: List[ChapterSummary]  # ç« èŠ‚æ‘˜è¦ï¼Œç”¨äºä¸Šä¸‹æ–‡æä¾›
    
    # é—®é¢˜ç´¢å¼•
    hypothetical_questions: List[Dict[str, Any]]  # å‡è®¾é—®é¢˜ç´¢å¼•ï¼Œæå‡å¬å›ç‡
    
class MetadataEnricher:
    """
    å…ƒæ•°æ®å¢å¼ºå™¨
    æ”¯æŒ"å°å—æ£€ç´¢ï¼Œå¤§å—å–‚å…»"æ¨¡å¼ï¼š
    1. ä¸ºæœ€å°åˆ†å—æ·»åŠ ç« èŠ‚å½’å±ä¿¡æ¯
    2. ç”Ÿæˆç« èŠ‚æ‘˜è¦ç”¨äºä¸Šä¸‹æ–‡æä¾›
    3. å…³è”å›¾ç‰‡/è¡¨æ ¼åˆ°ç›¸åº”åˆ†å—
    """
    
    def __init__(self, config: PDFProcessingConfig = None):
        self.config = config or PDFProcessingConfig()
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯ï¼ˆç”¨äºæ‘˜è¦å’Œé—®é¢˜ç”Ÿæˆï¼‰
        self.llm_client = self._init_llm_client()
        
        logger.info("âœ… å…ƒæ•°æ®å¢å¼ºå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _init_llm_client(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        try:
            if self.config.ai_content.default_llm_model.startswith('qwen'):
                return QwenClient(
                    model=self.config.ai_content.default_llm_model,
                    max_tokens=self.config.ai_content.max_context_length // 10,
                    max_retries=self.config.ai_content.max_retries,
                    enable_batch_mode=True
                )
            elif self.config.ai_content.default_llm_model.startswith('deepseek'):
                return DeepSeekClient()
            elif self.config.ai_content.default_llm_model.startswith('google/gemini'):
                return OpenRouterClient()
            else:
                logger.warning(f"ä¸æ”¯æŒçš„LLMæ¨¡å‹: {self.config.ai_content.default_llm_model}")
                return None
        except Exception as e:
            logger.error(f"LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            return None
    
    def enrich_metadata(self, 
                       document_structure: DocumentStructure,
                       minimal_chunks: List[MinimalChunk],
                       processing_result: ProcessingResult) -> IndexStructure:
        """
        å¢å¼ºå…ƒæ•°æ®å¹¶ç”Ÿæˆæ£€ç´¢ä¼˜åŒ–çš„ç´¢å¼•ç»“æ„
        
        Args:
            document_structure: æ–‡æ¡£ç»“æ„
            minimal_chunks: æœ€å°åˆ†å—åˆ—è¡¨
            processing_result: PDFå¤„ç†ç»“æœ
            
        Returns:
            IndexStructure: æ£€ç´¢ä¼˜åŒ–çš„ç´¢å¼•ç»“æ„
        """
        logger.info(f"ğŸ” å¼€å§‹å…ƒæ•°æ®å¢å¼ºï¼Œå¤„ç† {len(minimal_chunks)} ä¸ªæœ€å°åˆ†å—")
        
        # ç¬¬ä¸€æ­¥ï¼šåŸºç¡€å…ƒæ•°æ®å¢å¼º
        enriched_chunks = self._enrich_basic_metadata(minimal_chunks, document_structure)
        
        # ç¬¬äºŒæ­¥ï¼šå…³è”å›¾ç‰‡å’Œè¡¨æ ¼
        enriched_chunks = self._associate_media_with_chunks(enriched_chunks, processing_result, document_structure)
        
        # ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆå‡è®¾é—®é¢˜ï¼ˆå¯é€‰ï¼‰
        if self.llm_client:
            enriched_chunks = self._generate_hypothetical_questions(enriched_chunks)
        
        # ç¬¬å››æ­¥ï¼šç”Ÿæˆç« èŠ‚æ‘˜è¦
        chapter_summaries = self._build_chapter_summaries(enriched_chunks, document_structure)
        
        # ç¬¬äº”æ­¥ï¼šæ„å»ºé—®é¢˜ç´¢å¼•
        hypothetical_questions = self._build_question_index(enriched_chunks)
        
        # æ„å»ºæœ€ç»ˆç´¢å¼•ç»“æ„
        index_structure = IndexStructure(
            minimal_chunks=enriched_chunks,
            chapter_summaries=chapter_summaries,
            hypothetical_questions=hypothetical_questions
        )
        
        logger.info(f"âœ… å…ƒæ•°æ®å¢å¼ºå®Œæˆï¼Œç”Ÿæˆ {len(enriched_chunks)} ä¸ªå¢å¼ºåˆ†å—ï¼Œ{len(chapter_summaries)} ä¸ªç« èŠ‚æ‘˜è¦")
        
        return index_structure
    
    def _enrich_basic_metadata(self, 
                              minimal_chunks: List[MinimalChunk], 
                              document_structure: DocumentStructure) -> List[EnrichedChunk]:
        """å¢å¼ºåŸºç¡€å…ƒæ•°æ®"""
        enriched_chunks = []
        
        # åˆ›å»ºç« èŠ‚IDåˆ°ç« èŠ‚ä¿¡æ¯çš„æ˜ å°„
        chapter_map = {chapter.chapter_id: chapter for chapter in document_structure.toc}
        
        for chunk in minimal_chunks:
            chapter_info = chapter_map.get(chunk.belongs_to_chapter)
            chapter_summary = chapter_info.content_summary if chapter_info else ""
            
            enriched_chunk = EnrichedChunk(
                chunk_id=chunk.chunk_id,
                content=chunk.content,
                chunk_type=chunk.chunk_type,
                belongs_to_chapter=chunk.belongs_to_chapter,
                chapter_title=chunk.chapter_title,
                chapter_level=chunk.chapter_level,
                chapter_summary=chapter_summary
            )
            
            enriched_chunks.append(enriched_chunk)
        
        return enriched_chunks
    
    def _associate_media_with_chunks(self, 
                                   enriched_chunks: List[EnrichedChunk],
                                   processing_result: ProcessingResult,
                                   document_structure: DocumentStructure) -> List[EnrichedChunk]:
        """å…³è”å›¾ç‰‡å’Œè¡¨æ ¼åˆ°ç›¸åº”çš„åˆ†å—ï¼ˆé€šè¿‡ç« èŠ‚å½’å±ï¼‰"""
        logger.info("ğŸ–¼ï¸ å…³è”å›¾ç‰‡å’Œè¡¨æ ¼åˆ°åˆ†å—")
        
        # æ”¶é›†æ‰€æœ‰åª’ä½“å¹¶æŒ‰å†…å®¹åŒ¹é…åˆ°ç« èŠ‚
        all_images = processing_result.get_all_images()
        all_tables = processing_result.get_all_tables()
        
        # ä¸ºæ¯ä¸ªåˆ†å—å…³è”ç›¸å…³åª’ä½“
        for chunk in enriched_chunks:
            # ç®€åŒ–çš„å…³è”ç­–ç•¥ï¼šå¦‚æœå›¾ç‰‡/è¡¨æ ¼çš„é¡µé¢ä¸Šä¸‹æ–‡åŒ…å«åˆ†å—å†…å®¹çš„å…³é”®è¯ï¼Œå°±å…³è”
            chunk_keywords = self._extract_keywords(chunk.content)
            
            # å…³è”å›¾ç‰‡
            for image in all_images:
                if self._is_media_related_to_chunk(image.page_context, chunk_keywords, chunk.content):
                    image_info = {
                        "image_path": image.image_path,
                        "page_number": image.page_number,
                        "ai_description": image.ai_description
                    }
                    chunk.related_images.append(image_info)
            
            # å…³è”è¡¨æ ¼
            for table in all_tables:
                if self._is_media_related_to_chunk(table.page_context, chunk_keywords, chunk.content):
                    table_info = {
                        "table_path": table.table_path,
                        "page_number": table.page_number,
                        "ai_description": table.ai_description
                    }
                    chunk.related_tables.append(table_info)
        
        return enriched_chunks
    
    def _extract_keywords(self, content: str) -> List[str]:
        """ä»å†…å®¹ä¸­æå–å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå–ï¼šå»é™¤å¸¸è§è¯æ±‡ï¼Œä¿ç•™é•¿åº¦>2çš„è¯
        common_words = {'çš„', 'æ˜¯', 'åœ¨', 'äº†', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'å› ä¸º', 'æ‰€ä»¥', 'è¿™', 'é‚£', 'è¿™ä¸ª', 'é‚£ä¸ª'}
        words = content.replace('ï¼Œ', ' ').replace('ã€‚', ' ').replace('ã€', ' ').split()
        keywords = [word.strip() for word in words if len(word) > 2 and word not in common_words]
        return keywords[:10]  # åªå–å‰10ä¸ªå…³é”®è¯
    
    def _is_media_related_to_chunk(self, media_context: str, chunk_keywords: List[str], chunk_content: str) -> bool:
        """åˆ¤æ–­åª’ä½“æ˜¯å¦ä¸åˆ†å—ç›¸å…³"""
        # å¦‚æœåª’ä½“ä¸Šä¸‹æ–‡åŒ…å«åˆ†å—çš„å…³é”®è¯ï¼Œæˆ–è€…æœ‰å†…å®¹é‡å ï¼Œåˆ™è®¤ä¸ºç›¸å…³
        if len(chunk_keywords) == 0:
            return False
        
        match_count = sum(1 for keyword in chunk_keywords if keyword in media_context)
        return match_count >= min(2, len(chunk_keywords) // 2)  # è‡³å°‘åŒ¹é…ä¸€åŠå…³é”®è¯
    
    def _generate_hypothetical_questions(self, enriched_chunks: List[EnrichedChunk]) -> List[EnrichedChunk]:
        """ç”Ÿæˆå‡è®¾é—®é¢˜ï¼ˆå¯é€‰ï¼Œç”¨äºæå‡å¬å›ç‡ï¼‰"""
        logger.info("ğŸ§  ç”Ÿæˆå‡è®¾é—®é¢˜")
        
        for chunk in enriched_chunks:
            # åªä¸ºè¾ƒé•¿çš„æ®µè½ç”Ÿæˆé—®é¢˜
            if chunk.chunk_type == "paragraph" and len(chunk.content) > 100:
                try:
                    questions_prompt = f"""
åŸºäºä»¥ä¸‹æ–‡æœ¬å†…å®¹ï¼Œç”Ÿæˆ2-3ä¸ªå‡è®¾é—®é¢˜ï¼Œè¿™äº›é—®é¢˜åº”è¯¥èƒ½å¤Ÿè¢«è¯¥æ–‡æœ¬å›ç­”ï¼š

{chunk.content}

é—®é¢˜è¦æ±‚ï¼š
1. ä½¿ç”¨è‡ªç„¶çš„æé—®æ–¹å¼
2. é€‚åˆä½œä¸ºæœç´¢æŸ¥è¯¢
3. æ¯ä¸ªé—®é¢˜ä¸€è¡Œï¼Œä¸éœ€è¦ç¼–å·

è¾“å‡ºæ ¼å¼ï¼š
é—®é¢˜1
é—®é¢˜2
é—®é¢˜3
"""
                    
                    questions_response = self.llm_client.call_api(questions_prompt)
                    chunk.hypothetical_questions = [q.strip() for q in questions_response.split('\n') if q.strip()]
                    
                except Exception as e:
                    logger.error(f"ç”Ÿæˆå‡è®¾é—®é¢˜å¤±è´¥: {e}")
                    chunk.hypothetical_questions = []
        
        return enriched_chunks
    
    def _build_chapter_summaries(self, 
                               enriched_chunks: List[EnrichedChunk],
                               document_structure: DocumentStructure) -> List[ChapterSummary]:
        """æ„å»ºç« èŠ‚æ‘˜è¦ï¼ˆç”¨äº"å¤§å—å–‚å…»"ï¼‰"""
        logger.info("ğŸ—ï¸ æ„å»ºç« èŠ‚æ‘˜è¦")
        
        chapter_summaries = []
        
        # æŒ‰ç« èŠ‚åˆ†ç»„åˆ†å—
        chapters_dict = {}
        for chunk in enriched_chunks:
            chapter_id = chunk.belongs_to_chapter
            if chapter_id not in chapters_dict:
                chapters_dict[chapter_id] = []
            chapters_dict[chapter_id].append(chunk)
        
        # ä¸ºæ¯ä¸ªç« èŠ‚ç”Ÿæˆæ‘˜è¦
        for chapter_info in document_structure.toc:
            chapter_id = chapter_info.chapter_id
            chunks_in_chapter = chapters_dict.get(chapter_id, [])
            
            if chunks_in_chapter:
                # ç»Ÿè®¡ç›¸å…³åª’ä½“æ•°é‡
                total_images = sum(len(chunk.related_images) for chunk in chunks_in_chapter)
                total_tables = sum(len(chunk.related_tables) for chunk in chunks_in_chapter)
                
                chapter_summary = ChapterSummary(
                    chapter_id=chapter_id,
                    chapter_title=chapter_info.title,
                    level=chapter_info.level,
                    content_summary=chapter_info.content_summary,
                    total_chunks=len(chunks_in_chapter),
                    chunk_ids=[chunk.chunk_id for chunk in chunks_in_chapter],
                    related_media_count=total_images + total_tables
                )
                
                chapter_summaries.append(chapter_summary)
        
        return chapter_summaries
    
    def _build_question_index(self, enriched_chunks: List[EnrichedChunk]) -> List[Dict[str, Any]]:
        """æ„å»ºå‡è®¾é—®é¢˜ç´¢å¼•"""
        question_index = []
        
        for chunk in enriched_chunks:
            for question in chunk.hypothetical_questions:
                question_entry = {
                    'question': question,
                    'chunk_id': chunk.chunk_id,
                    'belongs_to_chapter': chunk.belongs_to_chapter,
                    'chapter_title': chunk.chapter_title
                }
                question_index.append(question_entry)
        
        return question_index
    
    def save_index_structure(self, index_structure: IndexStructure, output_path: str):
        """ä¿å­˜ç´¢å¼•ç»“æ„åˆ°æ–‡ä»¶"""
        try:
            # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
            serializable_data = {
                'minimal_chunks': [asdict(chunk) for chunk in index_structure.minimal_chunks],
                'chapter_summaries': [asdict(summary) for summary in index_structure.chapter_summaries],
                'hypothetical_questions': index_structure.hypothetical_questions
            }
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(serializable_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ“„ ç´¢å¼•ç»“æ„å·²ä¿å­˜åˆ°: {output_path}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç´¢å¼•ç»“æ„å¤±è´¥: {e}")
    
    def load_index_structure(self, input_path: str) -> IndexStructure:
        """ä»æ–‡ä»¶åŠ è½½ç´¢å¼•ç»“æ„"""
        try:
            with open(input_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # é‡å»ºå¯¹è±¡
            minimal_chunks = []
            for chunk_data in data['minimal_chunks']:
                chunk = EnrichedChunk(**chunk_data)
                minimal_chunks.append(chunk)
            
            chapter_summaries = []
            for summary_data in data['chapter_summaries']:
                summary = ChapterSummary(**summary_data)
                chapter_summaries.append(summary)
            
            index_structure = IndexStructure(
                minimal_chunks=minimal_chunks,
                chapter_summaries=chapter_summaries,
                hypothetical_questions=data['hypothetical_questions']
            )
            
            logger.info(f"ğŸ“„ ç´¢å¼•ç»“æ„å·²ä»æ–‡ä»¶åŠ è½½: {input_path}")
            return index_structure
            
        except Exception as e:
            logger.error(f"åŠ è½½ç´¢å¼•ç»“æ„å¤±è´¥: {e}")
            raise 