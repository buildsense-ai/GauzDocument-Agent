#!/usr/bin/env python3
"""
æ–‡æ¡£æ‘˜è¦ç”Ÿæˆå™¨
åŸºäºæ–‡æ¡£çš„å„ä¸ªéƒ¨åˆ†ç”Ÿæˆæ–‡æ¡£çº§åˆ«çš„æ‘˜è¦
"""

import json
import os
import time
from datetime import datetime
from typing import List, Dict, Any, Optional
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from .metadata_schema import DocumentSummaryMetadata, DocumentType, create_content_id
from .text_chunker import ChunkingResult
from src.qwen_client import QwenClient


class DocumentSummaryGenerator:
    """æ–‡æ¡£æ‘˜è¦ç”Ÿæˆå™¨"""
    
    def __init__(self, model: str = "qwen-plus"):
        """
        åˆå§‹åŒ–æ–‡æ¡£æ‘˜è¦ç”Ÿæˆå™¨
        
        Args:
            model: ä½¿ç”¨çš„AIæ¨¡å‹
        """
        self.client = QwenClient(model=model, temperature=0.3)
        self.model = model
        
    def generate_document_summary(self, 
                                document_info: Dict[str, Any],
                                chunking_result: ChunkingResult,
                                toc_data: Dict[str, Any],
                                image_count: int = 0,
                                table_count: int = 0) -> tuple[DocumentSummaryMetadata, str]:
        """
        ç”Ÿæˆæ–‡æ¡£æ‘˜è¦
        
        Args:
            document_info: æ–‡æ¡£åŸºç¡€ä¿¡æ¯
            chunking_result: åˆ†å—ç»“æœ
            toc_data: TOCæ•°æ®
            image_count: å›¾ç‰‡æ•°é‡
            table_count: è¡¨æ ¼æ•°é‡
            
        Returns:
            DocumentSummaryMetadata: æ–‡æ¡£æ‘˜è¦å…ƒæ•°æ®
        """
        start_time = time.time()
        
        # 1. æ”¶é›†æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯
        stats = self._collect_document_stats(document_info, chunking_result, toc_data, image_count, table_count)
        
        # 2. æ¨æ–­æ–‡æ¡£ç±»å‹
        document_type = self._infer_document_type(document_info, toc_data, stats)
        
        # 3. ç”Ÿæˆæ‘˜è¦å†…å®¹
        summary_content = self._generate_summary_content(document_info, chunking_result, toc_data, stats)
        
        # 4. åˆ›å»ºDocumentSummaryMetadata
        processing_time = time.time() - start_time
        content_id = create_content_id(document_info["document_id"], "document_summary", 1)
        
        document_summary = DocumentSummaryMetadata(
            content_id=content_id,
            document_id=document_info["document_id"],
            document_type_id=document_type.type_id,
            source_file_path=document_info["source_file_path"],
            file_name=document_info["file_name"],
            file_size=self._get_file_size(document_info["source_file_path"]),
            total_pages=stats["total_pages"],
            total_word_count=stats["total_word_count"],
            chapter_count=stats["chapter_count"],
            image_count=image_count,
            table_count=table_count,
            processing_time=processing_time,
            created_at=datetime.now(),
            toc_root_id="1"  # å‡è®¾æ ¹èŠ‚ç‚¹IDä¸º1
        )
        
        return document_summary, summary_content
    
    def _collect_document_stats(self, document_info: Dict[str, Any], 
                               chunking_result: ChunkingResult,
                               toc_data: Dict[str, Any],
                               image_count: int,
                               table_count: int) -> Dict[str, Any]:
        """æ”¶é›†æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯"""
        
        # è®¡ç®—æ€»å­—æ•°
        total_word_count = sum(chunk.word_count for chunk in chunking_result.minimal_chunks)
        
        # ç« èŠ‚ç»Ÿè®¡
        chapter_count = len(chunking_result.first_level_chapters)
        
        stats = {
            "total_pages": document_info["total_pages"],
            "total_word_count": total_word_count,
            "chapter_count": chapter_count,
            "image_count": image_count,
            "table_count": table_count,
            "chunk_count": len(chunking_result.minimal_chunks),
            "avg_words_per_chapter": total_word_count // chapter_count if chapter_count > 0 else 0,
            "avg_words_per_page": total_word_count // document_info["total_pages"] if document_info["total_pages"] > 0 else 0
        }
        
        return stats
    
    def _infer_document_type(self, document_info: Dict[str, Any], 
                            toc_data: Dict[str, Any], 
                            stats: Dict[str, Any]) -> DocumentType:
        """æ¨æ–­æ–‡æ¡£ç±»å‹"""
        
        file_name = document_info["file_name"].lower()
        toc_items = toc_data.get("toc_items", [])
        
        # åŸºäºæ–‡ä»¶åçš„ç®€å•æ¨æ–­
        if any(keyword in file_name for keyword in ["è®¾è®¡æ–¹æ¡ˆ", "è®¾è®¡", "æ–¹æ¡ˆ"]):
            return DocumentType(
                type_id="design_proposal",
                type_name="è®¾è®¡æ–¹æ¡ˆ",
                category="å·¥ç¨‹èµ„æ–™",
                description="å»ºç­‘å·¥ç¨‹è®¾è®¡æ–¹æ¡ˆæ–‡æ¡£",
                typical_structure=["é¡¹ç›®æ¦‚å†µ", "è®¾è®¡ä¾æ®", "æ–¹æ¡ˆè®¾è®¡", "æŠ€æœ¯æªæ–½"],
                created_at=datetime.now()
            )
        elif any(keyword in file_name for keyword in ["è¯„ä¼°", "æŠ¥å‘Š", "åˆ†æ"]):
            return DocumentType(
                type_id="assessment_report",
                type_name="è¯„ä¼°æŠ¥å‘Š",
                category="å·¥ç¨‹èµ„æ–™",
                description="å·¥ç¨‹é¡¹ç›®è¯„ä¼°æŠ¥å‘Š",
                typical_structure=["é¡¹ç›®èƒŒæ™¯", "è¯„ä¼°å†…å®¹", "åˆ†æç»“æœ", "å»ºè®®æªæ–½"],
                created_at=datetime.now()
            )
        elif any(keyword in file_name for keyword in ["è§„èŒƒ", "æ ‡å‡†", "è§„ç¨‹"]):
            return DocumentType(
                type_id="standard_specification",
                type_name="æ ‡å‡†è§„èŒƒ",
                category="æ ‡å‡†è§„èŒƒ",
                description="è¡Œä¸šæ ‡å‡†è§„èŒƒæ–‡æ¡£",
                typical_structure=["æ€»åˆ™", "æœ¯è¯­", "åŸºæœ¬è¦æ±‚", "æŠ€æœ¯è¦æ±‚"],
                created_at=datetime.now()
            )
        else:
            return DocumentType(
                type_id="general_document",
                type_name="é€šç”¨æ–‡æ¡£",
                category="é€šç”¨èµ„æ–™",
                description="é€šç”¨æŠ€æœ¯æ–‡æ¡£",
                typical_structure=["æ¦‚è¿°", "å†…å®¹", "ç»“è®º"],
                created_at=datetime.now()
            )
    
    def _generate_summary_content(self, document_info: Dict[str, Any],
                                 chunking_result: ChunkingResult,
                                 toc_data: Dict[str, Any],
                                 stats: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ‘˜è¦å†…å®¹"""
        
        # å‡†å¤‡è¾“å…¥æ•°æ®
        chapters_info = []
        for chapter in chunking_result.first_level_chapters:
            chapters_info.append({
                "title": chapter.title,
                "word_count": chapter.word_count,
                "content_preview": chapter.content[:200] + "..." if len(chapter.content) > 200 else chapter.content
            })
        
        # æ„å»ºæç¤ºè¯
        prompt = self._build_summary_prompt(document_info, chapters_info, stats)
        
        try:
            # è°ƒç”¨AIç”Ÿæˆæ‘˜è¦
            summary_content = self.client.generate_response(prompt)
            
            # åå¤„ç†æ‘˜è¦å†…å®¹
            summary_content = self._post_process_summary(summary_content)
            
            return summary_content
            
        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆæ–‡æ¡£æ‘˜è¦æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            # è¿”å›åŸºç¡€æ‘˜è¦
            return self._generate_basic_summary(document_info, stats)
    
    def _build_summary_prompt(self, document_info: Dict[str, Any], 
                             chapters_info: List[Dict[str, Any]], 
                             stats: Dict[str, Any]) -> str:
        """æ„å»ºæ‘˜è¦ç”Ÿæˆæç¤ºè¯"""
        
        chapters_text = "\n".join([
            f"- {chapter['title']} ({chapter['word_count']}å­—)\n  å†…å®¹é¢„è§ˆ: {chapter['content_preview']}"
            for chapter in chapters_info
        ])
        
        prompt = f"""
è¯·ä¸ºä»¥ä¸‹æ–‡æ¡£ç”Ÿæˆä¸€ä¸ªç»¼åˆæ€§æ‘˜è¦ï¼š

## æ–‡æ¡£åŸºæœ¬ä¿¡æ¯
- æ–‡æ¡£åç§°: {document_info['file_name']}
- æ€»é¡µæ•°: {stats['total_pages']}é¡µ
- æ€»å­—æ•°: {stats['total_word_count']}å­—
- ç« èŠ‚æ•°: {stats['chapter_count']}ç« 
- å›¾ç‰‡æ•°: {stats['image_count']}å¼ 
- è¡¨æ ¼æ•°: {stats['table_count']}ä¸ª

## ç« èŠ‚ç»“æ„
{chapters_text}

## è¦æ±‚
è¯·ç”Ÿæˆä¸€ä¸ª200-300å­—çš„æ–‡æ¡£æ‘˜è¦ï¼ŒåŒ…å«ä»¥ä¸‹å†…å®¹ï¼š
1. æ–‡æ¡£çš„ä¸»è¦ç›®çš„å’Œæ€§è´¨
2. æ ¸å¿ƒå†…å®¹æ¦‚æ‹¬
3. ä¸»è¦ç« èŠ‚çš„é‡ç‚¹å†…å®¹
4. æ–‡æ¡£çš„ä»·å€¼å’Œé€‚ç”¨èŒƒå›´

è¯·ç”¨ä¸“ä¸šã€ç®€æ´çš„è¯­è¨€ï¼Œç¡®ä¿æ‘˜è¦å‡†ç¡®åæ˜ æ–‡æ¡£çš„æ ¸å¿ƒå†…å®¹ã€‚
"""
        
        return prompt
    
    def _post_process_summary(self, summary_content: str) -> str:
        """åå¤„ç†æ‘˜è¦å†…å®¹"""
        # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
        lines = [line.strip() for line in summary_content.split('\n') if line.strip()]
        return '\n'.join(lines)
    
    def _generate_basic_summary(self, document_info: Dict[str, Any], stats: Dict[str, Any]) -> str:
        """ç”ŸæˆåŸºç¡€æ‘˜è¦ï¼ˆå½“AIç”Ÿæˆå¤±è´¥æ—¶ä½¿ç”¨ï¼‰"""
        return f"""
{document_info['file_name']} æ˜¯ä¸€ä»½åŒ…å«{stats['total_pages']}é¡µã€{stats['chapter_count']}ä¸ªç« èŠ‚çš„æŠ€æœ¯æ–‡æ¡£ã€‚
æ–‡æ¡£æ€»å­—æ•°çº¦{stats['total_word_count']}å­—ï¼ŒåŒ…å«{stats['image_count']}å¼ å›¾ç‰‡å’Œ{stats['table_count']}ä¸ªè¡¨æ ¼ã€‚
è¯¥æ–‡æ¡£æŒ‰ç« èŠ‚ç»“æ„ç»„ç»‡ï¼Œæ¶µç›–äº†é¡¹ç›®çš„å„ä¸ªæ–¹é¢ï¼Œä¸ºç›¸å…³å·¥ä½œæä¾›äº†è¯¦ç»†çš„æŠ€æœ¯æŒ‡å¯¼å’Œå‚è€ƒèµ„æ–™ã€‚
"""
    
    def _get_file_size(self, file_path: str) -> int:
        """è·å–æ–‡ä»¶å¤§å°"""
        try:
            return os.path.getsize(file_path)
        except:
            return 0
    
    def save_document_summary(self, summary_metadata: DocumentSummaryMetadata, 
                            summary_content: str, output_dir: str):
        """
        ä¿å­˜æ–‡æ¡£æ‘˜è¦
        
        Args:
            summary_metadata: æ‘˜è¦å…ƒæ•°æ®
            summary_content: æ‘˜è¦å†…å®¹
            output_dir: è¾“å‡ºç›®å½•
        """
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata_file = os.path.join(output_dir, "document_summary_metadata.json")
        metadata_dict = {
            "content_id": summary_metadata.content_id,
            "document_id": summary_metadata.document_id,
            "document_type_id": summary_metadata.document_type_id,
            "source_file_path": summary_metadata.source_file_path,
            "file_name": summary_metadata.file_name,
            "file_size": summary_metadata.file_size,
            "total_pages": summary_metadata.total_pages,
            "total_word_count": summary_metadata.total_word_count,
            "chapter_count": summary_metadata.chapter_count,
            "image_count": summary_metadata.image_count,
            "table_count": summary_metadata.table_count,
            "processing_time": summary_metadata.processing_time,
            "created_at": summary_metadata.created_at.isoformat(),
            "toc_root_id": summary_metadata.toc_root_id
        }
        
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata_dict, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æ‘˜è¦å†…å®¹
        content_file = os.path.join(output_dir, "document_summary_content.txt")
        with open(content_file, 'w', encoding='utf-8') as f:
            f.write(summary_content)
        
        print(f"ğŸ“„ æ–‡æ¡£æ‘˜è¦å·²ä¿å­˜:")
        print(f"   - å…ƒæ•°æ®: {metadata_file}")
        print(f"   - å†…å®¹: {content_file}")
        print(f"   - å¤„ç†æ—¶é—´: {summary_metadata.processing_time:.2f}ç§’") 