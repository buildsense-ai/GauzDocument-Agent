#!/usr/bin/env python3
"""
Metadataæå–å™¨
å°†ç°æœ‰çš„å¤„ç†ç»“æœè½¬æ¢ä¸ºæ ‡å‡†åŒ–çš„metadataç»“æ„
"""

import json
import os
import uuid
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from .metadata_schema import (
    DocumentSummaryMetadata, ChapterSummaryMetadata, 
    TextChunkMetadata, ImageChunkMetadata, TableChunkMetadata,
    DocumentType, DocumentTOC, DocumentScope, ContentType,
    create_content_id, UNIVERSAL_METADATA_FIELDS
)
from .text_chunker import ChunkingResult


class MetadataExtractor:
    """ä»ç°æœ‰æ•°æ®ç»“æ„æå–æ ‡å‡†åŒ–metadata"""
    
    def __init__(self, project_name: str = "default_project"):
        """
        åˆå§‹åŒ–metadataæå–å™¨
        
        Args:
            project_name: é¡¹ç›®åç§°
        """
        self.project_name = project_name
        self.document_scope = DocumentScope.PROJECT
        
    def extract_from_page_split_result(self, page_split_file: str) -> Dict[str, Any]:
        """
        ä»page_splitç»“æœæå–åŸºç¡€metadata
        
        Args:
            page_split_file: page_splitç»“æœæ–‡ä»¶è·¯å¾„
            
        Returns:
            DictåŒ…å«æå–çš„åŸºç¡€ä¿¡æ¯
        """
        if not os.path.exists(page_split_file):
            raise FileNotFoundError(f"Page splitç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {page_split_file}")
            
        with open(page_split_file, 'r', encoding='utf-8') as f:
            page_split_data = json.load(f)
            
        # æå–æ–‡æ¡£åŸºç¡€ä¿¡æ¯
        document_info = {
            "document_id": self._generate_document_id(page_split_data.get("source_file", "unknown")),
            "source_file_path": page_split_data.get("source_file", ""),
            "file_name": Path(page_split_data.get("source_file", "")).name,
            "total_pages": len(page_split_data.get("pages", [])),
            "processing_time": page_split_data.get("processing_time", 0.0),
            "created_at": datetime.now()
        }
        
        # æå–å›¾ç‰‡metadata
        image_metadata = self._extract_image_metadata(page_split_data, document_info["document_id"])
        
        # æå–è¡¨æ ¼metadata
        table_metadata = self._extract_table_metadata(page_split_data, document_info["document_id"])
        
        return {
            "document_info": document_info,
            "image_metadata": image_metadata,
            "table_metadata": table_metadata,
            "page_split_data": page_split_data
        }
    
    def extract_from_toc_result(self, toc_file: str, document_id: str) -> Tuple[DocumentTOC, Dict[str, Any]]:
        """
        ä»TOCç»“æœæå–TOC metadata
        
        Args:
            toc_file: TOCç»“æœæ–‡ä»¶è·¯å¾„
            document_id: æ–‡æ¡£ID
            
        Returns:
            Tuple[DocumentTOC, Dict[str, Any]]: TOCå…ƒæ•°æ®å’Œç« èŠ‚æ˜ å°„
        """
        if not os.path.exists(toc_file):
            raise FileNotFoundError(f"TOCç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {toc_file}")
            
        with open(toc_file, 'r', encoding='utf-8') as f:
            toc_data = json.load(f)
            
        # åˆ›å»ºDocumentTOC
        toc_items = toc_data.get("toc_items", [])
        doc_toc = DocumentTOC(
            document_id=document_id,
            toc_json=json.dumps(toc_items, ensure_ascii=False),
            chapter_count=len([item for item in toc_items if item.get("level") == 1]),
            total_sections=len(toc_items),
            created_at=datetime.now()
        )
        
        # åˆ›å»ºç« èŠ‚æ˜ å°„ï¼ˆID -> æ ‡é¢˜ï¼‰
        chapter_mapping = {}
        for item in toc_items:
            if item.get("level") == 1:  # åªå¤„ç†ç¬¬ä¸€çº§ç« èŠ‚
                chapter_mapping[item["id"]] = {
                    "title": item["title"],
                    "order": int(item["id"]) if item["id"].isdigit() else 0
                }
        
        return doc_toc, chapter_mapping
    
    def extract_from_chunking_result(self, chunking_result: ChunkingResult, 
                                   image_metadata: List[ImageChunkMetadata],
                                   table_metadata: List[TableChunkMetadata]) -> Dict[str, Any]:
        """
        ä»chunkingç»“æœä¸­æå–metadataå¹¶ç²¾ç¡®åˆ†é…ç« èŠ‚
        
        Args:
            chunking_result: æ–‡æœ¬åˆ†å—ç»“æœ
            image_metadata: å›¾ç‰‡å…ƒæ•°æ®åˆ—è¡¨  
            table_metadata: è¡¨æ ¼å…ƒæ•°æ®åˆ—è¡¨
            
        Returns:
            Dict: æå–çš„å…ƒæ•°æ®
        """
        # ç²¾ç¡®åˆ†é…å›¾ç‰‡å’Œè¡¨æ ¼çš„ç« èŠ‚ID
        self._assign_precise_chapter_ids_from_chunks(
            image_metadata, table_metadata, chunking_result
        )
        
        # ç”Ÿæˆchunkçº§åˆ«çš„metadata
        text_chunk_metadata = self._extract_text_chunk_metadata(
            chunking_result, image_metadata[0].document_id if image_metadata else "unknown"
        )
        
        return {
            "text_chunks": text_chunk_metadata,
            "image_metadata": image_metadata,
            "table_metadata": table_metadata,
            "chunking_result": chunking_result
        }

    def _assign_precise_chapter_ids_from_chunks(self, 
                                              image_metadata: List[ImageChunkMetadata],
                                              table_metadata: List[TableChunkMetadata],
                                              chunking_result: ChunkingResult) -> None:
        """
        åŸºäºchunkingç»“æœç²¾ç¡®åˆ†é…å›¾ç‰‡å’Œè¡¨æ ¼çš„ç« èŠ‚ID
        
        è¿™ä¸ªæ–¹æ³•é€šè¿‡è§£æfull_textä¸­çš„å›¾ç‰‡/è¡¨æ ¼å¼•ç”¨æ¥ç¡®å®šå®ƒä»¬æ‰€å±çš„ç« èŠ‚
        """
        # æ„å»ºè·¯å¾„åˆ°metadataçš„æ˜ å°„
        image_path_to_metadata = {img.image_path: img for img in image_metadata}
        table_path_to_metadata = {tbl.table_path: tbl for tbl in table_metadata}
        
        # éå†æ‰€æœ‰ç« èŠ‚ï¼ŒæŸ¥æ‰¾å›¾ç‰‡å’Œè¡¨æ ¼å¼•ç”¨
        for chapter in chunking_result.first_level_chapters:
            chapter_content = chapter.content
            chapter_id = chapter.chapter_id
            
            # æŸ¥æ‰¾å›¾ç‰‡å¼•ç”¨ï¼š[å›¾ç‰‡|ID:xxx|PATH:xxx: æè¿°]
            import re
            image_pattern = r'\[å›¾ç‰‡\|ID:(\d+)\|PATH:([^:]+):'
            image_matches = re.findall(image_pattern, chapter_content)
            
            for img_id, img_path in image_matches:
                # å¯»æ‰¾åŒ¹é…çš„å›¾ç‰‡metadata
                matching_image = None
                for img in image_metadata:
                    if img.image_path == img_path or img.image_path.endswith(img_path):
                        matching_image = img
                        break
                
                if matching_image:
                    matching_image.chapter_id = str(chapter_id)
                    print(f"ğŸ“ åˆ†é…å›¾ç‰‡ {img_id} ({os.path.basename(img_path)}) åˆ°ç« èŠ‚ {chapter_id}")
            
            # æŸ¥æ‰¾è¡¨æ ¼å¼•ç”¨ï¼š[è¡¨æ ¼|ID:xxx|PATH:xxx: æè¿°]
            table_pattern = r'\[è¡¨æ ¼\|ID:(\d+)\|PATH:([^:]+):'
            table_matches = re.findall(table_pattern, chapter_content)
            
            for tbl_id, tbl_path in table_matches:
                # å¯»æ‰¾åŒ¹é…çš„è¡¨æ ¼metadata
                matching_table = None
                for tbl in table_metadata:
                    if tbl.table_path == tbl_path or tbl.table_path.endswith(tbl_path):
                        matching_table = tbl
                        break
                
                if matching_table:
                    matching_table.chapter_id = str(chapter_id)
                    print(f"ğŸ“ åˆ†é…è¡¨æ ¼ {tbl_id} ({os.path.basename(tbl_path)}) åˆ°ç« èŠ‚ {chapter_id}")
        
        # ä¸ºæœªåˆ†é…ç« èŠ‚çš„å›¾ç‰‡/è¡¨æ ¼åˆ†é…é»˜è®¤ç« èŠ‚
        unassigned_images = [img for img in image_metadata if not img.chapter_id]
        unassigned_tables = [tbl for tbl in table_metadata if not tbl.chapter_id]
        
        if unassigned_images:
            print(f"âš ï¸ {len(unassigned_images)} ä¸ªå›¾ç‰‡æœªæ‰¾åˆ°ç« èŠ‚å½’å±ï¼Œåˆ†é…åˆ°é»˜è®¤ç« èŠ‚")
            for img in unassigned_images:
                img.chapter_id = "1"  # é»˜è®¤åˆ†é…åˆ°ç¬¬ä¸€ç« 
        
        if unassigned_tables:
            print(f"âš ï¸ {len(unassigned_tables)} ä¸ªè¡¨æ ¼æœªæ‰¾åˆ°ç« èŠ‚å½’å±ï¼Œåˆ†é…åˆ°é»˜è®¤ç« èŠ‚")
            for tbl in unassigned_tables:
                tbl.chapter_id = "1"  # é»˜è®¤åˆ†é…åˆ°ç¬¬ä¸€ç« 

    def _extract_text_chunk_metadata(self, chunking_result: ChunkingResult, 
                                   document_id: str) -> List[TextChunkMetadata]:
        """ä»chunkingç»“æœæå–æ–‡æœ¬chunk metadata"""
        text_chunk_metadata = []
        
        for chunk in chunking_result.minimal_chunks:
            # ä½¿ç”¨chunkè‡ªèº«çš„ç« èŠ‚å½’å±ä¿¡æ¯
            chapter_id = chunk.belongs_to_chapter
            
            text_chunk = TextChunkMetadata(
                content_id=create_content_id(document_id, "text_chunk", len(text_chunk_metadata) + 1),
                document_id=document_id,
                chapter_id=str(chapter_id),
                chunk_type=chunk.chunk_type,
                word_count=chunk.word_count,
                position_in_chapter=chunk.start_pos,  # ä½¿ç”¨start_posä½œä¸ºä½ç½®
                paragraph_index=0,  # MinimalChunkæ²¡æœ‰è¿™ä¸ªå­—æ®µï¼Œè®¾ä¸ºé»˜è®¤å€¼
                is_first_paragraph=False,  # MinimalChunkæ²¡æœ‰è¿™ä¸ªå­—æ®µï¼Œè®¾ä¸ºé»˜è®¤å€¼
                is_last_paragraph=False,   # MinimalChunkæ²¡æœ‰è¿™ä¸ªå­—æ®µï¼Œè®¾ä¸ºé»˜è®¤å€¼
                preceding_chunk_id=None,  # å¯ä»¥åç»­ä¼˜åŒ–
                following_chunk_id=None,  # å¯ä»¥åç»­ä¼˜åŒ–
                created_at=datetime.now()
            )
            text_chunk_metadata.append(text_chunk)
        
        return text_chunk_metadata

    def _find_chunk_chapter(self, chunk, chapters) -> str:
        """æ‰¾åˆ°chunkæ‰€å±çš„ç« èŠ‚"""
        # é€šè¿‡chunkçš„å†…å®¹åœ¨ç« èŠ‚ä¸­çš„ä½ç½®æ¥ç¡®å®šå½’å±
        chunk_content = chunk.content[:100]  # å–å‰100å­—ç¬¦ç”¨äºåŒ¹é…
        
        for chapter in chapters:
            if chunk_content in chapter.content:
                return chapter.chapter_id
        
        return "1"  # é»˜è®¤è¿”å›ç¬¬ä¸€ç« 
    
    def create_universal_metadata(self, content_id: str, document_id: str, 
                                content_type: str, content_level: str,
                                chapter_id: Optional[str] = None,
                                document_title: str = "",
                                chapter_title: str = "") -> Dict[str, Any]:
        """
        åˆ›å»ºé€šç”¨metadataå­—æ®µ
        
        Args:
            content_id: å†…å®¹ID
            document_id: æ–‡æ¡£ID
            content_type: å†…å®¹ç±»å‹
            content_level: å†…å®¹å±‚çº§
            chapter_id: ç« èŠ‚ID
            document_title: æ–‡æ¡£æ ‡é¢˜
            chapter_title: ç« èŠ‚æ ‡é¢˜
            
        Returns:
            DictåŒ…å«é€šç”¨metadataå­—æ®µ
        """
        return {
            "content_id": content_id,
            "document_id": document_id,
            "content_type": content_type,
            "content_level": content_level,
            "chapter_id": chapter_id or "",
            "document_title": document_title,
            "chapter_title": chapter_title,
            "document_scope": self.document_scope.value,
            "project_name": self.project_name,
            "created_at": datetime.now()
        }
    
    def _extract_image_metadata(self, page_split_data: Dict[str, Any], document_id: str) -> List[ImageChunkMetadata]:
        """æå–å›¾ç‰‡metadata"""
        image_metadata = []
        
        for page_num, page_data in enumerate(page_split_data.get("pages", []), 1):
            images = page_data.get("images", [])
            
            for img_idx, image in enumerate(images):
                # ä¿®å¤å­—æ®µååŒ¹é…é—®é¢˜
                metadata = image.get("metadata", {})
                image_chunk = ImageChunkMetadata(
                    content_id=create_content_id(document_id, "image_chunk", len(image_metadata) + 1),
                    document_id=document_id,
                    chapter_id=None,  # éœ€è¦æ ¹æ®é¡µé¢ä½ç½®ç¡®å®šç« èŠ‚
                    image_path=image.get("image_path", ""),  # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®å­—æ®µå
                    page_number=image.get("page_number", page_num),  # ä½¿ç”¨å›¾ç‰‡è‡ªå¸¦çš„é¡µç 
                    caption=image.get("caption", f"å›¾ç‰‡ {img_idx + 1}"),
                    width=metadata.get("width", 0),  # ä¿®å¤ï¼šä»metadataä¸­è·å–
                    height=metadata.get("height", 0),  # ä¿®å¤ï¼šä»metadataä¸­è·å–
                    size=metadata.get("size", 0),  # ä¿®å¤ï¼šä»metadataä¸­è·å–
                    aspect_ratio=metadata.get("aspect_ratio", 0.0),  # ä¿®å¤ï¼šä»metadataä¸­è·å–
                    ai_description=image.get("ai_description", ""),
                    page_context=image.get("page_context", page_data.get("cleaned_text", page_data.get("raw_text", ""))),
                    created_at=datetime.now()
                )
                image_metadata.append(image_chunk)
                
        return image_metadata
    
    def _extract_table_metadata(self, page_split_data: Dict[str, Any], document_id: str) -> List[TableChunkMetadata]:
        """æå–è¡¨æ ¼metadata"""
        table_metadata = []
        
        for page_num, page_data in enumerate(page_split_data.get("pages", []), 1):
            tables = page_data.get("tables", [])
            
            for tbl_idx, table in enumerate(tables):
                # ä¿®å¤å­—æ®µååŒ¹é…é—®é¢˜
                metadata = table.get("metadata", {})
                table_chunk = TableChunkMetadata(
                    content_id=create_content_id(document_id, "table_chunk", len(table_metadata) + 1),
                    document_id=document_id,
                    chapter_id=None,  # éœ€è¦æ ¹æ®é¡µé¢ä½ç½®ç¡®å®šç« èŠ‚
                    table_path=table.get("table_path", ""),  # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®å­—æ®µå
                    page_number=table.get("page_number", page_num),  # ä½¿ç”¨è¡¨æ ¼è‡ªå¸¦çš„é¡µç 
                    caption=table.get("caption", f"è¡¨æ ¼ {tbl_idx + 1}"),
                    width=metadata.get("width", 0),  # ä¿®å¤ï¼šä»metadataä¸­è·å–
                    height=metadata.get("height", 0),  # ä¿®å¤ï¼šä»metadataä¸­è·å–
                    size=metadata.get("size", 0),  # ä¿®å¤ï¼šä»metadataä¸­è·å–
                    aspect_ratio=metadata.get("aspect_ratio", 0.0),  # ä¿®å¤ï¼šä»metadataä¸­è·å–
                    ai_description=table.get("ai_description", ""),
                    page_context=table.get("page_context", page_data.get("cleaned_text", page_data.get("raw_text", ""))),
                    created_at=datetime.now()
                )
                table_metadata.append(table_chunk)
                
        return table_metadata
    
    def _generate_document_id(self, source_file: str) -> str:
        """ç”Ÿæˆæ–‡æ¡£ID"""
        # ä½¿ç”¨æ–‡ä»¶åçš„hashä½œä¸ºæ–‡æ¡£IDçš„ä¸€éƒ¨åˆ†
        file_name = Path(source_file).stem
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        return f"doc_{file_name}_{timestamp}"
    
    def map_chapters_to_content(self, image_metadata: List[ImageChunkMetadata],
                               table_metadata: List[TableChunkMetadata],
                               chapter_mapping: Dict[str, Any],
                               page_split_data: Dict[str, Any]) -> None:
        """
        æ ¹æ®é¡µé¢ä½ç½®å°†å›¾ç‰‡å’Œè¡¨æ ¼æ˜ å°„åˆ°ç« èŠ‚
        
        Args:
            image_metadata: å›¾ç‰‡å…ƒæ•°æ®åˆ—è¡¨
            table_metadata: è¡¨æ ¼å…ƒæ•°æ®åˆ—è¡¨
            chapter_mapping: ç« èŠ‚æ˜ å°„
            page_split_data: é¡µé¢åˆ†å‰²æ•°æ®
        """
        # æ”¹è¿›çš„ç« èŠ‚åˆ†é…ç­–ç•¥
        chapter_page_ranges = self._build_chapter_page_ranges(chapter_mapping, page_split_data)
        
        # ä¸ºå›¾ç‰‡åˆ†é…ç« èŠ‚
        for image in image_metadata:
            chapter_id = self._find_chapter_for_page(image.page_number, chapter_page_ranges)
            image.chapter_id = chapter_id
            
        # ä¸ºè¡¨æ ¼åˆ†é…ç« èŠ‚
        for table in table_metadata:
            chapter_id = self._find_chapter_for_page(table.page_number, chapter_page_ranges)
            table.chapter_id = chapter_id

    def _build_chapter_page_ranges(self, chapter_mapping: Dict[str, Any], 
                                  page_split_data: Dict[str, Any]) -> Dict[str, Dict[str, int]]:
        """
        æ„å»ºç« èŠ‚é¡µé¢èŒƒå›´æ˜ å°„
        
        Returns:
            Dict[chapter_id, {"start_page": int, "end_page": int}]
        """
        total_pages = len(page_split_data.get("pages", []))
        
        # å¦‚æœæ²¡æœ‰ç« èŠ‚ä¿¡æ¯ï¼Œè¿”å›å•ä¸€ç« èŠ‚è¦†ç›–æ‰€æœ‰é¡µé¢
        if not chapter_mapping:
            return {"1": {"start_page": 1, "end_page": total_pages}}
        
        # æ–¹æ³•1ï¼šå¦‚æœæœ‰TOCä¿¡æ¯ï¼Œå°è¯•ä»ä¸­æå–é¡µé¢èŒƒå›´
        if isinstance(chapter_mapping, dict) and "chapters" in chapter_mapping:
            chapters = chapter_mapping["chapters"]
            chapter_ranges = {}
            
            # å‡è®¾ç« èŠ‚æŒ‰é¡ºåºæ’åˆ—ï¼Œè®¡ç®—æ¯ä¸ªç« èŠ‚çš„é¡µé¢èŒƒå›´
            chapter_list = sorted(chapters.items(), key=lambda x: int(x[0]))
            pages_per_chapter = total_pages // len(chapter_list) if chapter_list else total_pages
            
            for i, (chapter_id, chapter_info) in enumerate(chapter_list):
                start_page = i * pages_per_chapter + 1
                end_page = min((i + 1) * pages_per_chapter, total_pages)
                
                # æœ€åä¸€ç« åŒ…å«æ‰€æœ‰å‰©ä½™é¡µé¢
                if i == len(chapter_list) - 1:
                    end_page = total_pages
                    
                chapter_ranges[str(chapter_id)] = {
                    "start_page": start_page,
                    "end_page": end_page
                }
            
            return chapter_ranges
        
        # æ–¹æ³•2ï¼šåŸºäºé¡µé¢å†…å®¹æ™ºèƒ½åˆ†æï¼ˆå¦‚æœæœ‰æ–‡æœ¬å†…å®¹ï¼‰
        chapter_ranges = self._analyze_content_based_chapters(page_split_data)
        if chapter_ranges:
            return chapter_ranges
        
        # æ–¹æ³•3ï¼šå›é€€åˆ°ç®€å•çš„å‡åˆ†ç­–ç•¥
        chapter_count = len(chapter_mapping) if isinstance(chapter_mapping, dict) else 6  # é»˜è®¤6ç« 
        pages_per_chapter = max(1, total_pages // chapter_count)
        
        chapter_ranges = {}
        for i in range(chapter_count):
            chapter_id = str(i + 1)
            start_page = i * pages_per_chapter + 1
            end_page = min((i + 1) * pages_per_chapter, total_pages)
            
            # æœ€åä¸€ç« åŒ…å«æ‰€æœ‰å‰©ä½™é¡µé¢
            if i == chapter_count - 1:
                end_page = total_pages
                
            chapter_ranges[chapter_id] = {
                "start_page": start_page,
                "end_page": end_page
            }
        
        return chapter_ranges

    def _analyze_content_based_chapters(self, page_split_data: Dict[str, Any]) -> Optional[Dict[str, Dict[str, int]]]:
        """
        åŸºäºé¡µé¢å†…å®¹åˆ†æç« èŠ‚åˆ†å¸ƒ
        
        é€šè¿‡è¯†åˆ«ç« èŠ‚æ ‡é¢˜ç­‰å…³é”®è¯æ¥ç¡®å®šç« èŠ‚è¾¹ç•Œ
        """
        import re
        
        pages = page_split_data.get("pages", [])
        chapter_pages = []
        
        # å®šä¹‰ç« èŠ‚æ ‡é¢˜çš„æ­£åˆ™æ¨¡å¼
        chapter_patterns = [
            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« ',  # ç¬¬Xç« 
            r'^ç« èŠ‚[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+',  # ç« èŠ‚X
            r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ã€\.ï¼]',  # æ•°å­—ç¼–å·
            r'^\d+\.',  # é˜¿æ‹‰ä¼¯æ•°å­—ç¼–å·
            r'^[IVX]+\.',  # ç½—é©¬æ•°å­—ç¼–å·
        ]
        
        for page_idx, page_data in enumerate(pages):
            page_text = page_data.get("cleaned_text") or page_data.get("raw_text", "")
            lines = page_text.split('\n')
            
            for line in lines[:5]:  # åªæ£€æŸ¥å‰5è¡Œ
                line = line.strip()
                if not line:
                    continue
                    
                # æ£€æŸ¥æ˜¯å¦åŒ¹é…ç« èŠ‚æ ‡é¢˜æ¨¡å¼
                for pattern in chapter_patterns:
                    if re.match(pattern, line):
                        chapter_pages.append({
                            "page": page_idx + 1,
                            "title": line[:50],  # ä¿ç•™å‰50ä¸ªå­—ç¬¦ä½œä¸ºæ ‡é¢˜
                            "chapter_num": len(chapter_pages) + 1
                        })
                        break
                        
                if len(chapter_pages) > 0 and chapter_pages[-1]["page"] == page_idx + 1:
                    break  # æ‰¾åˆ°ç« èŠ‚æ ‡é¢˜ååœæ­¢æ£€æŸ¥å½“å‰é¡µé¢
        
        # å¦‚æœæ‰¾åˆ°äº†ç« èŠ‚æ ‡é¢˜ï¼Œæ„å»ºé¡µé¢èŒƒå›´
        if len(chapter_pages) >= 2:  # è‡³å°‘è¦æœ‰2ä¸ªç« èŠ‚æ‰æœ‰æ„ä¹‰
            chapter_ranges = {}
            total_pages = len(pages)
            
            for i, chapter_info in enumerate(chapter_pages):
                chapter_id = str(chapter_info["chapter_num"])
                start_page = chapter_info["page"]
                
                # ç»“æŸé¡µé¢æ˜¯ä¸‹ä¸€ä¸ªç« èŠ‚çš„å‰ä¸€é¡µï¼Œæˆ–è€…æ˜¯æœ€åä¸€é¡µ
                if i + 1 < len(chapter_pages):
                    end_page = chapter_pages[i + 1]["page"] - 1
                else:
                    end_page = total_pages
                
                chapter_ranges[chapter_id] = {
                    "start_page": start_page,
                    "end_page": end_page
                }
            
            return chapter_ranges
        
        return None

    def _find_chapter_for_page(self, page_number: int, 
                              chapter_ranges: Dict[str, Dict[str, int]]) -> Optional[str]:
        """
        æ ¹æ®é¡µç æŸ¥æ‰¾å¯¹åº”çš„ç« èŠ‚ID
        """
        for chapter_id, page_range in chapter_ranges.items():
            if page_range["start_page"] <= page_number <= page_range["end_page"]:
                return chapter_id
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ç« èŠ‚ï¼Œè¿”å›ç¬¬ä¸€ä¸ªç« èŠ‚
        if chapter_ranges:
            return list(chapter_ranges.keys())[0]
        
        return "1"  # é»˜è®¤ç« èŠ‚
    
    def save_extracted_metadata(self, output_dir: str, **metadata_collections):
        """
        ä¿å­˜æå–çš„metadata
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            **metadata_collections: å„ç±»metadataé›†åˆ
        """
        os.makedirs(output_dir, exist_ok=True)
        
        for collection_name, collection_data in metadata_collections.items():
            if collection_data:
                output_file = os.path.join(output_dir, f"{collection_name}_metadata.json")
                
                # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
                if isinstance(collection_data, list):
                    serializable_data = [self._to_serializable(item) for item in collection_data]
                else:
                    serializable_data = self._to_serializable(collection_data)
                
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(serializable_data, f, ensure_ascii=False, indent=2, default=str)
                    
                print(f"ğŸ’¾ {collection_name} metadataå·²ä¿å­˜: {output_file}")
    
    def _to_serializable(self, obj) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸"""
        if hasattr(obj, '__dict__'):
            result = {}
            for key, value in obj.__dict__.items():
                if isinstance(value, datetime):
                    result[key] = value.isoformat()
                else:
                    result[key] = value
            return result
        return obj 