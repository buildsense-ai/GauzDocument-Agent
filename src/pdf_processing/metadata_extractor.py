#!/usr/bin/env python3
"""
Metadataæå–å™¨
å°†ç°æœ‰çš„å¤„ç†ç»“æœè½¬æ¢ä¸ºæ ‡å‡†åŒ–çš„metadataç»“æ„
"""

import json
import os
import uuid
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from .metadata_schema import (
    DocumentSummaryMetadata, ChapterSummaryMetadata, 
    TextChunkMetadata, ImageChunkMetadata, TableChunkMetadata,
    DocumentType, DocumentTOC, DocumentScope, ContentType,
    create_content_id, UNIVERSAL_METADATA_FIELDS
)
from .text_chunker import ChunkingResult


class MetadataExtractor:
    """ä»ç°æœ‰æ•°æ®ç»“æ„æå–æ ‡å‡†åŒ–metadata"""
    
    def __init__(self, project_name: str = "default_project"):
        """
        åˆå§‹åŒ–metadataæå–å™¨
        
        Args:
            project_name: é¡¹ç›®åç§°
        """
        self.project_name = project_name
        self.document_scope = DocumentScope.PROJECT
        
    def extract_from_page_split_result(self, page_split_file: str) -> Dict[str, Any]:
        """
        ä»page_splitç»“æœæå–åŸºç¡€metadata
        
        Args:
            page_split_file: page_splitç»“æœæ–‡ä»¶è·¯å¾„
            
        Returns:
            DictåŒ…å«æå–çš„åŸºç¡€ä¿¡æ¯
        """
        if not os.path.exists(page_split_file):
            raise FileNotFoundError(f"Page splitç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {page_split_file}")
            
        with open(page_split_file, 'r', encoding='utf-8') as f:
            page_split_data = json.load(f)
            
        # æå–æ–‡æ¡£åŸºç¡€ä¿¡æ¯
        document_info = {
            "document_id": self._generate_document_id(page_split_data.get("source_file", "unknown")),
            "source_file_path": page_split_data.get("source_file", ""),
            "file_name": Path(page_split_data.get("source_file", "")).name,
            "total_pages": len(page_split_data.get("pages", [])),
            "processing_time": page_split_data.get("processing_time", 0.0),
            "created_at": datetime.now()
        }
        
        # æå–å›¾ç‰‡metadata
        image_metadata = self._extract_image_metadata(page_split_data, document_info["document_id"])
        
        # æå–è¡¨æ ¼metadata
        table_metadata = self._extract_table_metadata(page_split_data, document_info["document_id"])
        
        return {
            "document_info": document_info,
            "image_metadata": image_metadata,
            "table_metadata": table_metadata,
            "page_split_data": page_split_data
        }
    
    def extract_from_toc_result(self, toc_file: str, document_id: str) -> Tuple[DocumentTOC, Dict[str, Any]]:
        """
        ä»TOCç»“æœæå–TOC metadata
        
        Args:
            toc_file: TOCç»“æœæ–‡ä»¶è·¯å¾„
            document_id: æ–‡æ¡£ID
            
        Returns:
            Tuple[DocumentTOC, Dict[str, Any]]: TOCå…ƒæ•°æ®å’Œç« èŠ‚æ˜ å°„
        """
        if not os.path.exists(toc_file):
            raise FileNotFoundError(f"TOCç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {toc_file}")
            
        with open(toc_file, 'r', encoding='utf-8') as f:
            toc_data = json.load(f)
            
        # åˆ›å»ºDocumentTOC
        toc_items = toc_data.get("toc_items", [])
        doc_toc = DocumentTOC(
            document_id=document_id,
            toc_json=json.dumps(toc_items, ensure_ascii=False),
            chapter_count=len([item for item in toc_items if item.get("level") == 1]),
            total_sections=len(toc_items),
            created_at=datetime.now()
        )
        
        # åˆ›å»ºç« èŠ‚æ˜ å°„ï¼ˆID -> æ ‡é¢˜ï¼‰
        chapter_mapping = {}
        for item in toc_items:
            if item.get("level") == 1:  # åªå¤„ç†ç¬¬ä¸€çº§ç« èŠ‚
                chapter_mapping[item["id"]] = {
                    "title": item["title"],
                    "order": int(item["id"]) if item["id"].isdigit() else 0
                }
        
        return doc_toc, chapter_mapping
    
    def extract_from_chunking_result(self, chunking_result: ChunkingResult, 
                                   document_id: str, chapter_mapping: Dict[str, Any]) -> List[TextChunkMetadata]:
        """
        ä»åˆ†å—ç»“æœæå–æ–‡æœ¬chunk metadata
        
        Args:
            chunking_result: åˆ†å—ç»“æœ
            document_id: æ–‡æ¡£ID
            chapter_mapping: ç« èŠ‚æ˜ å°„
            
        Returns:
            List[TextChunkMetadata]: æ–‡æœ¬chunkå…ƒæ•°æ®åˆ—è¡¨
        """
        text_chunks = []
        
        for chunk in chunking_result.minimal_chunks:
            # è·å–ç« èŠ‚ä¿¡æ¯
            chapter_info = chapter_mapping.get(chunk.belongs_to_chapter, {})
            
            # åˆ›å»ºTextChunkMetadata
            text_chunk = TextChunkMetadata(
                content_id=create_content_id(document_id, "text_chunk", len(text_chunks) + 1),
                document_id=document_id,
                chapter_id=chunk.belongs_to_chapter,
                chunk_type=chunk.chunk_type,
                word_count=chunk.word_count,
                position_in_chapter=chunk.start_pos,
                paragraph_index=0,  # éœ€è¦ä»ai_chunkerä¸­è·å–æ›´è¯¦ç»†ä¿¡æ¯
                is_first_paragraph=False,  # éœ€è¦ä»ai_chunkerä¸­è·å–æ›´è¯¦ç»†ä¿¡æ¯
                is_last_paragraph=False,   # éœ€è¦ä»ai_chunkerä¸­è·å–æ›´è¯¦ç»†ä¿¡æ¯
                preceding_chunk_id=None,   # éœ€è¦å»ºç«‹é“¾æ¥å…³ç³»
                following_chunk_id=None,   # éœ€è¦å»ºç«‹é“¾æ¥å…³ç³»
                created_at=datetime.now()
            )
            
            text_chunks.append(text_chunk)
            
        # å»ºç«‹å‰åé“¾æ¥å…³ç³»
        for i, chunk in enumerate(text_chunks):
            if i > 0:
                chunk.preceding_chunk_id = text_chunks[i-1].content_id
            if i < len(text_chunks) - 1:
                chunk.following_chunk_id = text_chunks[i+1].content_id
                
        return text_chunks
    
    def create_universal_metadata(self, content_id: str, document_id: str, 
                                content_type: str, content_level: str,
                                chapter_id: Optional[str] = None,
                                document_title: str = "",
                                chapter_title: str = "") -> Dict[str, Any]:
        """
        åˆ›å»ºé€šç”¨metadataå­—æ®µ
        
        Args:
            content_id: å†…å®¹ID
            document_id: æ–‡æ¡£ID
            content_type: å†…å®¹ç±»å‹
            content_level: å†…å®¹å±‚çº§
            chapter_id: ç« èŠ‚ID
            document_title: æ–‡æ¡£æ ‡é¢˜
            chapter_title: ç« èŠ‚æ ‡é¢˜
            
        Returns:
            DictåŒ…å«é€šç”¨metadataå­—æ®µ
        """
        return {
            "content_id": content_id,
            "document_id": document_id,
            "content_type": content_type,
            "content_level": content_level,
            "chapter_id": chapter_id or "",
            "document_title": document_title,
            "chapter_title": chapter_title,
            "document_scope": self.document_scope.value,
            "project_name": self.project_name,
            "created_at": datetime.now()
        }
    
    def _extract_image_metadata(self, page_split_data: Dict[str, Any], document_id: str) -> List[ImageChunkMetadata]:
        """æå–å›¾ç‰‡metadata"""
        image_metadata = []
        
        for page_num, page_data in enumerate(page_split_data.get("pages", []), 1):
            images = page_data.get("images", [])
            
            for img_idx, image in enumerate(images):
                image_chunk = ImageChunkMetadata(
                    content_id=create_content_id(document_id, "image_chunk", len(image_metadata) + 1),
                    document_id=document_id,
                    chapter_id=None,  # éœ€è¦æ ¹æ®é¡µé¢ä½ç½®ç¡®å®šç« èŠ‚
                    image_path=image.get("path", ""),
                    page_number=page_num,
                    caption=image.get("caption", f"å›¾ç‰‡ {img_idx + 1}"),
                    width=image.get("width", 0),
                    height=image.get("height", 0),
                    size=image.get("size", 0),
                    aspect_ratio=image.get("aspect_ratio", 0.0),
                    ai_description=image.get("ai_description", ""),
                    page_context=page_data.get("text", ""),
                    created_at=datetime.now()
                )
                image_metadata.append(image_chunk)
                
        return image_metadata
    
    def _extract_table_metadata(self, page_split_data: Dict[str, Any], document_id: str) -> List[TableChunkMetadata]:
        """æå–è¡¨æ ¼metadata"""
        table_metadata = []
        
        for page_num, page_data in enumerate(page_split_data.get("pages", []), 1):
            tables = page_data.get("tables", [])
            
            for tbl_idx, table in enumerate(tables):
                table_chunk = TableChunkMetadata(
                    content_id=create_content_id(document_id, "table_chunk", len(table_metadata) + 1),
                    document_id=document_id,
                    chapter_id=None,  # éœ€è¦æ ¹æ®é¡µé¢ä½ç½®ç¡®å®šç« èŠ‚
                    table_path=table.get("path", ""),
                    page_number=page_num,
                    caption=table.get("caption", f"è¡¨æ ¼ {tbl_idx + 1}"),
                    width=table.get("width", 0),
                    height=table.get("height", 0),
                    size=table.get("size", 0),
                    aspect_ratio=table.get("aspect_ratio", 0.0),
                    ai_description=table.get("ai_description", ""),
                    page_context=page_data.get("text", ""),
                    created_at=datetime.now()
                )
                table_metadata.append(table_chunk)
                
        return table_metadata
    
    def _generate_document_id(self, source_file: str) -> str:
        """ç”Ÿæˆæ–‡æ¡£ID"""
        # ä½¿ç”¨æ–‡ä»¶åçš„hashä½œä¸ºæ–‡æ¡£IDçš„ä¸€éƒ¨åˆ†
        file_name = Path(source_file).stem
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        return f"doc_{file_name}_{timestamp}"
    
    def map_chapters_to_content(self, image_metadata: List[ImageChunkMetadata],
                               table_metadata: List[TableChunkMetadata],
                               chapter_mapping: Dict[str, Any],
                               page_split_data: Dict[str, Any]) -> None:
        """
        æ ¹æ®é¡µé¢ä½ç½®å°†å›¾ç‰‡å’Œè¡¨æ ¼æ˜ å°„åˆ°ç« èŠ‚
        
        Args:
            image_metadata: å›¾ç‰‡å…ƒæ•°æ®åˆ—è¡¨
            table_metadata: è¡¨æ ¼å…ƒæ•°æ®åˆ—è¡¨
            chapter_mapping: ç« èŠ‚æ˜ å°„
            page_split_data: é¡µé¢åˆ†å‰²æ•°æ®
        """
        # è¿™é‡Œéœ€è¦å®ç°ä¸€ä¸ªç®€å•çš„é¡µé¢åˆ°ç« èŠ‚çš„æ˜ å°„é€»è¾‘
        # å¯ä»¥æ ¹æ®é¡µé¢æ•°é‡å’Œç« èŠ‚æ•°é‡è¿›è¡Œä¼°ç®—
        total_pages = len(page_split_data.get("pages", []))
        chapter_count = len(chapter_mapping)
        
        if chapter_count > 0:
            pages_per_chapter = total_pages // chapter_count
            
            # ä¸ºå›¾ç‰‡åˆ†é…ç« èŠ‚
            for image in image_metadata:
                estimated_chapter = min((image.page_number - 1) // pages_per_chapter + 1, chapter_count)
                image.chapter_id = str(estimated_chapter)
                
            # ä¸ºè¡¨æ ¼åˆ†é…ç« èŠ‚
            for table in table_metadata:
                estimated_chapter = min((table.page_number - 1) // pages_per_chapter + 1, chapter_count)
                table.chapter_id = str(estimated_chapter)
    
    def save_extracted_metadata(self, output_dir: str, **metadata_collections):
        """
        ä¿å­˜æå–çš„metadata
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            **metadata_collections: å„ç±»metadataé›†åˆ
        """
        os.makedirs(output_dir, exist_ok=True)
        
        for collection_name, collection_data in metadata_collections.items():
            if collection_data:
                output_file = os.path.join(output_dir, f"{collection_name}_metadata.json")
                
                # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
                if isinstance(collection_data, list):
                    serializable_data = [self._to_serializable(item) for item in collection_data]
                else:
                    serializable_data = self._to_serializable(collection_data)
                
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(serializable_data, f, ensure_ascii=False, indent=2, default=str)
                    
                print(f"ğŸ’¾ {collection_name} metadataå·²ä¿å­˜: {output_file}")
    
    def _to_serializable(self, obj) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸"""
        if hasattr(obj, '__dict__'):
            result = {}
            for key, value in obj.__dict__.items():
                if isinstance(value, datetime):
                    result[key] = value.isoformat()
                else:
                    result[key] = value
            return result
        return obj 