#!/usr/bin/env python3
"""
é—®é¢˜ç”Ÿæˆå™¨ï¼ˆå¯é€‰ç»„ä»¶ï¼‰
åŸºäºæ–‡æœ¬chunkç”Ÿæˆæ´¾ç”Ÿé—®é¢˜ï¼Œæé«˜æ£€ç´¢å¤šæ ·æ€§
"""

import json
import os
import time
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
import sys
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from .metadata_schema import DerivedQuestionMetadata, create_content_id
from .text_chunker import ChunkingResult, MinimalChunk
from src.qwen_client import QwenClient


class QuestionGenerator:
    """é—®é¢˜ç”Ÿæˆå™¨"""
    
    def __init__(self, model: str = "qwen-turbo"):
        """
        åˆå§‹åŒ–é—®é¢˜ç”Ÿæˆå™¨
        
        Args:
            model: ä½¿ç”¨çš„AIæ¨¡å‹ï¼Œé»˜è®¤ä½¿ç”¨è½»é‡æ¨¡å‹
        """
        self.client = QwenClient(model=model, temperature=0.7)  # è¾ƒé«˜æ¸©åº¦å¢åŠ å¤šæ ·æ€§
        self.model = model
        
    def generate_questions_from_chunks(self,
                                     document_id: str,
                                     chunking_result: ChunkingResult,
                                     chapter_mapping: Dict[str, Any],
                                     questions_per_chunk: int = 2,
                                     parallel_processing: bool = True) -> List[Tuple[DerivedQuestionMetadata, str]]:
        """
        ä»æ–‡æœ¬chunksç”Ÿæˆæ´¾ç”Ÿé—®é¢˜
        
        Args:
            document_id: æ–‡æ¡£ID
            chunking_result: åˆ†å—ç»“æœ
            chapter_mapping: ç« èŠ‚æ˜ å°„
            questions_per_chunk: æ¯ä¸ªchunkç”Ÿæˆçš„é—®é¢˜æ•°é‡
            parallel_processing: æ˜¯å¦å¹¶è¡Œå¤„ç†
            
        Returns:
            List[Tuple[DerivedQuestionMetadata, str]]: é—®é¢˜å…ƒæ•°æ®å’Œé—®é¢˜å†…å®¹çš„åˆ—è¡¨
        """
        
        # è¿‡æ»¤å‡ºé€‚åˆç”Ÿæˆé—®é¢˜çš„chunks
        suitable_chunks = self._filter_suitable_chunks(chunking_result.minimal_chunks)
        
        print(f"ğŸ“ æ‰¾åˆ° {len(suitable_chunks)} ä¸ªé€‚åˆç”Ÿæˆé—®é¢˜çš„chunks")
        
        if not suitable_chunks:
            return []
        
        if parallel_processing:
            return self._generate_questions_parallel(
                document_id, suitable_chunks, chapter_mapping, questions_per_chunk
            )
        else:
            return self._generate_questions_sequential(
                document_id, suitable_chunks, chapter_mapping, questions_per_chunk
            )
    
    def _filter_suitable_chunks(self, chunks: List[MinimalChunk]) -> List[MinimalChunk]:
        """è¿‡æ»¤å‡ºé€‚åˆç”Ÿæˆé—®é¢˜çš„chunks"""
        
        suitable_chunks = []
        
        for chunk in chunks:
            # è¿‡æ»¤æ¡ä»¶ï¼š
            # 1. æ–‡æœ¬é•¿åº¦é€‚ä¸­ï¼ˆä¸å¤ªçŸ­ä¹Ÿä¸å¤ªé•¿ï¼‰
            # 2. æ˜¯æ®µè½ç±»å‹ï¼ˆä¸æ˜¯æ ‡é¢˜æˆ–åˆ—è¡¨é¡¹ï¼‰
            # 3. åŒ…å«è¶³å¤Ÿçš„ä¿¡æ¯å†…å®¹
            
            if (chunk.chunk_type == "paragraph" and 
                50 <= chunk.word_count <= 300 and
                self._has_substantial_content(chunk.content)):
                suitable_chunks.append(chunk)
        
        return suitable_chunks
    
    def _has_substantial_content(self, content: str) -> bool:
        """æ£€æŸ¥chunkæ˜¯å¦åŒ…å«è¶³å¤Ÿçš„å®è´¨å†…å®¹"""
        
        # ç®€å•çš„å†…å®¹è´¨é‡æ£€æŸ¥
        if not content or len(content.strip()) < 100:
            return False
            
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æŠ€æœ¯æœ¯è¯­æˆ–ä¸“ä¸šè¯æ±‡
        technical_keywords = ["æŠ€æœ¯", "æ–¹æ³•", "è®¾è®¡", "åˆ†æ", "è¯„ä¼°", "å®æ–½", "æ ‡å‡†", "è§„èŒƒ", "è¦æ±‚"]
        
        content_lower = content.lower()
        keyword_count = sum(1 for keyword in technical_keywords if keyword in content_lower)
        
        return keyword_count >= 1
    
    def _generate_questions_parallel(self,
                                   document_id: str,
                                   chunks: List[MinimalChunk],
                                   chapter_mapping: Dict[str, Any],
                                   questions_per_chunk: int) -> List[Tuple[DerivedQuestionMetadata, str]]:
        """å¹¶è¡Œç”Ÿæˆé—®é¢˜"""
        
        all_questions = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=8) as executor:
            # æäº¤ä»»åŠ¡
            future_to_chunk = {
                executor.submit(
                    self._generate_questions_for_chunk,
                    document_id, chunk, chapter_mapping, questions_per_chunk
                ): chunk for chunk in chunks
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_chunk):
                chunk = future_to_chunk[future]
                try:
                    questions = future.result()
                    all_questions.extend(questions)
                    print(f"âœ… ä¸ºchunkç”Ÿæˆé—®é¢˜å®Œæˆ: {len(questions)}ä¸ªé—®é¢˜")
                except Exception as e:
                    print(f"âŒ ä¸ºchunkç”Ÿæˆé—®é¢˜å¤±è´¥: {e}")
        
        print(f"ğŸ“Š æ€»å…±ç”Ÿæˆ {len(all_questions)} ä¸ªé—®é¢˜")
        return all_questions
    
    def _generate_questions_sequential(self,
                                     document_id: str,
                                     chunks: List[MinimalChunk],
                                     chapter_mapping: Dict[str, Any],
                                     questions_per_chunk: int) -> List[Tuple[DerivedQuestionMetadata, str]]:
        """é¡ºåºç”Ÿæˆé—®é¢˜"""
        
        all_questions = []
        
        for chunk in chunks:
            try:
                questions = self._generate_questions_for_chunk(
                    document_id, chunk, chapter_mapping, questions_per_chunk
                )
                all_questions.extend(questions)
                print(f"âœ… ä¸ºchunkç”Ÿæˆé—®é¢˜å®Œæˆ: {len(questions)}ä¸ªé—®é¢˜")
            except Exception as e:
                print(f"âŒ ä¸ºchunkç”Ÿæˆé—®é¢˜å¤±è´¥: {e}")
        
        print(f"ğŸ“Š æ€»å…±ç”Ÿæˆ {len(all_questions)} ä¸ªé—®é¢˜")
        return all_questions
    
    def _generate_questions_for_chunk(self,
                                    document_id: str,
                                    chunk: MinimalChunk,
                                    chapter_mapping: Dict[str, Any],
                                    questions_per_chunk: int) -> List[Tuple[DerivedQuestionMetadata, str]]:
        """ä¸ºå•ä¸ªchunkç”Ÿæˆé—®é¢˜"""
        
        chapter_info = chapter_mapping.get(chunk.belongs_to_chapter, {})
        
        # ç”Ÿæˆé—®é¢˜
        questions = self._generate_questions_content(chunk, chapter_info, questions_per_chunk)
        
        # åˆ›å»ºå…ƒæ•°æ®
        result = []
        for i, question in enumerate(questions):
            content_id = create_content_id(document_id, "derived_question", 
                                         int(f"{chunk.belongs_to_chapter}{i+1:03d}"))
            
            # æ¨æ–­é—®é¢˜åˆ†ç±»
            category = self._infer_question_category(question, chapter_info)
            
            question_metadata = DerivedQuestionMetadata(
                content_id=content_id,
                document_id=document_id,
                chapter_id=chunk.belongs_to_chapter,
                question_category=category,
                generated_from_chunk_id=chunk.chunk_id,
                created_at=datetime.now()
            )
            
            result.append((question_metadata, question))
        
        return result
    
    def _generate_questions_content(self,
                                  chunk: MinimalChunk,
                                  chapter_info: Dict[str, Any],
                                  questions_per_chunk: int) -> List[str]:
        """ç”Ÿæˆé—®é¢˜å†…å®¹"""
        
        # æ„å»ºæç¤ºè¯
        prompt = self._build_question_generation_prompt(chunk, chapter_info, questions_per_chunk)
        
        try:
            # è°ƒç”¨AIç”Ÿæˆé—®é¢˜
            response = self.client.generate_response(prompt)
            
            # è§£æç”Ÿæˆçš„é—®é¢˜
            questions = self._parse_generated_questions(response)
            
            return questions[:questions_per_chunk]  # é™åˆ¶é—®é¢˜æ•°é‡
            
        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆé—®é¢˜æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            # è¿”å›åŸºç¡€é—®é¢˜
            return self._generate_basic_questions(chunk, chapter_info)
    
    def _build_question_generation_prompt(self,
                                        chunk: MinimalChunk,
                                        chapter_info: Dict[str, Any],
                                        questions_per_chunk: int) -> str:
        """æ„å»ºé—®é¢˜ç”Ÿæˆæç¤ºè¯"""
        
        chapter_title = chapter_info.get("title", "æœªçŸ¥ç« èŠ‚")
        
        prompt = f"""
åŸºäºä»¥ä¸‹æ–‡æœ¬å†…å®¹ï¼Œç”Ÿæˆ{questions_per_chunk}ä¸ªç›¸å…³çš„é—®é¢˜ã€‚è¿™äº›é—®é¢˜åº”è¯¥èƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·æ›´å¥½åœ°ç†è§£å’Œæ£€ç´¢æ–‡æ¡£å†…å®¹ã€‚

## ç« èŠ‚ä¿¡æ¯
- ç« èŠ‚æ ‡é¢˜: {chapter_title}
- å†…å®¹ç±»å‹: {chunk.chunk_type}

## æ–‡æœ¬å†…å®¹
{chunk.content}

## è¦æ±‚
1. ç”Ÿæˆ{questions_per_chunk}ä¸ªä¸åŒç±»å‹çš„é—®é¢˜
2. é—®é¢˜åº”è¯¥æ¶µç›–ï¼š
   - æ ¸å¿ƒæ¦‚å¿µé—®é¢˜ï¼ˆä»€ä¹ˆæ˜¯...ï¼Ÿï¼‰
   - æ–¹æ³•é—®é¢˜ï¼ˆå¦‚ä½•...ï¼Ÿï¼‰
   - åŸå› é—®é¢˜ï¼ˆä¸ºä»€ä¹ˆ...ï¼Ÿï¼‰
   - åº”ç”¨é—®é¢˜ï¼ˆåœ¨ä»€ä¹ˆæƒ…å†µä¸‹...ï¼Ÿï¼‰
3. é—®é¢˜åº”è¯¥å…·ä½“ã€æ¸…æ™°ã€å®ç”¨
4. æ¯ä¸ªé—®é¢˜ç‹¬å ä¸€è¡Œï¼Œä»¥"Q: "å¼€å¤´

ç¤ºä¾‹æ ¼å¼ï¼š
Q: ä»€ä¹ˆæ˜¯...ï¼Ÿ
Q: å¦‚ä½•...ï¼Ÿ
"""
        
        return prompt
    
    def _parse_generated_questions(self, response: str) -> List[str]:
        """è§£æç”Ÿæˆçš„é—®é¢˜"""
        
        questions = []
        
        for line in response.split('\n'):
            line = line.strip()
            if line.startswith('Q: '):
                question = line[3:].strip()  # ç§»é™¤"Q: "å‰ç¼€
                if question and question.endswith('ï¼Ÿ'):
                    questions.append(question)
        
        return questions
    
    def _generate_basic_questions(self, chunk: MinimalChunk, chapter_info: Dict[str, Any]) -> List[str]:
        """ç”ŸæˆåŸºç¡€é—®é¢˜ï¼ˆå½“AIç”Ÿæˆå¤±è´¥æ—¶ä½¿ç”¨ï¼‰"""
        
        chapter_title = chapter_info.get("title", "ç›¸å…³å†…å®¹")
        
        # æå–å…³é”®è¯
        keywords = self._extract_keywords(chunk.content)
        
        basic_questions = []
        
        if keywords:
            basic_questions.append(f"ä»€ä¹ˆæ˜¯{keywords[0]}ï¼Ÿ")
            if len(keywords) > 1:
                basic_questions.append(f"å¦‚ä½•ç†è§£{keywords[1]}ï¼Ÿ")
        
        basic_questions.append(f"{chapter_title}åŒ…å«å“ªäº›ä¸»è¦å†…å®¹ï¼Ÿ")
        
        return basic_questions[:2]  # æœ€å¤šè¿”å›2ä¸ªåŸºç¡€é—®é¢˜
    
    def _extract_keywords(self, content: str) -> List[str]:
        """æå–å…³é”®è¯"""
        
        # ç®€å•çš„å…³é”®è¯æå–ï¼ˆå¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–ï¼‰
        common_words = {"çš„", "æ˜¯", "åœ¨", "å’Œ", "ä¸", "æˆ–", "ä½†", "è€Œ", "ä¹Ÿ", "éƒ½", "æœ‰", "ä¸º", "äº†", "ç­‰", "åŠ"}
        
        words = content.replace("ï¼Œ", " ").replace("ã€‚", " ").replace("ï¼›", " ").split()
        
        keywords = []
        for word in words:
            if len(word) >= 2 and word not in common_words:
                keywords.append(word)
        
        return keywords[:5]  # è¿”å›å‰5ä¸ªå…³é”®è¯
    
    def _infer_question_category(self, question: str, chapter_info: Dict[str, Any]) -> str:
        """æ¨æ–­é—®é¢˜åˆ†ç±»"""
        
        chapter_title = chapter_info.get("title", "").lower()
        question_lower = question.lower()
        
        # åŸºäºé—®é¢˜å†…å®¹å’Œç« èŠ‚æ ‡é¢˜æ¨æ–­åˆ†ç±»
        if "ä»€ä¹ˆ" in question_lower or "å®šä¹‰" in question_lower:
            return "æ¦‚å¿µå®šä¹‰"
        elif "å¦‚ä½•" in question_lower or "æ€æ ·" in question_lower:
            return "æ–¹æ³•æµç¨‹"
        elif "ä¸ºä»€ä¹ˆ" in question_lower or "åŸå› " in question_lower:
            return "åŸå› åˆ†æ"
        elif "è®¾è®¡" in chapter_title or "æ–¹æ¡ˆ" in chapter_title:
            return "è®¾è®¡æ–¹æ¡ˆ"
        elif "è¯„ä¼°" in chapter_title or "åˆ†æ" in chapter_title:
            return "è¯„ä¼°åˆ†æ"
        elif "æ ‡å‡†" in chapter_title or "è§„èŒƒ" in chapter_title:
            return "æ ‡å‡†è§„èŒƒ"
        else:
            return "é€šç”¨å†…å®¹"
    
    def save_derived_questions(self, 
                             questions: List[Tuple[DerivedQuestionMetadata, str]], 
                             output_dir: str):
        """
        ä¿å­˜æ´¾ç”Ÿé—®é¢˜
        
        Args:
            questions: é—®é¢˜åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
        """
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜é—®é¢˜å…ƒæ•°æ®
        all_metadata = []
        all_content = {}
        
        for question_metadata, question_content in questions:
            # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
            metadata_dict = {
                "content_id": question_metadata.content_id,
                "document_id": question_metadata.document_id,
                "chapter_id": question_metadata.chapter_id,
                "question_category": question_metadata.question_category,
                "generated_from_chunk_id": question_metadata.generated_from_chunk_id,
                "created_at": question_metadata.created_at.isoformat()
            }
            
            all_metadata.append(metadata_dict)
            all_content[question_metadata.content_id] = question_content
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata_file = os.path.join(output_dir, "derived_questions_metadata.json")
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(all_metadata, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜é—®é¢˜å†…å®¹
        content_file = os.path.join(output_dir, "derived_questions_content.json")
        with open(content_file, 'w', encoding='utf-8') as f:
            json.dump(all_content, f, ensure_ascii=False, indent=2)
        
        # æŒ‰åˆ†ç±»ç»Ÿè®¡
        category_stats = {}
        for metadata, _ in questions:
            category = metadata.question_category
            category_stats[category] = category_stats.get(category, 0) + 1
        
        print(f"â“ æ´¾ç”Ÿé—®é¢˜å·²ä¿å­˜:")
        print(f"   - å…ƒæ•°æ®: {metadata_file}")
        print(f"   - å†…å®¹: {content_file}")
        print(f"   - æ€»é—®é¢˜æ•°: {len(questions)}")
        print(f"   - åˆ†ç±»ç»Ÿè®¡: {category_stats}")
    
    def generate_questions_by_category(self, 
                                     document_id: str,
                                     chunking_result: ChunkingResult,
                                     chapter_mapping: Dict[str, Any],
                                     target_categories: List[str]) -> List[Tuple[DerivedQuestionMetadata, str]]:
        """
        æŒ‰æŒ‡å®šåˆ†ç±»ç”Ÿæˆé—®é¢˜
        
        Args:
            document_id: æ–‡æ¡£ID
            chunking_result: åˆ†å—ç»“æœ
            chapter_mapping: ç« èŠ‚æ˜ å°„
            target_categories: ç›®æ ‡åˆ†ç±»åˆ—è¡¨
            
        Returns:
            List[Tuple[DerivedQuestionMetadata, str]]: é—®é¢˜åˆ—è¡¨
        """
        
        # æ ¹æ®ç« èŠ‚æ ‡é¢˜ç­›é€‰ç›¸å…³chunks
        relevant_chunks = []
        
        for chunk in chunking_result.minimal_chunks:
            chapter_info = chapter_mapping.get(chunk.belongs_to_chapter, {})
            chapter_title = chapter_info.get("title", "").lower()
            
            # æ£€æŸ¥ç« èŠ‚æ˜¯å¦å±äºç›®æ ‡åˆ†ç±»
            if any(category in chapter_title for category in target_categories):
                relevant_chunks.append(chunk)
        
        print(f"ğŸ“‹ æ‰¾åˆ° {len(relevant_chunks)} ä¸ªä¸ç›®æ ‡åˆ†ç±»ç›¸å…³çš„chunks")
        
        if not relevant_chunks:
            return []
        
        # ç”Ÿæˆé—®é¢˜
        return self._generate_questions_parallel(
            document_id, relevant_chunks, chapter_mapping, 3  # æ¯ä¸ªchunkç”Ÿæˆ3ä¸ªé—®é¢˜
        ) 