#!/usr/bin/env python3
"""
ç« èŠ‚æ‘˜è¦ç”Ÿæˆå™¨
ä¸ºæ¯ä¸ªç« èŠ‚ç”Ÿæˆè¯¦ç»†çš„æ‘˜è¦ä¿¡æ¯
"""

import json
import os
import time
from datetime import datetime
from typing import List, Dict, Any, Optional
from pathlib import Path
import sys
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from .metadata_schema import ChapterSummaryMetadata, create_content_id
from .text_chunker import ChunkingResult, ChapterContent
from src.qwen_client import QwenClient


class ChapterSummaryGenerator:
    """ç« èŠ‚æ‘˜è¦ç”Ÿæˆå™¨"""
    
    def __init__(self, model: str = "qwen-plus"):
        """
        åˆå§‹åŒ–ç« èŠ‚æ‘˜è¦ç”Ÿæˆå™¨
        
        Args:
            model: ä½¿ç”¨çš„AIæ¨¡å‹
        """
        self.client = QwenClient(model=model, temperature=0.3)
        self.model = model
        
    def generate_chapter_summaries(self, 
                                 document_id: str,
                                 chunking_result: ChunkingResult,
                                 toc_data: Dict[str, Any],
                                 image_metadata: Optional[List[Dict[str, Any]]] = None,
                                 table_metadata: Optional[List[Dict[str, Any]]] = None,
                                 parallel_processing: bool = True) -> List[tuple[ChapterSummaryMetadata, str]]:
        """
        ç”Ÿæˆæ‰€æœ‰ç« èŠ‚çš„æ‘˜è¦
        
        Args:
            document_id: æ–‡æ¡£ID
            chunking_result: åˆ†å—ç»“æœ
            toc_data: TOCæ•°æ®
            image_metadata: å›¾ç‰‡å…ƒæ•°æ®
            table_metadata: è¡¨æ ¼å…ƒæ•°æ®
            parallel_processing: æ˜¯å¦å¹¶è¡Œå¤„ç†
            
        Returns:
            List[tuple[ChapterSummaryMetadata, str]]: ç« èŠ‚æ‘˜è¦å…ƒæ•°æ®å’Œå†…å®¹çš„åˆ—è¡¨
        """
        if not chunking_result.first_level_chapters:
            return []
            
        # å‡†å¤‡ç« èŠ‚æ˜ å°„
        chapter_mapping = self._create_chapter_mapping(toc_data)
        
        # ç»Ÿè®¡æ¯ä¸ªç« èŠ‚çš„å›¾ç‰‡å’Œè¡¨æ ¼æ•°é‡
        chapter_media_stats = self._count_chapter_media(
            chunking_result.first_level_chapters,
            image_metadata or [],
            table_metadata or []
        )
        
        if parallel_processing:
            return self._generate_summaries_parallel(
                document_id, chunking_result.first_level_chapters,
                chapter_mapping, chapter_media_stats
            )
        else:
            return self._generate_summaries_sequential(
                document_id, chunking_result.first_level_chapters,
                chapter_mapping, chapter_media_stats
            )
    
    def _generate_summaries_parallel(self, 
                                   document_id: str,
                                   chapters: List[ChapterContent],
                                   chapter_mapping: Dict[str, Any],
                                   chapter_media_stats: Dict[str, Dict[str, int]]) -> List[tuple[ChapterSummaryMetadata, str]]:
        """å¹¶è¡Œç”Ÿæˆç« èŠ‚æ‘˜è¦"""
        
        results = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=5) as executor:
            # æäº¤ä»»åŠ¡
            future_to_chapter = {
                executor.submit(
                    self._generate_single_chapter_summary,
                    document_id, chapter, chapter_mapping, chapter_media_stats
                ): chapter for chapter in chapters
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_chapter):
                chapter = future_to_chapter[future]
                try:
                    result = future.result()
                    results.append(result)
                    print(f"âœ… ç« èŠ‚æ‘˜è¦ç”Ÿæˆå®Œæˆ: {chapter.title}")
                except Exception as e:
                    print(f"âŒ ç« èŠ‚æ‘˜è¦ç”Ÿæˆå¤±è´¥: {chapter.title}, é”™è¯¯: {e}")
                    # åˆ›å»ºåŸºç¡€æ‘˜è¦
                    basic_summary = self._create_basic_chapter_summary(
                        document_id, chapter, chapter_mapping, chapter_media_stats
                    )
                    results.append(basic_summary)
        
        # æŒ‰ç« èŠ‚é¡ºåºæ’åº
        results.sort(key=lambda x: x[0].chapter_order)
        
        return results
    
    def _generate_summaries_sequential(self,
                                     document_id: str,
                                     chapters: List[ChapterContent],
                                     chapter_mapping: Dict[str, Any],
                                     chapter_media_stats: Dict[str, Dict[str, int]]) -> List[tuple[ChapterSummaryMetadata, str]]:
        """é¡ºåºç”Ÿæˆç« èŠ‚æ‘˜è¦"""
        
        results = []
        
        for chapter in chapters:
            try:
                result = self._generate_single_chapter_summary(
                    document_id, chapter, chapter_mapping, chapter_media_stats
                )
                results.append(result)
                print(f"âœ… ç« èŠ‚æ‘˜è¦ç”Ÿæˆå®Œæˆ: {chapter.title}")
            except Exception as e:
                print(f"âŒ ç« èŠ‚æ‘˜è¦ç”Ÿæˆå¤±è´¥: {chapter.title}, é”™è¯¯: {e}")
                # åˆ›å»ºåŸºç¡€æ‘˜è¦
                basic_summary = self._create_basic_chapter_summary(
                    document_id, chapter, chapter_mapping, chapter_media_stats
                )
                results.append(basic_summary)
        
        return results
    
    def _generate_single_chapter_summary(self,
                                       document_id: str,
                                       chapter: ChapterContent,
                                       chapter_mapping: Dict[str, Any],
                                       chapter_media_stats: Dict[str, Dict[str, int]]) -> tuple[ChapterSummaryMetadata, str]:
        """ç”Ÿæˆå•ä¸ªç« èŠ‚çš„æ‘˜è¦"""
        
        start_time = time.time()
        
        # è·å–ç« èŠ‚ä¿¡æ¯
        chapter_info = chapter_mapping.get(chapter.chapter_id, {})
        media_stats = chapter_media_stats.get(chapter.chapter_id, {"image_count": 0, "table_count": 0})
        
        # ç»Ÿè®¡ç« èŠ‚ä¿¡æ¯
        paragraph_count = self._count_paragraphs(chapter.content)
        text_chunk_count = self._count_text_chunks_in_chapter(chapter.chapter_id, chapter_mapping)
        
        # ç”Ÿæˆæ‘˜è¦å†…å®¹
        summary_content = self._generate_chapter_summary_content(chapter, chapter_info, media_stats)
        
        # åˆ›å»ºå…ƒæ•°æ®
        processing_time = time.time() - start_time
        content_id = create_content_id(document_id, "chapter_summary", int(chapter.chapter_id))
        
        chapter_summary = ChapterSummaryMetadata(
            content_id=content_id,
            document_id=document_id,
            chapter_id=chapter.chapter_id,
            toc_id=chapter.chapter_id,  # å‡è®¾toc_idå’Œchapter_idç›¸åŒ
            chapter_order=int(chapter.chapter_id) if chapter.chapter_id.isdigit() else 0,
            word_count=chapter.word_count,
            paragraph_count=paragraph_count,
            text_chunk_count=text_chunk_count,
            image_count=media_stats["image_count"],
            table_count=media_stats["table_count"],
            created_at=datetime.now()
        )
        
        return chapter_summary, summary_content
    
    def _create_basic_chapter_summary(self,
                                    document_id: str,
                                    chapter: ChapterContent,
                                    chapter_mapping: Dict[str, Any],
                                    chapter_media_stats: Dict[str, Dict[str, int]]) -> tuple[ChapterSummaryMetadata, str]:
        """åˆ›å»ºåŸºç¡€ç« èŠ‚æ‘˜è¦ï¼ˆå½“AIç”Ÿæˆå¤±è´¥æ—¶ä½¿ç”¨ï¼‰"""
        
        chapter_info = chapter_mapping.get(chapter.chapter_id, {})
        media_stats = chapter_media_stats.get(chapter.chapter_id, {"image_count": 0, "table_count": 0})
        
        # ç»Ÿè®¡ä¿¡æ¯
        paragraph_count = self._count_paragraphs(chapter.content)
        text_chunk_count = self._count_text_chunks_in_chapter(chapter.chapter_id, chapter_mapping)
        
        # åŸºç¡€æ‘˜è¦å†…å®¹
        summary_content = f"""
{chapter.title}

æœ¬ç« èŠ‚åŒ…å«{chapter.word_count}å­—çš„å†…å®¹ï¼Œå…±{paragraph_count}ä¸ªæ®µè½ã€‚
ç« èŠ‚å†…å®¹æ¶µç›–äº†{chapter.title}ç›¸å…³çš„ä¸»è¦ä¿¡æ¯å’ŒæŠ€æœ¯è¦ç‚¹ã€‚
{"" if media_stats["image_count"] == 0 else f"åŒ…å«{media_stats['image_count']}å¼ å›¾ç‰‡"}
{"" if media_stats["table_count"] == 0 else f"åŒ…å«{media_stats['table_count']}ä¸ªè¡¨æ ¼"}
"""
        
        # åˆ›å»ºå…ƒæ•°æ®
        content_id = create_content_id(document_id, "chapter_summary", int(chapter.chapter_id))
        
        chapter_summary = ChapterSummaryMetadata(
            content_id=content_id,
            document_id=document_id,
            chapter_id=chapter.chapter_id,
            toc_id=chapter.chapter_id,
            chapter_order=int(chapter.chapter_id) if chapter.chapter_id.isdigit() else 0,
            word_count=chapter.word_count,
            paragraph_count=paragraph_count,
            text_chunk_count=text_chunk_count,
            image_count=media_stats["image_count"],
            table_count=media_stats["table_count"],
            created_at=datetime.now()
        )
        
        return chapter_summary, summary_content.strip()
    
    def _generate_chapter_summary_content(self,
                                        chapter: ChapterContent,
                                        chapter_info: Dict[str, Any],
                                        media_stats: Dict[str, int]) -> str:
        """ç”Ÿæˆç« èŠ‚æ‘˜è¦å†…å®¹"""
        
        # æ„å»ºæç¤ºè¯
        prompt = self._build_chapter_summary_prompt(chapter, chapter_info, media_stats)
        
        try:
            # è°ƒç”¨AIç”Ÿæˆæ‘˜è¦
            summary_content = self.client.generate_response(prompt)
            
            # åå¤„ç†æ‘˜è¦å†…å®¹
            summary_content = self._post_process_summary(summary_content)
            
            return summary_content
            
        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆç« èŠ‚æ‘˜è¦æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            # è¿”å›åŸºç¡€æ‘˜è¦
            return self._generate_basic_chapter_content(chapter, media_stats)
    
    def _build_chapter_summary_prompt(self,
                                    chapter: ChapterContent,
                                    chapter_info: Dict[str, Any],
                                    media_stats: Dict[str, int]) -> str:
        """æ„å»ºç« èŠ‚æ‘˜è¦ç”Ÿæˆæç¤ºè¯"""
        
        # æˆªå–ç« èŠ‚å†…å®¹ï¼ˆé¿å…è¿‡é•¿ï¼‰
        content_preview = chapter.content[:1000] + "..." if len(chapter.content) > 1000 else chapter.content
        
        media_info = []
        if media_stats["image_count"] > 0:
            media_info.append(f"{media_stats['image_count']}å¼ å›¾ç‰‡")
        if media_stats["table_count"] > 0:
            media_info.append(f"{media_stats['table_count']}ä¸ªè¡¨æ ¼")
        
        media_text = f"ï¼ŒåŒ…å«{' and '.join(media_info)}" if media_info else ""
        
        prompt = f"""
è¯·ä¸ºä»¥ä¸‹ç« èŠ‚ç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„æ‘˜è¦ï¼š

## ç« èŠ‚ä¿¡æ¯
- ç« èŠ‚æ ‡é¢˜: {chapter.title}
- å­—æ•°: {chapter.word_count}å­—
- åª’ä½“å†…å®¹: {media_text}

## ç« èŠ‚å†…å®¹
{content_preview}

## è¦æ±‚
è¯·ç”Ÿæˆä¸€ä¸ª150-200å­—çš„ç« èŠ‚æ‘˜è¦ï¼ŒåŒ…å«ä»¥ä¸‹å†…å®¹ï¼š
1. ç« èŠ‚çš„ä¸»è¦ä¸»é¢˜å’Œç›®æ ‡
2. æ ¸å¿ƒå†…å®¹è¦ç‚¹ï¼ˆ3-5ä¸ªå…³é”®ç‚¹ï¼‰
3. é‡è¦çš„æŠ€æœ¯ç»†èŠ‚æˆ–æ–¹æ³•
4. ç« èŠ‚åœ¨æ•´ä¸ªæ–‡æ¡£ä¸­çš„ä½œç”¨

è¯·ç”¨ä¸“ä¸šã€ç®€æ´çš„è¯­è¨€ï¼Œç¡®ä¿æ‘˜è¦å‡†ç¡®åæ˜ ç« èŠ‚çš„æ ¸å¿ƒå†…å®¹ã€‚
"""
        
        return prompt
    
    def _post_process_summary(self, summary_content: str) -> str:
        """åå¤„ç†æ‘˜è¦å†…å®¹"""
        # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
        lines = [line.strip() for line in summary_content.split('\n') if line.strip()]
        return '\n'.join(lines)
    
    def _generate_basic_chapter_content(self, chapter: ChapterContent, media_stats: Dict[str, int]) -> str:
        """ç”ŸæˆåŸºç¡€ç« èŠ‚å†…å®¹"""
        media_info = []
        if media_stats["image_count"] > 0:
            media_info.append(f"{media_stats['image_count']}å¼ å›¾ç‰‡")
        if media_stats["table_count"] > 0:
            media_info.append(f"{media_stats['table_count']}ä¸ªè¡¨æ ¼")
        
        media_text = f"ï¼ŒåŒ…å«{' and '.join(media_info)}" if media_info else ""
        
        return f"""
{chapter.title}

æœ¬ç« èŠ‚åŒ…å«{chapter.word_count}å­—çš„è¯¦ç»†å†…å®¹{media_text}ã€‚
ç« èŠ‚å†…å®¹æ¶µç›–äº†{chapter.title}ç›¸å…³çš„ä¸»è¦ä¿¡æ¯å’ŒæŠ€æœ¯è¦ç‚¹ï¼Œ
ä¸ºé¡¹ç›®çš„å®æ–½æä¾›äº†é‡è¦çš„æŒ‡å¯¼å’Œå‚è€ƒã€‚
"""
    
    def _create_chapter_mapping(self, toc_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºç« èŠ‚æ˜ å°„"""
        chapter_mapping = {}
        
        for item in toc_data.get("toc_items", []):
            if item.get("level") == 1:  # åªå¤„ç†ç¬¬ä¸€çº§ç« èŠ‚
                chapter_mapping[item["id"]] = {
                    "title": item["title"],
                    "order": int(item["id"]) if item["id"].isdigit() else 0
                }
        
        return chapter_mapping
    
    def _count_chapter_media(self,
                           chapters: List[ChapterContent],
                           image_metadata: List[Dict[str, Any]],
                           table_metadata: List[Dict[str, Any]]) -> Dict[str, Dict[str, int]]:
        """ç»Ÿè®¡æ¯ä¸ªç« èŠ‚çš„å›¾ç‰‡å’Œè¡¨æ ¼æ•°é‡"""
        
        chapter_media_stats = {}
        
        for chapter in chapters:
            chapter_id = chapter.chapter_id
            
            # ç»Ÿè®¡å›¾ç‰‡æ•°é‡
            image_count = sum(1 for img in image_metadata if img.get("chapter_id") == chapter_id)
            
            # ç»Ÿè®¡è¡¨æ ¼æ•°é‡
            table_count = sum(1 for tbl in table_metadata if tbl.get("chapter_id") == chapter_id)
            
            chapter_media_stats[chapter_id] = {
                "image_count": image_count,
                "table_count": table_count
            }
        
        return chapter_media_stats
    
    def _count_paragraphs(self, content: str) -> int:
        """ç»Ÿè®¡æ®µè½æ•°é‡"""
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
        return len(paragraphs)
    
    def _count_text_chunks_in_chapter(self, chapter_id: str, chapter_mapping: Dict[str, Any]) -> int:
        """ç»Ÿè®¡ç« èŠ‚ä¸­çš„æ–‡æœ¬å—æ•°é‡"""
        # è¿™é‡Œæš‚æ—¶è¿”å›ä¼°ç®—å€¼ï¼Œå®é™…åº”è¯¥ä»chunking_resultä¸­è·å–
        return 10  # é»˜è®¤å€¼
    
    def save_chapter_summaries(self, 
                             chapter_summaries: List[tuple[ChapterSummaryMetadata, str]], 
                             output_dir: str):
        """
        ä¿å­˜ç« èŠ‚æ‘˜è¦
        
        Args:
            chapter_summaries: ç« èŠ‚æ‘˜è¦åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
        """
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜æ‰€æœ‰ç« èŠ‚æ‘˜è¦å…ƒæ•°æ®
        all_metadata = []
        all_content = {}
        
        for chapter_summary, summary_content in chapter_summaries:
            # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
            metadata_dict = {
                "content_id": chapter_summary.content_id,
                "document_id": chapter_summary.document_id,
                "chapter_id": chapter_summary.chapter_id,
                "toc_id": chapter_summary.toc_id,
                "chapter_order": chapter_summary.chapter_order,
                "word_count": chapter_summary.word_count,
                "paragraph_count": chapter_summary.paragraph_count,
                "text_chunk_count": chapter_summary.text_chunk_count,
                "image_count": chapter_summary.image_count,
                "table_count": chapter_summary.table_count,
                "created_at": chapter_summary.created_at.isoformat()
            }
            
            all_metadata.append(metadata_dict)
            all_content[chapter_summary.chapter_id] = summary_content
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata_file = os.path.join(output_dir, "chapter_summaries_metadata.json")
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(all_metadata, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æ‘˜è¦å†…å®¹
        content_file = os.path.join(output_dir, "chapter_summaries_content.json")
        with open(content_file, 'w', encoding='utf-8') as f:
            json.dump(all_content, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“š ç« èŠ‚æ‘˜è¦å·²ä¿å­˜:")
        print(f"   - å…ƒæ•°æ®: {metadata_file}")
        print(f"   - å†…å®¹: {content_file}")
        print(f"   - ç« èŠ‚æ•°é‡: {len(chapter_summaries)}") 