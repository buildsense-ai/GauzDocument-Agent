#!/usr/bin/env python3
"""
TOCæå–å™¨ - åŸºäºç”Ÿæˆå¼AIçš„ç›®å½•ç»“æ„æå–
ä½¿ç”¨qwen_plusæ¨ç†æ¨¡å¼æå–æ–‡æ¡£ç›®å½•ç»“æ„
"""

import json
import os
import sys
import time
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from openai import OpenAI
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

load_dotenv()

@dataclass
class TOCItem:
    """TOCæ¡ç›®"""
    title: str
    level: int
    start_text: str  # ç”¨äºåŒ¹é…çš„å¼€å¤´æ–‡æœ¬
    page_number: Optional[int] = None
    parent_id: Optional[str] = None
    id: Optional[str] = None

class TOCExtractor:
    """TOCæå–å™¨"""
    
    def __init__(self, api_key: Optional[str] = None, base_url: Optional[str] = None):
        """
        åˆå§‹åŒ–TOCæå–å™¨
        
        Args:
            api_key: Qwen APIå¯†é’¥
            base_url: APIåŸºç¡€URL
        """
        self.api_key = api_key or os.getenv("QWEN_API_KEY")
        self.base_url = base_url or os.getenv("QWEN_BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1")
        
        if not self.api_key:
            raise ValueError("Qwen API key is required. Set QWEN_API_KEY environment variable.")
        
        # åˆå§‹åŒ–OpenAIå…¼å®¹å®¢æˆ·ç«¯
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
            timeout=120  # å¢åŠ è¶…æ—¶æ—¶é—´
        )
        
        print("âœ… TOCæå–å™¨åˆå§‹åŒ–æˆåŠŸ")
        print(f"ğŸ¯ æ¨¡å‹: qwen-plus")
        print(f"ğŸ”— åŸºç¡€URL: {self.base_url}")
    
    def stitch_full_text(self, basic_result_path: str) -> str:
        """
        ç¼åˆå®Œæ•´æ–‡æœ¬
        
        Args:
            basic_result_path: basic_processing_result.jsonæ–‡ä»¶è·¯å¾„
            
        Returns:
            str: ç¼åˆåçš„å®Œæ•´æ–‡æœ¬
        """
        print("ğŸ§µ å¼€å§‹ç¼åˆå®Œæ•´æ–‡æœ¬...")
        
        # è¯»å–åŸºç¡€å¤„ç†ç»“æœ
        with open(basic_result_path, 'r', encoding='utf-8') as f:
            basic_result = json.load(f)
        
        pages = basic_result.get('pages', [])
        
        # æŒ‰é¡µç æ’åº
        pages.sort(key=lambda x: x.get('page_number', 0))
        
        full_text_parts = []
        
        for page in pages:
            page_num = page.get('page_number', 0)
            cleaned_text = page.get('cleaned_text', '')
            images = page.get('images', [])
            tables = page.get('tables', [])
            
            # æ·»åŠ é¡µç æ ‡è®°
            full_text_parts.append(f"\n===== ç¬¬ {page_num} é¡µ =====\n")
            
            # æ·»åŠ é¡µé¢æ–‡æœ¬
            if cleaned_text.strip():
                full_text_parts.append(cleaned_text.strip())
            
            # æ·»åŠ å›¾ç‰‡ä¿¡æ¯
            for i, image in enumerate(images, 1):
                description = image.get('ai_description', 'å›¾ç‰‡æè¿°')
                full_text_parts.append(f"\n[å›¾ç‰‡ {page_num}-{i}: {description}]\n")
            
            # æ·»åŠ è¡¨æ ¼ä¿¡æ¯
            for i, table in enumerate(tables, 1):
                description = table.get('ai_description', 'è¡¨æ ¼æè¿°')
                full_text_parts.append(f"\n[è¡¨æ ¼ {page_num}-{i}: {description}]\n")
        
        full_text = "\n".join(full_text_parts)
        
        print(f"âœ… æ–‡æœ¬ç¼åˆå®Œæˆï¼Œæ€»é•¿åº¦: {len(full_text)} å­—ç¬¦")
        print(f"ğŸ“„ æ€»é¡µæ•°: {len(pages)}")
        
        return full_text
    
    def extract_toc_with_reasoning(self, full_text: str) -> List[TOCItem]:
        """
        ä½¿ç”¨æ¨ç†æ¨¡å¼æå–TOC
        
        Args:
            full_text: å®Œæ•´æ–‡æ¡£æ–‡æœ¬
            
        Returns:
            List[TOCItem]: TOCé¡¹ç›®åˆ—è¡¨
        """
        print("ğŸ§  å¼€å§‹ä½¿ç”¨æ¨ç†æ¨¡å¼æå–TOC...")
        
        # æ„å»ºç³»ç»Ÿæç¤º
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£ç»“æ„åˆ†æå¸ˆã€‚è¯·åˆ†æç»™å®šçš„æ–‡æ¡£æ–‡æœ¬ï¼Œæå–å®Œæ•´çš„ç›®å½•ç»“æ„(TOC)ã€‚

è¦æ±‚ï¼š
1. è¯†åˆ«æ‰€æœ‰å±‚çº§çš„ç« èŠ‚æ ‡é¢˜ï¼ˆä¸€çº§ã€äºŒçº§ã€ä¸‰çº§ç­‰ï¼‰
2. ä¸ºæ¯ä¸ªç« èŠ‚æä¾›ç”¨äºåŒ¹é…çš„å¼€å¤´æ–‡æœ¬ç‰‡æ®µï¼ˆ15-30ä¸ªå­—ç¬¦ï¼‰
3. ä¼°ç®—æ¯ä¸ªç« èŠ‚çš„é¡µç ä½ç½®
4. ä¿æŒç« èŠ‚çš„å±‚çº§å…³ç³»

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{
  "toc": [
    {
      "title": "ç« èŠ‚æ ‡é¢˜",
      "level": 1,
      "start_text": "ç« èŠ‚å¼€å¤´çš„æ–‡æœ¬ç‰‡æ®µ",
      "page_number": 1,
      "id": "chapter_1"
    }
  ]
}

æ³¨æ„ï¼š
- levelä»1å¼€å§‹ï¼ˆ1=ä¸€çº§æ ‡é¢˜ï¼Œ2=äºŒçº§æ ‡é¢˜ï¼Œä»¥æ­¤ç±»æ¨ï¼‰
- start_textåº”è¯¥æ˜¯ç« èŠ‚æ ‡é¢˜åç´§æ¥ç€çš„æ–‡æœ¬ï¼Œç”¨äºç²¾ç¡®åŒ¹é…
- page_numberæ ¹æ®"===== ç¬¬ X é¡µ ====="æ ‡è®°æ¥ä¼°ç®—
- idä½¿ç”¨chapter_1, chapter_2ç­‰æ ¼å¼
"""
        
        # æ„å»ºç”¨æˆ·æç¤º
        user_prompt = f"""è¯·åˆ†æä»¥ä¸‹æ–‡æ¡£æ–‡æœ¬ï¼Œæå–å®Œæ•´çš„ç›®å½•ç»“æ„ï¼š

{full_text[:20000]}  # é™åˆ¶æ–‡æœ¬é•¿åº¦é¿å…è¶…è¿‡tokené™åˆ¶

è¯·ä½¿ç”¨æ¨ç†æ¨¡å¼ä»”ç»†åˆ†ææ–‡æ¡£ç»“æ„ï¼Œè¯†åˆ«æ‰€æœ‰ç« èŠ‚æ ‡é¢˜å’Œå±‚çº§å…³ç³»ã€‚
"""
        
        try:
            # è°ƒç”¨qwen-plusæ¨ç†æ¨¡å¼
            response = self.client.chat.completions.create(
                model="qwen-plus",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=4000,
                temperature=0.1,
                # å¯ç”¨æ¨ç†æ¨¡å¼
                extra_body={
                    "enable_reasoning": True,
                    "reasoning_effort": "high"
                }
            )
            
            # æå–å“åº”å†…å®¹
            content = response.choices[0].message.content
            
            # å¤„ç†```jsonæ ¼å¼çš„å“åº”
            if content.startswith('```json'):
                # æå–JSONéƒ¨åˆ†
                json_start = content.find('{')
                json_end = content.rfind('}') + 1
                if json_start != -1 and json_end != -1:
                    content = content[json_start:json_end]
            
            # å°è¯•è§£æJSON
            try:
                result = json.loads(content)
                toc_data = result.get('toc', [])
                
                # è½¬æ¢ä¸ºTOCItemå¯¹è±¡
                toc_items = []
                for item in toc_data:
                    toc_item = TOCItem(
                        title=item.get('title', ''),
                        level=item.get('level', 1),
                        start_text=item.get('start_text', ''),
                        page_number=item.get('page_number'),
                        id=item.get('id')
                    )
                    toc_items.append(toc_item)
                
                print(f"âœ… TOCæå–æˆåŠŸï¼Œå…±è¯†åˆ« {len(toc_items)} ä¸ªç« èŠ‚")
                
                # æ˜¾ç¤ºæå–ç»“æœ
                for item in toc_items:
                    indent = "  " * (item.level - 1)
                    print(f"{indent}{item.level}. {item.title} (é¡µç : {item.page_number})")
                
                return toc_items
                
            except json.JSONDecodeError as e:
                print(f"âŒ JSONè§£æå¤±è´¥: {e}")
                print(f"ğŸ” åŸå§‹å“åº”: {content}")
                return []
                
        except Exception as e:
            print(f"âŒ TOCæå–å¤±è´¥: {e}")
            return []
    
    def save_toc_result(self, toc_items: List[TOCItem], output_path: str):
        """
        ä¿å­˜TOCæå–ç»“æœ
        
        Args:
            toc_items: TOCé¡¹ç›®åˆ—è¡¨
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
        toc_data = []
        for item in toc_items:
            toc_data.append({
                "title": item.title,
                "level": item.level,
                "start_text": item.start_text,
                "page_number": item.page_number,
                "id": item.id,
                "parent_id": item.parent_id
            })
        
        result = {
            "toc": toc_data,
            "extraction_time": time.time(),
            "total_chapters": len(toc_items)
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ TOCç»“æœå·²ä¿å­˜åˆ°: {output_path}")

def main():
    """ä¸»å‡½æ•°"""
    # è¾“å…¥æ–‡ä»¶è·¯å¾„
    basic_result_path = "parser_output/20250714_145102_zpdlfg/basic_processing_result.json"
    
    if not os.path.exists(basic_result_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {basic_result_path}")
        return
    
    # åˆå§‹åŒ–TOCæå–å™¨
    extractor = TOCExtractor()
    
    # ç¼åˆå®Œæ•´æ–‡æœ¬
    full_text = extractor.stitch_full_text(basic_result_path)
    
    # ä¿å­˜ç¼åˆåçš„æ–‡æœ¬ç”¨äºè°ƒè¯•
    debug_path = "parser_output/20250714_145102_zpdlfg/full_text_debug.txt"
    with open(debug_path, 'w', encoding='utf-8') as f:
        f.write(full_text)
    print(f"ğŸ› è°ƒè¯•æ–‡æœ¬å·²ä¿å­˜åˆ°: {debug_path}")
    
    # æå–TOC
    toc_items = extractor.extract_toc_with_reasoning(full_text)
    
    # ä¿å­˜TOCç»“æœ
    if toc_items:
        output_path = "parser_output/20250714_145102_zpdlfg/toc_extraction_result.json"
        extractor.save_toc_result(toc_items, output_path)
        print(f"ğŸ‰ TOCæå–å®Œæˆ! ç»“æœä¿å­˜åœ¨: {output_path}")
    else:
        print("âŒ TOCæå–å¤±è´¥ï¼Œæœªèƒ½è¯†åˆ«ç« èŠ‚ç»“æ„")

if __name__ == "__main__":
    main() 