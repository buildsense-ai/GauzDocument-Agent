#!/usr/bin/env python3
"""
PDF Parser Tool - é‡æ„ç‰ˆæœ¬
æ•´åˆæ‰€æœ‰PDFå¤„ç†ç»„ä»¶ï¼Œæä¾›ç»Ÿä¸€çš„å·¥å…·æ¥å£
"""

import json
import logging
import time
from typing import Dict, List, Any, Optional, Union, Tuple
from pathlib import Path
import os

from .config import PDFProcessingConfig
from .pdf_document_parser import PDFDocumentParser
from .media_extractor import MediaExtractor
from .ai_content_reorganizer import AIContentReorganizer
from .toc_extractor import TOCExtractor
from .ai_chunker import AIChunker
from .text_chunker import TextChunker
from .data_models import ProcessingResult, PageData, ImageWithContext, TableWithContext
try:
    from ..base_tool import Tool
except ImportError:
    from base_tool import Tool

logger = logging.getLogger(__name__)


class PDFParserTool(Tool):
    """
    PDFè§£æå·¥å…· - é‡æ„ç‰ˆæœ¬
    
    æ”¯æŒä¸¤ç§å¤„ç†æ¨¡å¼ï¼š
    1. åŸºç¡€æ¨¡å¼ï¼šé¡µé¢çº§è§£æã€åª’ä½“æå–ã€AIå†…å®¹é‡ç»„
    2. é«˜çº§æ¨¡å¼ï¼šåŒ…å«æ–‡æ¡£ç»“æ„åˆ†æã€æ™ºèƒ½åˆ†å—ã€å…ƒæ•°æ®å¢å¼º
    """
    
    def __init__(self, config: PDFProcessingConfig = None):
        super().__init__(
            name="pdf_parser",
            description="ğŸš€ PDFè§£æå·¥å…· - æ™ºèƒ½æå–PDFä¸­çš„æ–‡æœ¬ã€å›¾ç‰‡ã€è¡¨æ ¼ï¼Œæ”¯æŒç»“æ„åŒ–åˆ†æå’Œæ™ºèƒ½åˆ†å—"
        )
        
        self.config = config or PDFProcessingConfig()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._init_components()
        
        logger.info("âœ… PDFè§£æå·¥å…·åˆå§‹åŒ–å®Œæˆ")
    
    def _init_components(self):
        """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""
        try:
            # åŸºç¡€å¤„ç†ç»„ä»¶
            self.document_parser = PDFDocumentParser(self.config)
            self.media_extractor = MediaExtractor(
                parallel_processing=self.config.media_extractor.parallel_processing,
                max_workers=self.config.media_extractor.max_workers
            )
            self.ai_reorganizer = AIContentReorganizer(self.config)
            
            # TOCå’Œåˆ†å—ç»„ä»¶
            self.toc_extractor = TOCExtractor()
            self.ai_chunker = AIChunker()
            self.text_chunker = TextChunker()
            
            # Metadataå¤„ç†ç»„ä»¶
            from .metadata_extractor import MetadataExtractor
            from .document_summary_generator import DocumentSummaryGenerator
            from .chapter_summary_generator import ChapterSummaryGenerator
            from .question_generator import QuestionGenerator
            
            project_name = getattr(self.config, 'project_name', "default_project")
            self.metadata_extractor = MetadataExtractor(project_name)
            self.document_summary_generator = DocumentSummaryGenerator()
            self.chapter_summary_generator = ChapterSummaryGenerator()
            self.question_generator = QuestionGenerator()
            
        except Exception as e:
            logger.error(f"ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def execute(self, **kwargs) -> str:
        """
        æ‰§è¡ŒPDFè§£æ
        
        Args:
            action: æ“ä½œç±»å‹
                - "parse_basic": åŸºç¡€è§£ææ¨¡å¼ï¼ˆä½¿ç”¨ç°æœ‰é€»è¾‘ï¼‰
                - "parse_page_split": é¡µé¢åˆ†å‰²è§£ææ¨¡å¼ï¼ˆæ–°å¢ï¼Œè§£å†³é¡µé¢æ ‡æ³¨æ¼‚ç§»ï¼‰
                - "parse_advanced": é«˜çº§è§£ææ¨¡å¼ï¼ˆåŒ…å«ç»“æ„åˆ†æï¼‰
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
            enable_ai_enhancement: æ˜¯å¦å¯ç”¨AIå¢å¼ºå¤„ç†
            parallel_processing: æ˜¯å¦å¯ç”¨å¹¶è¡Œå¤„ç†
            
        Returns:
            str: è§£æç»“æœçš„JSONå­—ç¬¦ä¸²
        """
        action = kwargs.get("action", "parse_basic")
        
        if action == "parse_basic":
            return self._parse_basic(**kwargs)
        elif action == "parse_page_split":
            return self._parse_page_split(**kwargs)
        elif action == "parse_advanced":
            return json.dumps({
                "status": "error",
                "message": "é«˜çº§è§£ææ¨¡å¼æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ä½¿ç”¨ parse_basic æˆ– parse_page_split"
            }, indent=2, ensure_ascii=False)
        else:
            return json.dumps({
                "status": "error",
                "message": f"ä¸æ”¯æŒçš„æ“ä½œ: {action}ã€‚æ”¯æŒçš„æ“ä½œ: parse_basic, parse_page_split"
            }, indent=2, ensure_ascii=False)
    
    def _parse_basic(self, pdf_path: str, output_dir: str = None, enable_ai_enhancement: bool = True) -> str:
        """
        åŸºç¡€è§£ææ¨¡å¼
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            enable_ai_enhancement: æ˜¯å¦å¯ç”¨AIå¢å¼º
            
        Returns:
            str: JSONæ ¼å¼çš„å¤„ç†ç»“æœ
        """
        start_time = time.time()
        
        logger.info(f"ğŸš€ å¼€å§‹åŸºç¡€è§£æ: {pdf_path}")
        
        try:
            # éªŒè¯è¾“å…¥
            if not Path(pdf_path).exists():
                return json.dumps({
                    "status": "error",
                    "message": f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}"
                }, ensure_ascii=False)
            
            # è®¾ç½®è¾“å‡ºç›®å½•
            if output_dir is None:
                from .config import create_output_directory
                output_dir = create_output_directory(self.config)
            
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            
            # ç¬¬ä¸€æ­¥ï¼šè§£æPDFæ–‡æ¡£
            logger.info("ğŸ“„ ç¬¬ä¸€æ­¥ï¼šè§£æPDFæ–‡æ¡£")
            raw_result, page_texts = self.document_parser.parse_pdf(pdf_path)
            
            # ç¬¬äºŒæ­¥ï¼šæå–åª’ä½“æ–‡ä»¶
            logger.info("ğŸ–¼ï¸ ç¬¬äºŒæ­¥ï¼šæå–åª’ä½“æ–‡ä»¶")
            pages = self.media_extractor.extract_media_from_pages(
                raw_result=raw_result,
                page_texts=page_texts,
                output_dir=str(output_path)
            )
            
            # ä¿å­˜åª’ä½“ä¿¡æ¯
            self.media_extractor.save_media_info(pages, str(output_path))
            
            # æ„å»ºProcessingResultå¯¹è±¡
            from .data_models import ProcessingResult
            media_result = ProcessingResult(
                source_file=pdf_path,
                pages=pages
            )
            
            # ç¬¬ä¸‰æ­¥ï¼šAIå†…å®¹é‡ç»„ï¼ˆå¯é€‰ï¼‰
            if enable_ai_enhancement:
                logger.info("ğŸ§  ç¬¬ä¸‰æ­¥ï¼šAIå†…å®¹é‡ç»„")
                enhanced_pages = self.ai_reorganizer.process_pages(
                    pages, 
                    parallel_processing=self.config.ai_content.enable_parallel_processing
                )
                # æ›´æ–°å¤„ç†ç»“æœ
                media_result.pages = enhanced_pages
            
            # æ„å»ºå¤„ç†ç»“æœ
            final_result = media_result
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = time.time() - start_time
            final_result.summary["processing_time"] = processing_time
            
            # ä¿å­˜ç»“æœ
            result_file = output_path / "basic_processing_result.json"
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(final_result.to_dict(), f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… åŸºç¡€è§£æå®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}ç§’")
            
            return json.dumps({
                "status": "success",
                "message": "åŸºç¡€è§£æå®Œæˆ",
                "result": final_result.to_dict(),
                "output_files": {
                    "result_file": str(result_file),
                    "output_directory": str(output_path)
                }
            }, ensure_ascii=False)
            
        except Exception as e:
            logger.error(f"åŸºç¡€è§£æå¤±è´¥: {e}")
            return json.dumps({
                "status": "error",
                "message": f"åŸºç¡€è§£æå¤±è´¥: {str(e)}"
            }, ensure_ascii=False)
    
    def _parse_page_split(self, **kwargs) -> str:
        """
        ä½¿ç”¨é¡µé¢åˆ†å‰²æ–¹æ³•è§£æPDFï¼ˆè§£å†³é¡µé¢æ ‡æ³¨æ¼‚ç§»é—®é¢˜ï¼‰
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            enable_ai_enhancement: æ˜¯å¦å¯ç”¨AIå¢å¼ºå¤„ç†
            parallel_processing: æ˜¯å¦å¯ç”¨å¹¶è¡Œå¤„ç†
            
        Returns:
            str: è§£æç»“æœçš„JSONå­—ç¬¦ä¸²
        """
        import time
        import tempfile
        import fitz  # PyMuPDF
        from pathlib import Path
        
        # æ£€æŸ¥å¿…éœ€å‚æ•°
        pdf_path = kwargs.get("pdf_path")
        if not pdf_path:
            return json.dumps({
                "status": "error",
                "message": "ç¼ºå°‘å¿…éœ€å‚æ•°: pdf_path"
            }, indent=2, ensure_ascii=False)
        
        if not os.path.exists(pdf_path):
            return json.dumps({
                "status": "error", 
                "message": f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}"
            }, indent=2, ensure_ascii=False)
        
        # è·å–é…ç½®
        output_dir = kwargs.get("output_dir")
        if not output_dir:
            # åˆ›å»ºåŸºäºæ—¶é—´æˆ³çš„è¾“å‡ºç›®å½•
            import datetime
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir = f"parser_output/{timestamp}_page_split"
            os.makedirs(output_dir, exist_ok=True)
        
        enable_ai_enhancement = kwargs.get("enable_ai_enhancement", True)
        
        # åŒºåˆ†ä¸¤ç§å¹¶è¡Œå¤„ç†
        docling_parallel_processing = kwargs.get("docling_parallel_processing", False)  # doclingå¹¶è¡Œå¤„ç†ï¼ŒMacä¸Šç¦ç”¨
        ai_parallel_processing = kwargs.get("ai_parallel_processing", True)  # AIå¹¶è¡Œå¤„ç†ï¼Œå¯ç‹¬ç«‹å¯ç”¨
        
        # å‘åå…¼å®¹ï¼šå¦‚æœä½¿ç”¨æ—§çš„parallel_processingå‚æ•°
        if "parallel_processing" in kwargs:
            docling_parallel_processing = kwargs.get("parallel_processing", False)
            # AIå¹¶è¡Œå¤„ç†ä¿æŒå¯ç”¨ï¼Œé™¤éæ˜¾å¼ç¦ç”¨
            ai_parallel_processing = kwargs.get("ai_parallel_processing", True)
        
        print(f"ğŸ”„ å¼€å§‹é¡µé¢åˆ†å‰²è§£æ: {pdf_path}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        print(f"ğŸ§  AIå¢å¼º: {'å¯ç”¨' if enable_ai_enhancement else 'ç¦ç”¨'}")
        print(f"âš¡ Doclingå¹¶è¡Œå¤„ç†: {'å¯ç”¨' if docling_parallel_processing else 'ç¦ç”¨'}")
        print(f"ğŸš€ AIå¹¶è¡Œå¤„ç†: {'å¯ç”¨' if ai_parallel_processing else 'ç¦ç”¨'}")
        
        start_time = time.time()
        
        try:
            # æ­¥éª¤1: åˆ†å‰²PDFä¸ºå•é¡µæ–‡ä»¶
            pages_data = self._split_and_process_pdf_pages(pdf_path, output_dir, docling_parallel_processing)
            
            # æ­¥éª¤2: AIå¢å¼ºå¤„ç†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if enable_ai_enhancement:
                print("ğŸ§  å¼€å§‹AIå¢å¼ºå¤„ç†...")
                pages_data = self.ai_reorganizer.process_pages(pages_data, ai_parallel_processing)
            
            # æ­¥éª¤3: ç”Ÿæˆå¤„ç†ç»“æœ
            from .data_models import ProcessingResult
            result = ProcessingResult(
                source_file=pdf_path,
                pages=pages_data
            )
            
            # æ­¥éª¤4: ä¿å­˜ç»“æœ
            result_file = os.path.join(output_dir, "page_split_processing_result.json")
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(result.to_dict(), f, ensure_ascii=False, indent=2)
            
            # æ­¥éª¤5: TOCæå–ï¼ˆåŸºäºç¼åˆåçš„å®Œæ•´æ–‡æœ¬ï¼‰
            print("ğŸ“– å¼€å§‹TOCæå–...")
            try:
                full_text = self.toc_extractor.stitch_full_text(result_file)
                toc_items, reasoning_content = self.toc_extractor.extract_toc_with_reasoning(full_text)
                
                # ä¿å­˜TOCç»“æœ
                toc_result = {
                    "toc": [item.__dict__ for item in toc_items],
                    "reasoning_content": reasoning_content
                }
                toc_file = os.path.join(output_dir, "toc_extraction_result.json")
                with open(toc_file, 'w', encoding='utf-8') as f:
                    json.dump(toc_result, f, ensure_ascii=False, indent=2)
                
                print(f"âœ… TOCæå–å®Œæˆï¼Œå…±æ‰¾åˆ° {len(toc_items)} ä¸ªç« èŠ‚")
                
            except Exception as e:
                print(f"âŒ TOCæå–å¤±è´¥: {e}")
                toc_items = []
                toc_result = {"toc": [], "reasoning_content": ""}
            
            # æ­¥éª¤6: AIåˆ†å—ï¼ˆåŸºäºTOCç»“æœï¼‰
            print("ğŸ”ª å¼€å§‹AIåˆ†å—...")
            chunks_result = None
            try:
                if toc_items:
                    chunks_result = self.text_chunker.chunk_text_with_toc(full_text, toc_result)
                    
                    # ä¿å­˜åˆ†å—ç»“æœ
                    chunks_file = os.path.join(output_dir, "chunks_result.json")
                    chunks_dict = {
                        "first_level_chapters": [
                            {
                                "chapter_id": chapter.chapter_id,
                                "title": chapter.title,
                                "content": chapter.content,
                                "start_pos": chapter.start_pos,
                                "end_pos": chapter.end_pos,
                                "word_count": chapter.word_count,
                                "has_images": chapter.has_images,
                                "has_tables": chapter.has_tables
                            }
                            for chapter in chunks_result.first_level_chapters
                        ],
                        "minimal_chunks": [
                            {
                                "chunk_id": chunk.chunk_id,
                                "content": chunk.content,
                                "chunk_type": chunk.chunk_type,
                                "belongs_to_chapter": chunk.belongs_to_chapter,
                                "chapter_title": chunk.chapter_title,
                                "start_pos": chunk.start_pos,
                                "end_pos": chunk.end_pos,
                                "word_count": chunk.word_count
                            }
                            for chunk in chunks_result.minimal_chunks
                        ],
                        "total_chapters": chunks_result.total_chapters,
                        "total_chunks": chunks_result.total_chunks,
                        "processing_metadata": chunks_result.processing_metadata
                    }
                    with open(chunks_file, 'w', encoding='utf-8') as f:
                        json.dump(chunks_dict, f, ensure_ascii=False, indent=2)
                    
                    print(f"âœ… AIåˆ†å—å®Œæˆï¼Œå…±ç”Ÿæˆ {chunks_result.total_chunks} ä¸ªåˆ†å—")
                else:
                    print("âš ï¸ æ²¡æœ‰TOCä¿¡æ¯ï¼Œè·³è¿‡åˆ†å—æ­¥éª¤")
                    
            except Exception as e:
                print(f"âŒ AIåˆ†å—å¤±è´¥: {e}")
            
            # æ­¥éª¤7: Metadataå¤„ç†ï¼ˆåŸºäºå‰é¢çš„æ‰€æœ‰ç»“æœï¼‰
            print("ğŸ“Š å¼€å§‹Metadataå¤„ç†...")
            try:
                metadata_dir = os.path.join(output_dir, "metadata")
                os.makedirs(metadata_dir, exist_ok=True)
                
                # 7.1 æå–åŸºç¡€metadata
                basic_metadata = self.metadata_extractor.extract_from_page_split_result(result_file)
                document_id = basic_metadata["document_info"]["document_id"]
                
                # å¤„ç†TOC metadataï¼ˆéœ€è¦ä½¿ç”¨TOCæ–‡ä»¶è·¯å¾„ï¼‰
                if toc_result and toc_result.get("toc"):
                    toc_metadata = self.metadata_extractor.extract_from_toc_result(toc_file, document_id)
                    # è¿™é‡Œéœ€è¦åˆå¹¶å…ƒæ•°æ®ï¼Œç”±äºextract_from_toc_resultè¿”å›tupleï¼Œéœ€è¦å¤„ç†
                    doc_toc, chapter_mapping = toc_metadata
                    basic_metadata["toc"] = doc_toc.__dict__
                
                # æå–å›¾ç‰‡å’Œè¡¨æ ¼metadataå¹¶åˆ†é…ç« èŠ‚ID
                if chunks_result:
                    image_metadata = self.metadata_extractor._extract_image_metadata(result.to_dict(), document_id)
                    table_metadata = self.metadata_extractor._extract_table_metadata(result.to_dict(), document_id)
                    
                    chunking_metadata = self.metadata_extractor.extract_from_chunking_result(chunks_result, image_metadata, table_metadata)
                    basic_metadata.update(chunking_metadata)
                
                # ä¿å­˜åŸºç¡€metadataï¼ˆä½¿ç”¨metadata_extractorçš„æ­£ç¡®åºåˆ—åŒ–æ–¹æ³•ï¼‰
                self.metadata_extractor.save_extracted_metadata(
                    metadata_dir,
                    basic=basic_metadata
                )
                
                # 7.2 ç”Ÿæˆæ–‡æ¡£æ‘˜è¦
                if chunks_result:
                    document_summary = self.document_summary_generator.generate_document_summary(
                        document_info=basic_metadata["document_info"],
                        chunking_result=chunks_result,
                        toc_data=toc_result,
                        image_count=len(basic_metadata.get("image_metadata", [])),
                        table_count=len(basic_metadata.get("table_metadata", []))
                    )
                    
                    # ä¿å­˜æ–‡æ¡£æ‘˜è¦ï¼ˆdocument_summaryæ˜¯tupleï¼‰
                    doc_summary_file = os.path.join(metadata_dir, "document_summary.json")
                    with open(doc_summary_file, 'w', encoding='utf-8') as f:
                        json.dump(document_summary[0].__dict__, f, ensure_ascii=False, indent=2, default=str)
                
                # 7.3 ç”Ÿæˆç« èŠ‚æ‘˜è¦
                if chunks_result and chunks_result.first_level_chapters and 'chapter_mapping' in locals():
                    chapter_summaries = self.chapter_summary_generator.generate_chapter_summaries(
                        document_id=document_id,
                        chunking_result=chunks_result,
                        toc_data=toc_result,
                        image_metadata=basic_metadata.get("image_metadata", []),
                        table_metadata=basic_metadata.get("table_metadata", [])
                    )
                    
                    # ä¿å­˜ç« èŠ‚æ‘˜è¦ï¼ˆchapter_summariesæ˜¯list of tuplesï¼‰
                    chapter_summary_file = os.path.join(metadata_dir, "chapter_summaries.json")
                    chapter_summaries_data = [summary[0].__dict__ for summary in chapter_summaries]
                    with open(chapter_summary_file, 'w', encoding='utf-8') as f:
                        json.dump(chapter_summaries_data, f, ensure_ascii=False, indent=2, default=str)
                
                # 7.4 ç”Ÿæˆè¡ç”Ÿé—®é¢˜
                if chunks_result and chunks_result.minimal_chunks and 'chapter_mapping' in locals():
                    derived_questions = self.question_generator.generate_questions_from_chunks(
                        document_id=document_id,
                        chunking_result=chunks_result,
                        chapter_mapping=chapter_mapping
                    )
                    
                    # ä¿å­˜è¡ç”Ÿé—®é¢˜ï¼ˆderived_questionsæ˜¯list of tuplesï¼‰
                    questions_file = os.path.join(metadata_dir, "derived_questions.json")
                    questions_data = [question[0].__dict__ for question in derived_questions]
                    with open(questions_file, 'w', encoding='utf-8') as f:
                        json.dump(questions_data, f, ensure_ascii=False, indent=2, default=str)
                
                print(f"âœ… Metadataå¤„ç†å®Œæˆï¼Œç»“æœä¿å­˜åœ¨: {metadata_dir}")
                
            except Exception as e:
                print(f"âŒ Metadataå¤„ç†å¤±è´¥: {e}")
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            print(f"âœ… å®Œæ•´æµç¨‹å¤„ç†å®Œæˆï¼ˆåŒ…å«Metadataï¼‰ï¼Œè€—æ—¶: {processing_time:.2f} ç§’")
            
            # æ„å»ºå®Œæ•´çš„è¿”å›ç»“æœ
            response_data = {
                "status": "success",
                "message": "é¡µé¢åˆ†å‰²è§£æå®Œæˆ",
                "output_directory": output_dir,
                "processing_time": processing_time,
                "pages": [page.to_dict() for page in pages_data],
                "pages_count": len(pages_data),
                "toc_count": len(toc_items),
                "chunks_count": chunks_result.total_chunks if chunks_result else 0
            }
            
            return json.dumps(response_data, indent=2, ensure_ascii=False)
            
        except Exception as e:
            logger.error(f"é¡µé¢åˆ†å‰²è§£æå¤±è´¥: {e}")
            return json.dumps({
                "status": "error",
                "message": f"é¡µé¢åˆ†å‰²è§£æå¤±è´¥: {str(e)}"
            }, indent=2, ensure_ascii=False)
    
    def _split_and_process_pdf_pages(self, pdf_path: str, output_dir: str, parallel_processing: bool) -> List[PageData]:
        """
        åˆ†å‰²PDFå¹¶å¤„ç†æ¯ä¸€é¡µ
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            parallel_processing: æ˜¯å¦å¯ç”¨å¹¶è¡Œå¤„ç†
            
        Returns:
            List[PageData]: æ‰€æœ‰é¡µé¢çš„å¤„ç†ç»“æœ
        """
        import fitz  # PyMuPDF
        import tempfile
        import shutil
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        # æ­¥éª¤1: åˆ†å‰²PDFä¸ºå•é¡µæ–‡ä»¶
        doc = fitz.open(pdf_path)
        total_pages = len(doc)
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = tempfile.mkdtemp(prefix="pdf_pages_")
        single_page_files = []
        
        try:
            for page_num in range(total_pages):
                # åˆ›å»ºæ–°çš„PDFæ–‡æ¡£ï¼ŒåªåŒ…å«å½“å‰é¡µ
                single_page_doc = fitz.open()
                single_page_doc.insert_pdf(doc, from_page=page_num, to_page=page_num)
                
                # ä¿å­˜å•é¡µPDF
                single_page_path = os.path.join(temp_dir, f"page_{page_num + 1}.pdf")
                single_page_doc.save(single_page_path)
                single_page_doc.close()
                
                single_page_files.append((page_num + 1, single_page_path))
                
        finally:
            doc.close()
        
        print(f"ğŸ“„ PDFåˆ†å‰²å®Œæˆï¼Œå…± {len(single_page_files)} é¡µ")
        
        # æ­¥éª¤2: å¤„ç†æ‰€æœ‰é¡µé¢
        try:
            if parallel_processing and len(single_page_files) > 1:
                pages_data = self._process_pages_parallel_split(single_page_files, output_dir)
            else:
                pages_data = self._process_pages_sequential_split(single_page_files, output_dir)
            
            # æ­¥éª¤3: æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            shutil.rmtree(temp_dir)
            print(f"ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {temp_dir}")
            
            return pages_data
            
        except Exception as e:
            # ç¡®ä¿æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            shutil.rmtree(temp_dir)
            raise e
    
    def _process_pages_parallel_split(self, single_page_files: List[Tuple[int, str]], output_dir: str) -> List[PageData]:
        """å¹¶è¡Œå¤„ç†åˆ†å‰²åçš„é¡µé¢"""
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        max_workers = self.config.media_extractor.max_workers
        print(f"âš¡ å¯ç”¨å¹¶è¡Œå¤„ç†æ¨¡å¼ï¼Œæœ€å¤§å·¥ä½œçº¿ç¨‹æ•°: {max_workers}")
        
        pages_data = [None] * len(single_page_files)
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_page = {
                executor.submit(
                    self._process_single_page_split, 
                    page_num, 
                    single_page_path, 
                    output_dir
                ): page_num
                for page_num, single_page_path in single_page_files
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_page):
                page_num = future_to_page[future]
                try:
                    page_data = future.result()
                    pages_data[page_num - 1] = page_data  # é¡µç ä»1å¼€å§‹ï¼Œç´¢å¼•ä»0å¼€å§‹
                    print(f"âœ… é¡µé¢ {page_num} å¤„ç†å®Œæˆ")
                except Exception as e:
                    print(f"âŒ é¡µé¢ {page_num} å¤„ç†å¤±è´¥: {e}")
                    # åˆ›å»ºç©ºçš„é¡µé¢æ•°æ®
                    pages_data[page_num - 1] = PageData(
                        page_number=page_num,
                        raw_text=f"é¡µé¢ {page_num} å¤„ç†å¤±è´¥: {str(e)}",
                        images=[],
                        tables=[]
                    )
        
        # è¿‡æ»¤Noneå€¼
        return [page for page in pages_data if page is not None]
    
    def _process_pages_sequential_split(self, single_page_files: List[Tuple[int, str]], output_dir: str) -> List[PageData]:
        """é¡ºåºå¤„ç†åˆ†å‰²åçš„é¡µé¢"""
        print("ğŸ”„ ä½¿ç”¨é¡ºåºå¤„ç†æ¨¡å¼")
        
        pages_data = []
        for page_num, single_page_path in single_page_files:
            try:
                page_data = self._process_single_page_split(page_num, single_page_path, output_dir)
                pages_data.append(page_data)
                print(f"âœ… é¡µé¢ {page_num} å¤„ç†å®Œæˆ")
            except Exception as e:
                print(f"âŒ é¡µé¢ {page_num} å¤„ç†å¤±è´¥: {e}")
                # åˆ›å»ºç©ºçš„é¡µé¢æ•°æ®
                pages_data.append(PageData(
                    page_number=page_num,
                    raw_text=f"é¡µé¢ {page_num} å¤„ç†å¤±è´¥: {str(e)}",
                    images=[],
                    tables=[]
                ))
        
        return pages_data
    
    def _process_single_page_split(self, page_num: int, single_page_path: str, output_dir: str) -> PageData:
        """
        å¤„ç†å•é¡µPDF
        
        Args:
            page_num: é¡µç 
            single_page_path: å•é¡µPDFæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            PageData: é¡µé¢æ•°æ®
        """
        from pathlib import Path
        
        # åˆ›å»ºé¡µé¢ä¸“ç”¨çš„è¾“å‡ºç›®å½•
        page_output_dir = os.path.join(output_dir, f"page_{page_num}")
        os.makedirs(page_output_dir, exist_ok=True)
        
        # ä½¿ç”¨ç°æœ‰çš„PDFè§£æå™¨å¤„ç†å•é¡µPDF
        raw_result, page_texts = self.document_parser.parse_pdf(single_page_path)
        
        # ç”±äºæ˜¯å•é¡µPDFï¼Œåº”è¯¥åªæœ‰ä¸€é¡µæ–‡æœ¬
        page_text = ""
        if page_texts:
            page_text = next(iter(page_texts.values()))  # è·å–ç¬¬ä¸€ä¸ªï¼ˆä¹Ÿæ˜¯å”¯ä¸€ä¸€ä¸ªï¼‰é¡µé¢çš„æ–‡æœ¬
        
        # åˆ›å»ºé¡µé¢æ•°æ®
        page_data = PageData(
            page_number=page_num,
            raw_text=page_text,
            images=[],
            tables=[]
        )
        
        # æå–å›¾ç‰‡å’Œè¡¨æ ¼ - ç”±äºæ˜¯å•é¡µPDFï¼Œæ‰€æœ‰åª’ä½“éƒ½å±äºå½“å‰é¡µ
        self._extract_media_for_single_page_split(raw_result, page_data, page_output_dir)
        
        return page_data
    
    def _extract_media_for_single_page_split(self, raw_result: Any, page_data: PageData, page_output_dir: str):
        """ä¸ºå•é¡µPDFæå–å›¾ç‰‡å’Œè¡¨æ ¼"""
        try:
            # ç”±äºæ˜¯å•é¡µPDFï¼Œæ‰€æœ‰åª’ä½“éƒ½å±äºå½“å‰é¡µ
            image_counter = 0
            table_counter = 0
            
            # æå–å›¾ç‰‡
            for picture in raw_result.document.pictures:
                image_counter += 1
                try:
                    # è·å–å›¾ç‰‡
                    picture_image = picture.get_image(raw_result.document)
                    if picture_image is None:
                        continue
                    
                    # ä¿å­˜å›¾ç‰‡
                    image_filename = f"picture-{image_counter}.png"
                    image_path = os.path.join(page_output_dir, image_filename)
                    with open(image_path, "wb") as fp:
                        picture_image.save(fp, "PNG")
                    
                    # è·å–å›¾ç‰‡ä¿¡æ¯
                    from PIL import Image
                    image_img = Image.open(image_path)
                    caption = picture.caption_text(raw_result.document) if hasattr(picture, 'caption_text') else ""
                    
                    # åˆ›å»ºImageWithContextå¯¹è±¡
                    image_with_context = ImageWithContext(
                        image_path=image_path,
                        page_number=page_data.page_number,
                        page_context=page_data.raw_text,
                        caption=caption or f"å›¾ç‰‡ {image_counter}",
                        metadata={
                            'width': image_img.width,
                            'height': image_img.height,
                            'size': image_img.width * image_img.height,
                            'aspect_ratio': image_img.width / image_img.height
                        }
                    )
                    
                    page_data.images.append(image_with_context)
                    
                except Exception as e:
                    print(f"âŒ é¡µé¢ {page_data.page_number} å›¾ç‰‡ {image_counter} å¤„ç†å¤±è´¥: {e}")
            
            # æå–è¡¨æ ¼
            for table in raw_result.document.tables:
                table_counter += 1
                try:
                    # è·å–è¡¨æ ¼å›¾ç‰‡
                    table_image = table.get_image(raw_result.document)
                    if table_image is None:
                        continue
                    
                    # ä¿å­˜è¡¨æ ¼å›¾ç‰‡
                    table_filename = f"table-{table_counter}.png"
                    table_path = os.path.join(page_output_dir, table_filename)
                    with open(table_path, "wb") as fp:
                        table_image.save(fp, "PNG")
                    
                    # è·å–è¡¨æ ¼ä¿¡æ¯
                    from PIL import Image
                    table_img = Image.open(table_path)
                    caption = table.caption_text(raw_result.document) if hasattr(table, 'caption_text') else ""
                    
                    # åˆ›å»ºTableWithContextå¯¹è±¡
                    table_with_context = TableWithContext(
                        table_path=table_path,
                        page_number=page_data.page_number,
                        page_context=page_data.raw_text,
                        caption=caption or f"è¡¨æ ¼ {table_counter}",
                        metadata={
                            'width': table_img.width,
                            'height': table_img.height,
                            'size': table_img.width * table_img.height,
                            'aspect_ratio': table_img.width / table_img.height
                        }
                    )
                    
                    page_data.tables.append(table_with_context)
                    
                except Exception as e:
                    print(f"âŒ é¡µé¢ {page_data.page_number} è¡¨æ ¼ {table_counter} å¤„ç†å¤±è´¥: {e}")
            
            print(f"ğŸ“Š é¡µé¢ {page_data.page_number}: {len(page_data.images)} ä¸ªå›¾ç‰‡, {len(page_data.tables)} ä¸ªè¡¨æ ¼")
            
        except Exception as e:
            print(f"âŒ é¡µé¢ {page_data.page_number} åª’ä½“æå–å¤±è´¥: {e}")
    
    def _parse_advanced_disabled(self, pdf_path: str, output_dir: str = None, enable_ai_enhancement: bool = True) -> str:
        """
        é«˜çº§è§£ææ¨¡å¼ï¼ˆæš‚æ—¶ç¦ç”¨ï¼‰
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            enable_ai_enhancement: æ˜¯å¦å¯ç”¨AIå¢å¼º
            
        Returns:
            str: JSONæ ¼å¼çš„å¤„ç†ç»“æœ
        """
        return json.dumps({
            "status": "error",
            "message": "é«˜çº§è§£ææ¨¡å¼æš‚æ—¶ä¸å¯ç”¨ï¼Œç›¸å…³ç»„ä»¶æ­£åœ¨é‡æ„ä¸­ã€‚è¯·ä½¿ç”¨ parse_basic æˆ– parse_page_split æ¨¡å¼ã€‚"
        }, indent=2, ensure_ascii=False)
    
    def _get_config(self) -> str:
        """è·å–é…ç½®ä¿¡æ¯"""
        from .config import create_output_directory
        
        output_dir = create_output_directory(self.config)
        
        return json.dumps({
            "status": "success",
            "config": {
                "output_dir": output_dir,
                "default_llm_model": self.config.ai_content.default_llm_model,
                "default_vlm_model": self.config.ai_content.default_vlm_model,
                "supported_models": self.config.supported_models,
                "max_concurrent_tasks": self.config.ai_content.max_workers,
                "enable_parallel_processing": self.config.ai_content.enable_parallel_processing,
                "enable_text_cleaning": self.config.ai_content.enable_text_cleaning,
                "enable_image_description": self.config.ai_content.enable_image_description,
                "enable_table_description": self.config.ai_content.enable_table_description,
                "base_output_dir": self.config.output.base_output_dir,
                "create_timestamped_dirs": self.config.output.create_timestamped_dirs,
                "custom_output_path": self.config.output.custom_output_path
            }
        }, ensure_ascii=False)
    
    # ä¿æŒå‘åå…¼å®¹çš„æ¥å£
    def parse_pdf(self, pdf_path: str, output_dir: str = None, use_advanced: bool = False) -> Dict[str, Any]:
        """
        è§£æPDFæ–‡ä»¶ï¼ˆå‘åå…¼å®¹æ¥å£ï¼‰
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            use_advanced: æ˜¯å¦ä½¿ç”¨é«˜çº§è§£æ
            
        Returns:
            Dict[str, Any]: è§£æç»“æœ
        """
        action = "parse_advanced" if use_advanced else "parse_basic"
        result_json = self.execute(action, pdf_path=pdf_path, output_dir=output_dir)
        return json.loads(result_json) 