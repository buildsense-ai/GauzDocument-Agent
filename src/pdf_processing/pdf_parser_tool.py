#!/usr/bin/env python3
"""
PDF Parser Tool - é‡æ„ç‰ˆæœ¬
æ•´åˆæ‰€æœ‰PDFå¤„ç†ç»„ä»¶ï¼Œæä¾›ç»Ÿä¸€çš„å·¥å…·æ¥å£
"""

import json
import logging
import time
from typing import Dict, List, Any, Optional, Union
from pathlib import Path

from .config import PDFProcessingConfig
from .pdf_document_parser import PDFDocumentParser
from .media_extractor import MediaExtractor
from .ai_content_reorganizer import AIContentReorganizer
from .document_structure_analyzer import DocumentStructureAnalyzer, MinimalChunk
from .metadata_enricher import MetadataEnricher, EnrichedChunk, ChapterSummary
from .data_models import ProcessingResult, AdvancedProcessingResult
from ..base_tool import Tool

logger = logging.getLogger(__name__)


class PDFParserTool(Tool):
    """
    PDFè§£æå·¥å…· - é‡æ„ç‰ˆæœ¬
    
    æ”¯æŒä¸¤ç§å¤„ç†æ¨¡å¼ï¼š
    1. åŸºç¡€æ¨¡å¼ï¼šé¡µé¢çº§è§£æã€åª’ä½“æå–ã€AIå†…å®¹é‡ç»„
    2. é«˜çº§æ¨¡å¼ï¼šåŒ…å«æ–‡æ¡£ç»“æ„åˆ†æã€æ™ºèƒ½åˆ†å—ã€å…ƒæ•°æ®å¢å¼º
    """
    
    def __init__(self, config: PDFProcessingConfig = None):
        super().__init__(
            name="pdf_parser",
            description="ğŸš€ PDFè§£æå·¥å…· - æ™ºèƒ½æå–PDFä¸­çš„æ–‡æœ¬ã€å›¾ç‰‡ã€è¡¨æ ¼ï¼Œæ”¯æŒç»“æ„åŒ–åˆ†æå’Œæ™ºèƒ½åˆ†å—"
        )
        
        self.config = config or PDFProcessingConfig()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._init_components()
        
        logger.info("âœ… PDFè§£æå·¥å…·åˆå§‹åŒ–å®Œæˆ")
    
    def _init_components(self):
        """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""
        try:
            # åŸºç¡€å¤„ç†ç»„ä»¶
            self.document_parser = PDFDocumentParser(self.config)
            self.media_extractor = MediaExtractor(
                parallel_processing=self.config.media_extractor.parallel_processing,
                max_workers=self.config.media_extractor.max_workers
            )
            self.ai_reorganizer = AIContentReorganizer(self.config)
            
            # é«˜çº§å¤„ç†ç»„ä»¶
            self.structure_analyzer = DocumentStructureAnalyzer(self.config)
            self.metadata_enricher = MetadataEnricher(self.config)
            
        except Exception as e:
            logger.error(f"ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def execute(self, action: str, **kwargs) -> str:
        """
        æ‰§è¡ŒPDFå¤„ç†æ“ä½œ
        
        Args:
            action: æ“ä½œç±»å‹
                - "parse_basic": åŸºç¡€è§£æ
                - "parse_advanced": é«˜çº§è§£æï¼ˆåŒ…å«ç»“æ„åˆ†æï¼‰
                - "get_config": è·å–é…ç½®ä¿¡æ¯
            **kwargs: å…¶ä»–å‚æ•°
                - pdf_path: PDFæ–‡ä»¶è·¯å¾„
                - output_dir: è¾“å‡ºç›®å½•
                - enable_ai_enhancement: æ˜¯å¦å¯ç”¨AIå¢å¼º
                
        Returns:
            str: JSONæ ¼å¼çš„å¤„ç†ç»“æœ
        """
        try:
            if action == "parse_basic":
                return self._parse_basic(**kwargs)
            elif action == "parse_advanced":
                return self._parse_advanced(**kwargs)
            elif action == "get_config":
                return self._get_config()
            else:
                return json.dumps({
                    "status": "error",
                    "message": f"ä¸æ”¯æŒçš„æ“ä½œç±»å‹: {action}",
                    "supported_actions": ["parse_basic", "parse_advanced", "get_config"]
                }, ensure_ascii=False)
                
        except Exception as e:
            logger.error(f"æ‰§è¡Œæ“ä½œå¤±è´¥: {e}")
            return json.dumps({
                "status": "error", 
                "message": f"æ‰§è¡Œå¤±è´¥: {str(e)}"
            }, ensure_ascii=False)
    
    def _parse_basic(self, pdf_path: str, output_dir: str = None, enable_ai_enhancement: bool = True) -> str:
        """
        åŸºç¡€è§£ææ¨¡å¼
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            enable_ai_enhancement: æ˜¯å¦å¯ç”¨AIå¢å¼º
            
        Returns:
            str: JSONæ ¼å¼çš„å¤„ç†ç»“æœ
        """
        start_time = time.time()
        
        logger.info(f"ğŸš€ å¼€å§‹åŸºç¡€è§£æ: {pdf_path}")
        
        try:
            # éªŒè¯è¾“å…¥
            if not Path(pdf_path).exists():
                return json.dumps({
                    "status": "error",
                    "message": f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}"
                }, ensure_ascii=False)
            
            # è®¾ç½®è¾“å‡ºç›®å½•
            if output_dir is None:
                from .config import create_output_directory
                output_dir = create_output_directory(self.config)
            
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            
            # ç¬¬ä¸€æ­¥ï¼šè§£æPDFæ–‡æ¡£
            logger.info("ğŸ“„ ç¬¬ä¸€æ­¥ï¼šè§£æPDFæ–‡æ¡£")
            raw_result, page_texts = self.document_parser.parse_pdf(pdf_path)
            
            # ç¬¬äºŒæ­¥ï¼šæå–åª’ä½“æ–‡ä»¶
            logger.info("ğŸ–¼ï¸ ç¬¬äºŒæ­¥ï¼šæå–åª’ä½“æ–‡ä»¶")
            pages = self.media_extractor.extract_media_from_pages(
                raw_result=raw_result,
                page_texts=page_texts,
                output_dir=str(output_path)
            )
            
            # ä¿å­˜åª’ä½“ä¿¡æ¯
            self.media_extractor.save_media_info(pages, str(output_path))
            
            # æ„å»ºProcessingResultå¯¹è±¡
            from .data_models import ProcessingResult
            media_result = ProcessingResult(
                source_file=pdf_path,
                pages=pages
            )
            
            # ç¬¬ä¸‰æ­¥ï¼šAIå†…å®¹é‡ç»„ï¼ˆå¯é€‰ï¼‰
            if enable_ai_enhancement:
                logger.info("ğŸ§  ç¬¬ä¸‰æ­¥ï¼šAIå†…å®¹é‡ç»„")
                enhanced_pages = self.ai_reorganizer.process_pages(
                    pages, 
                    parallel_processing=self.config.ai_content.enable_parallel_processing
                )
                # æ›´æ–°å¤„ç†ç»“æœ
                media_result.pages = enhanced_pages
            
            # æ„å»ºå¤„ç†ç»“æœ
            final_result = media_result
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = time.time() - start_time
            final_result.summary["processing_time"] = processing_time
            
            # ä¿å­˜ç»“æœ
            result_file = output_path / "basic_processing_result.json"
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(final_result.to_dict(), f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… åŸºç¡€è§£æå®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}ç§’")
            
            return json.dumps({
                "status": "success",
                "message": "åŸºç¡€è§£æå®Œæˆ",
                "result": final_result.to_dict(),
                "output_files": {
                    "result_file": str(result_file),
                    "output_directory": str(output_path)
                }
            }, ensure_ascii=False)
            
        except Exception as e:
            logger.error(f"åŸºç¡€è§£æå¤±è´¥: {e}")
            return json.dumps({
                "status": "error",
                "message": f"åŸºç¡€è§£æå¤±è´¥: {str(e)}"
            }, ensure_ascii=False)
    
    def _parse_advanced(self, pdf_path: str, output_dir: str = None, enable_ai_enhancement: bool = True) -> str:
        """
        é«˜çº§è§£ææ¨¡å¼
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            enable_ai_enhancement: æ˜¯å¦å¯ç”¨AIå¢å¼º
            
        Returns:
            str: JSONæ ¼å¼çš„å¤„ç†ç»“æœ
        """
        start_time = time.time()
        
        logger.info(f"ğŸš€ å¼€å§‹é«˜çº§è§£æ: {pdf_path}")
        
        try:
            # é¦–å…ˆæ‰§è¡ŒåŸºç¡€è§£æ
            basic_result_json = self._parse_basic(pdf_path, output_dir, enable_ai_enhancement)
            basic_result_data = json.loads(basic_result_json)
            
            if basic_result_data["status"] != "success":
                return basic_result_json
            
            # è·å–åŸºç¡€å¤„ç†ç»“æœ
            basic_result = ProcessingResult(
                source_file=pdf_path,
                pages=[]  # è¿™é‡Œéœ€è¦ä»basic_result_dataé‡å»ºPageDataå¯¹è±¡
            )
            
            # é‡å»ºPageDataå¯¹è±¡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            for page_data in basic_result_data["result"]["pages"]:
                from .data_models import PageData, ImageWithContext, TableWithContext
                
                images = []
                for img_data in page_data["images"]:
                    image = ImageWithContext(
                        image_path=img_data["image_path"],
                        page_number=img_data["page_number"],
                        page_context=img_data["page_context"],
                        ai_description=img_data.get("ai_description"),
                        caption=img_data.get("caption"),
                        metadata=img_data.get("metadata", {})
                    )
                    images.append(image)
                
                tables = []
                for table_data in page_data["tables"]:
                    table = TableWithContext(
                        table_path=table_data["table_path"],
                        page_number=table_data["page_number"],
                        page_context=table_data["page_context"],
                        ai_description=table_data.get("ai_description"),
                        caption=table_data.get("caption"),
                        metadata=table_data.get("metadata", {})
                    )
                    tables.append(table)
                
                page = PageData(
                    page_number=page_data["page_number"],
                    raw_text=page_data["raw_text"],
                    cleaned_text=page_data.get("cleaned_text"),
                    images=images,
                    tables=tables
                )
                basic_result.pages.append(page)
            
            # è®¾ç½®è¾“å‡ºç›®å½•
            if output_dir is None:
                from .config import create_output_directory
                output_dir = create_output_directory(self.config)
            output_path = Path(output_dir)
            
            # ç¬¬å››æ­¥ï¼šæ–‡æ¡£ç»“æ„åˆ†æå’Œæ™ºèƒ½åˆ†å—
            logger.info("ğŸ“Š ç¬¬å››æ­¥ï¼šæ–‡æ¡£ç»“æ„åˆ†æå’Œæ™ºèƒ½åˆ†å—")
            
            # åˆå¹¶æ‰€æœ‰é¡µé¢çš„æ¸…æ´—æ–‡æœ¬
            full_text = ""
            for page in basic_result.pages:
                page_text = page.cleaned_text if page.cleaned_text else page.raw_text
                full_text += page_text + "\n\n"
            
            document_structure, minimal_chunks = self.structure_analyzer.analyze_and_chunk(
                full_text, 
                basic_result.summary["total_pages"]
            )
            
            # ç¬¬äº”æ­¥ï¼šå…ƒæ•°æ®å¢å¼º
            logger.info("ğŸ” ç¬¬äº”æ­¥ï¼šå…ƒæ•°æ®å¢å¼º")
            index_structure = self.metadata_enricher.enrich_metadata(
                document_structure,
                minimal_chunks,
                basic_result
            )
            
            # æ„å»ºé«˜çº§å¤„ç†ç»“æœ
            processing_time = time.time() - start_time
            processing_metadata = {
                "total_processing_time": processing_time,
                "basic_processing_time": basic_result_data["result"]["summary"]["processing_time"],
                "advanced_processing_time": processing_time - basic_result_data["result"]["summary"]["processing_time"],
                "cache_optimization_enabled": True,
                "chunks_generated": len(minimal_chunks),
                "chapters_identified": len(document_structure.toc)
            }
            
            advanced_result = AdvancedProcessingResult(
                basic_result=basic_result,
                document_structure=document_structure,
                index_structure=index_structure,
                processing_metadata=processing_metadata
            )
            
            # ä¿å­˜é«˜çº§ç»“æœ
            advanced_result_file = output_path / "advanced_processing_result.json"
            with open(advanced_result_file, 'w', encoding='utf-8') as f:
                json.dump(advanced_result.to_dict(), f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜ç´¢å¼•ç»“æ„
            index_file = output_path / "index_structure.json"
            self.metadata_enricher.save_index_structure(index_structure, str(index_file))
            
            logger.info(f"âœ… é«˜çº§è§£æå®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}ç§’")
            
            return json.dumps({
                "status": "success",
                "message": "é«˜çº§è§£æå®Œæˆ",
                "result": advanced_result.to_dict(),
                "output_files": {
                    "advanced_result_file": str(advanced_result_file),
                    "index_file": str(index_file),
                    "output_directory": str(output_path)
                },
                "performance_metrics": {
                    "total_time": processing_time,
                    "chunks_generated": len(minimal_chunks),
                    "chapters_identified": len(document_structure.toc),
                    "cache_optimization": "enabled"
                }
            }, ensure_ascii=False)
            
        except Exception as e:
            logger.error(f"é«˜çº§è§£æå¤±è´¥: {e}")
            return json.dumps({
                "status": "error",
                "message": f"é«˜çº§è§£æå¤±è´¥: {str(e)}"
            }, ensure_ascii=False)
    
    def _get_config(self) -> str:
        """è·å–é…ç½®ä¿¡æ¯"""
        from .config import create_output_directory
        
        output_dir = create_output_directory(self.config)
        
        return json.dumps({
            "status": "success",
            "config": {
                "output_dir": output_dir,
                "default_llm_model": self.config.ai_content.default_llm_model,
                "default_vlm_model": self.config.ai_content.default_vlm_model,
                "supported_models": self.config.supported_models,
                "max_concurrent_tasks": self.config.ai_content.max_workers,
                "enable_parallel_processing": self.config.ai_content.enable_parallel_processing,
                "enable_text_cleaning": self.config.ai_content.enable_text_cleaning,
                "enable_image_description": self.config.ai_content.enable_image_description,
                "enable_table_description": self.config.ai_content.enable_table_description,
                "base_output_dir": self.config.output.base_output_dir,
                "create_timestamped_dirs": self.config.output.create_timestamped_dirs,
                "custom_output_path": self.config.output.custom_output_path
            }
        }, ensure_ascii=False)
    
    # ä¿æŒå‘åå…¼å®¹çš„æ¥å£
    def parse_pdf(self, pdf_path: str, output_dir: str = None, use_advanced: bool = False) -> Dict[str, Any]:
        """
        è§£æPDFæ–‡ä»¶ï¼ˆå‘åå…¼å®¹æ¥å£ï¼‰
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            use_advanced: æ˜¯å¦ä½¿ç”¨é«˜çº§è§£æ
            
        Returns:
            Dict[str, Any]: è§£æç»“æœ
        """
        action = "parse_advanced" if use_advanced else "parse_basic"
        result_json = self.execute(action, pdf_path=pdf_path, output_dir=output_dir)
        return json.loads(result_json) 