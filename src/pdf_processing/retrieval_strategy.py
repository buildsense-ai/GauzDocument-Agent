#!/usr/bin/env python3
"""
æ£€ç´¢ç­–ç•¥å®ç°
æ”¯æŒ"å°å—æ£€ç´¢ï¼Œå¤§å—å–‚å…»"æ¨¡å¼
å®ç°æœ€å°å—æ£€ç´¢å’Œå®Œæ•´ç« èŠ‚è¿”å›
"""

import json
import os
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from collections import defaultdict
import re

from .text_chunker import ChunkingResult, MinimalChunk, ChapterContent

@dataclass
class RetrievalQuery:
    """æ£€ç´¢æŸ¥è¯¢"""
    query_text: str
    top_k: int = 5
    chunk_types: Optional[List[str]] = None  # é™åˆ¶æ£€ç´¢çš„å—ç±»å‹
    chapter_filter: Optional[List[str]] = None  # é™åˆ¶æ£€ç´¢çš„ç« èŠ‚
    
@dataclass
class ChunkMatch:
    """å—åŒ¹é…ç»“æœ"""
    chunk: MinimalChunk
    score: float
    match_type: str  # exact, partial, semantic
    matched_text: str
    
@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœ"""
    query: RetrievalQuery
    chunk_matches: List[ChunkMatch]
    relevant_chapters: List[ChapterContent]
    chapter_scores: Dict[str, float]  # ç« èŠ‚ID -> èšåˆå¾—åˆ†
    total_matches: int


class RetrievalStrategy:
    """æ£€ç´¢ç­–ç•¥å®ç°"""
    
    def __init__(self, chunking_result: ChunkingResult):
        self.chunking_result = chunking_result
        self.chunks_by_chapter = self._group_chunks_by_chapter()
        self.chapters_by_id = {ch.chapter_id: ch for ch in chunking_result.first_level_chapters}
        
    def _group_chunks_by_chapter(self) -> Dict[str, List[MinimalChunk]]:
        """æŒ‰ç« èŠ‚åˆ†ç»„å—"""
        groups = defaultdict(list)
        for chunk in self.chunking_result.minimal_chunks:
            groups[chunk.belongs_to_chapter].append(chunk)
        return dict(groups)
    
    def search(self, query: RetrievalQuery) -> RetrievalResult:
        """
        æ‰§è¡Œæ£€ç´¢
        
        Args:
            query: æ£€ç´¢æŸ¥è¯¢
            
        Returns:
            RetrievalResult: æ£€ç´¢ç»“æœ
        """
        print(f"ğŸ” å¼€å§‹æ£€ç´¢: '{query.query_text}'")
        
        # 1. åœ¨æœ€å°å—ä¸­æ£€ç´¢
        chunk_matches = self._search_in_chunks(query)
        print(f"âœ… æ‰¾åˆ° {len(chunk_matches)} ä¸ªåŒ¹é…çš„å—")
        
        # 2. è®¡ç®—ç« èŠ‚å¾—åˆ†
        chapter_scores = self._calculate_chapter_scores(chunk_matches)
        print(f"ğŸ“Š æ¶‰åŠ {len(chapter_scores)} ä¸ªç« èŠ‚")
        
        # 3. è·å–ç›¸å…³ç« èŠ‚
        relevant_chapters = self._get_relevant_chapters(chapter_scores, query.top_k)
        print(f"ğŸ“– è¿”å› {len(relevant_chapters)} ä¸ªå®Œæ•´ç« èŠ‚")
        
        # 4. æ„å»ºç»“æœ
        result = RetrievalResult(
            query=query,
            chunk_matches=chunk_matches,
            relevant_chapters=relevant_chapters,
            chapter_scores=chapter_scores,
            total_matches=len(chunk_matches)
        )
        
        return result
    
    def _search_in_chunks(self, query: RetrievalQuery) -> List[ChunkMatch]:
        """åœ¨æœ€å°å—ä¸­æ£€ç´¢"""
        matches = []
        query_text = query.query_text.lower()
        
        for chunk in self.chunking_result.minimal_chunks:
            # åº”ç”¨å—ç±»å‹è¿‡æ»¤
            if query.chunk_types and chunk.chunk_type not in query.chunk_types:
                continue
                
            # åº”ç”¨ç« èŠ‚è¿‡æ»¤
            if query.chapter_filter and chunk.belongs_to_chapter not in query.chapter_filter:
                continue
            
            # æ£€æŸ¥åŒ¹é…
            chunk_text = chunk.content.lower()
            match_score = 0
            match_type = "none"
            matched_text = ""
            
            # 1. ç²¾ç¡®åŒ¹é…
            if query_text in chunk_text:
                match_score = 1.0
                match_type = "exact"
                matched_text = query_text
            
            # 2. éƒ¨åˆ†åŒ¹é…ï¼ˆæŸ¥è¯¢è¯çš„å­é›†ï¼‰
            elif self._has_partial_match(query_text, chunk_text):
                match_score = 0.7
                match_type = "partial"
                matched_text = self._get_partial_match(query_text, chunk_text)
            
            # 3. è¯­ä¹‰åŒ¹é…ï¼ˆå…³é”®è¯é‡å ï¼‰
            elif self._has_semantic_match(query_text, chunk_text):
                match_score = 0.4
                match_type = "semantic"
                matched_text = self._get_semantic_match(query_text, chunk_text)
            
            # æ ¹æ®å—ç±»å‹è°ƒæ•´å¾—åˆ†
            if match_score > 0:
                match_score = self._adjust_score_by_chunk_type(match_score, chunk.chunk_type)
                
                match = ChunkMatch(
                    chunk=chunk,
                    score=match_score,
                    match_type=match_type,
                    matched_text=matched_text
                )
                matches.append(match)
        
        # æŒ‰å¾—åˆ†æ’åº
        matches.sort(key=lambda x: x.score, reverse=True)
        
        return matches[:query.top_k * 3]  # è¿”å›æ›´å¤šå—ç”¨äºç« èŠ‚èšåˆ
    
    def _has_partial_match(self, query_text: str, chunk_text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰éƒ¨åˆ†åŒ¹é…"""
        query_words = query_text.split()
        if len(query_words) <= 1:
            return False
            
        # è‡³å°‘åŒ¹é…ä¸€åŠçš„è¯
        matched_words = sum(1 for word in query_words if word in chunk_text)
        return matched_words >= len(query_words) // 2
    
    def _get_partial_match(self, query_text: str, chunk_text: str) -> str:
        """è·å–éƒ¨åˆ†åŒ¹é…çš„æ–‡æœ¬"""
        query_words = query_text.split()
        matched_words = [word for word in query_words if word in chunk_text]
        return " ".join(matched_words)
    
    def _has_semantic_match(self, query_text: str, chunk_text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰è¯­ä¹‰åŒ¹é…"""
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        query_words = set(query_text.split())
        chunk_words = set(chunk_text.split())
        
        # è®¡ç®—äº¤é›†
        intersection = query_words.intersection(chunk_words)
        return len(intersection) > 0
    
    def _get_semantic_match(self, query_text: str, chunk_text: str) -> str:
        """è·å–è¯­ä¹‰åŒ¹é…çš„æ–‡æœ¬"""
        query_words = set(query_text.split())
        chunk_words = set(chunk_text.split())
        intersection = query_words.intersection(chunk_words)
        return " ".join(intersection)
    
    def _adjust_score_by_chunk_type(self, score: float, chunk_type: str) -> float:
        """æ ¹æ®å—ç±»å‹è°ƒæ•´å¾—åˆ†"""
        # ä¸åŒç±»å‹çš„å—æœ‰ä¸åŒçš„æ£€ç´¢ä»·å€¼
        type_weights = {
            "title": 1.2,      # æ ‡é¢˜å—æƒé‡é«˜
            "paragraph": 1.0,  # æ®µè½å—æ ‡å‡†æƒé‡
            "list_item": 0.9,  # åˆ—è¡¨é¡¹ç•¥ä½
            "image_desc": 0.7, # å›¾ç‰‡æè¿°è¾ƒä½
            "table_desc": 0.8  # è¡¨æ ¼æè¿°ä¸­ç­‰
        }
        
        weight = type_weights.get(chunk_type, 1.0)
        return score * weight
    
    def _calculate_chapter_scores(self, chunk_matches: List[ChunkMatch]) -> Dict[str, float]:
        """è®¡ç®—ç« èŠ‚èšåˆå¾—åˆ†"""
        chapter_scores = defaultdict(float)
        
        for match in chunk_matches:
            chapter_id = match.chunk.belongs_to_chapter
            
            # èšåˆå¾—åˆ†ï¼Œè€ƒè™‘å¤šä¸ªå—çš„è´¡çŒ®
            chapter_scores[chapter_id] += match.score
            
            # å¦‚æœåŒä¸€ç« èŠ‚æœ‰å¤šä¸ªåŒ¹é…ï¼Œç»™äºˆbonus
            existing_matches = sum(1 for m in chunk_matches 
                                 if m.chunk.belongs_to_chapter == chapter_id)
            if existing_matches > 1:
                chapter_scores[chapter_id] += 0.1 * (existing_matches - 1)
        
        return dict(chapter_scores)
    
    def _get_relevant_chapters(self, chapter_scores: Dict[str, float], top_k: int) -> List[ChapterContent]:
        """è·å–ç›¸å…³ç« èŠ‚"""
        # æŒ‰å¾—åˆ†æ’åº
        sorted_chapters = sorted(chapter_scores.items(), key=lambda x: x[1], reverse=True)
        
        # è·å–top_kç« èŠ‚
        relevant_chapters = []
        for chapter_id, score in sorted_chapters[:top_k]:
            if chapter_id in self.chapters_by_id:
                relevant_chapters.append(self.chapters_by_id[chapter_id])
        
        return relevant_chapters
    
    def format_retrieval_result(self, result: RetrievalResult) -> str:
        """æ ¼å¼åŒ–æ£€ç´¢ç»“æœä¸ºå¯è¯»æ–‡æœ¬"""
        output = []
        
        # æŸ¥è¯¢ä¿¡æ¯
        output.append(f"ğŸ” æŸ¥è¯¢: {result.query.query_text}")
        output.append(f"ğŸ“Š æ‰¾åˆ° {result.total_matches} ä¸ªåŒ¹é…å—ï¼Œæ¶‰åŠ {len(result.relevant_chapters)} ä¸ªç« èŠ‚\n")
        
        # ç« èŠ‚ç»“æœ
        for i, chapter in enumerate(result.relevant_chapters):
            chapter_score = result.chapter_scores.get(chapter.chapter_id, 0)
            output.append(f"ğŸ“– ç« èŠ‚ {i+1}: {chapter.title} (å¾—åˆ†: {chapter_score:.2f})")
            output.append(f"   ç« èŠ‚ID: {chapter.chapter_id}")
            output.append(f"   å­—æ•°: {chapter.word_count}")
            output.append(f"   å†…å®¹é¢„è§ˆ: {chapter.content[:200]}...")
            output.append("")
        
        # åŒ¹é…å—è¯¦æƒ…
        output.append("ğŸ” åŒ¹é…å—è¯¦æƒ…:")
        for i, match in enumerate(result.chunk_matches[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
            output.append(f"   {i+1}. {match.chunk.chunk_id} ({match.chunk.chunk_type})")
            output.append(f"      å¾—åˆ†: {match.score:.2f} ({match.match_type})")
            output.append(f"      åŒ¹é…æ–‡æœ¬: {match.matched_text}")
            output.append(f"      æ‰€å±ç« èŠ‚: {match.chunk.chapter_title}")
            output.append(f"      å†…å®¹: {match.chunk.content[:100]}...")
            output.append("")
        
        return "\n".join(output)
    
    def save_retrieval_result(self, result: RetrievalResult, output_path: str):
        """ä¿å­˜æ£€ç´¢ç»“æœ"""
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        result_dict = {
            "query": {
                "query_text": result.query.query_text,
                "top_k": result.query.top_k,
                "chunk_types": result.query.chunk_types,
                "chapter_filter": result.query.chapter_filter
            },
            "chunk_matches": [
                {
                    "chunk": {
                        "chunk_id": match.chunk.chunk_id,
                        "content": match.chunk.content,
                        "chunk_type": match.chunk.chunk_type,
                        "belongs_to_chapter": match.chunk.belongs_to_chapter,
                        "chapter_title": match.chunk.chapter_title,
                        "word_count": match.chunk.word_count
                    },
                    "score": match.score,
                    "match_type": match.match_type,
                    "matched_text": match.matched_text
                }
                for match in result.chunk_matches
            ],
            "relevant_chapters": [
                {
                    "chapter_id": chapter.chapter_id,
                    "title": chapter.title,
                    "content": chapter.content,
                    "word_count": chapter.word_count,
                    "has_images": chapter.has_images,
                    "has_tables": chapter.has_tables
                }
                for chapter in result.relevant_chapters
            ],
            "chapter_scores": result.chapter_scores,
            "total_matches": result.total_matches
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result_dict, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ æ£€ç´¢ç»“æœå·²ä¿å­˜åˆ°: {output_path}") 