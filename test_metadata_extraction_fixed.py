#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®å¤åçš„metadataæå–åŠŸèƒ½
éªŒè¯å›¾ç‰‡å’Œè¡¨æ ¼çš„æ­£ç¡®å­—æ®µæå–ä»¥åŠç« èŠ‚åˆ†é…
"""

import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
src_path = os.path.join(project_root, 'src')
if src_path not in sys.path:
    sys.path.insert(0, src_path)

from pdf_processing.metadata_extractor import MetadataExtractor
from pdf_processing.text_chunker import TextChunker
from pdf_processing.toc_extractor import TOCExtractor
import json

def test_fixed_metadata_extraction():
    """æµ‹è¯•ä¿®å¤åçš„metadataæå–"""
    
    print("ğŸ”§ æµ‹è¯•ä¿®å¤åçš„å›¾ç‰‡/è¡¨æ ¼metadataæå–...")
    
    # ä½¿ç”¨ç°æœ‰çš„æµ‹è¯•æ•°æ®
    page_split_file = "parser_output/20250715_131307_page_split/page_split_processing_result.json"
    
    if not os.path.exists(page_split_file):
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {page_split_file}")
        return
    
    # åˆå§‹åŒ–æå–å™¨
    extractor = MetadataExtractor(project_name="åŒ»çµå¤åº™è®¾è®¡æ–¹æ¡ˆ")
    
    # 1. æµ‹è¯•page_splitç»“æœçš„metadataæå–
    print("\nğŸ“„ æå–åŸºç¡€metadata...")
    basic_metadata = extractor.extract_from_page_split_result(page_split_file)
    
    print(f"âœ… æ–‡æ¡£ä¿¡æ¯: {basic_metadata['document_info']['file_name']}")
    print(f"ğŸ“Š å›¾ç‰‡æ•°é‡: {len(basic_metadata['image_metadata'])}")
    print(f"ğŸ“‹ è¡¨æ ¼æ•°é‡: {len(basic_metadata['table_metadata'])}")
    
    # éªŒè¯å›¾ç‰‡metadata
    if basic_metadata['image_metadata']:
        first_image = basic_metadata['image_metadata'][0]
        print(f"\nğŸ–¼ï¸ ç¬¬ä¸€ä¸ªå›¾ç‰‡metadataéªŒè¯:")
        print(f"   å›¾ç‰‡è·¯å¾„: {first_image.image_path}")
        print(f"   å°ºå¯¸: {first_image.width}x{first_image.height}")
        print(f"   æ–‡ä»¶å¤§å°: {first_image.size}")
        print(f"   å®½é«˜æ¯”: {first_image.aspect_ratio:.2f}")
        print(f"   AIæè¿°: {first_image.ai_description}")
        print(f"   ç« èŠ‚ID: {first_image.chapter_id}")
    
    # éªŒè¯è¡¨æ ¼metadata
    if basic_metadata['table_metadata']:
        first_table = basic_metadata['table_metadata'][0]
        print(f"\nğŸ“Š ç¬¬ä¸€ä¸ªè¡¨æ ¼metadataéªŒè¯:")
        print(f"   è¡¨æ ¼è·¯å¾„: {first_table.table_path}")
        print(f"   å°ºå¯¸: {first_table.width}x{first_table.height}")
        print(f"   æ–‡ä»¶å¤§å°: {first_table.size}")
        print(f"   å®½é«˜æ¯”: {first_table.aspect_ratio:.2f}")
        print(f"   AIæè¿°: {first_table.ai_description}")
        print(f"   ç« èŠ‚ID: {first_table.chapter_id}")
    
    # 2. æµ‹è¯•ç« èŠ‚åˆ†é…åŠŸèƒ½
    print("\nğŸ§µ æµ‹è¯•full_textç”Ÿæˆå’Œç« èŠ‚åˆ†é…...")
    
    # ç”Ÿæˆfull_textï¼ˆåŒ…å«å”¯ä¸€æ ‡è¯†ç¬¦ï¼‰
    toc_extractor = TOCExtractor()
    full_text = toc_extractor.stitch_full_text(page_split_file)
    
    print(f"ğŸ“ Full texté•¿åº¦: {len(full_text)} å­—ç¬¦")
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ–°çš„æ ¼å¼åŒ–å¼•ç”¨
    import re
    image_refs = re.findall(r'\[å›¾ç‰‡\|ID:(\d+)\|PATH:([^:]+):', full_text)
    table_refs = re.findall(r'\[è¡¨æ ¼\|ID:(\d+)\|PATH:([^:]+):', full_text)
    
    print(f"ğŸ” å‘ç°å›¾ç‰‡å¼•ç”¨: {len(image_refs)} ä¸ª")
    print(f"ğŸ” å‘ç°è¡¨æ ¼å¼•ç”¨: {len(table_refs)} ä¸ª")
    
    if image_refs:
        print(f"   ç¤ºä¾‹å›¾ç‰‡å¼•ç”¨: ID={image_refs[0][0]}, PATH={image_refs[0][1]}")
    if table_refs:
        print(f"   ç¤ºä¾‹è¡¨æ ¼å¼•ç”¨: ID={table_refs[0][0]}, PATH={table_refs[0][1]}")
    
    # 3. ç”ŸæˆTOCç»“æœç”¨äºæµ‹è¯•
    print("\nğŸ“‘ ç”ŸæˆTOCç»“æœç”¨äºæµ‹è¯•...")
    toc_items, reasoning = toc_extractor.extract_toc_with_reasoning(full_text)
    toc_result = {"toc_items": toc_items, "reasoning": reasoning}
    
    # 4. æµ‹è¯•æ–‡æœ¬åˆ†å—
    print("\nâœ‚ï¸ æµ‹è¯•æ–‡æœ¬åˆ†å—...")
    text_chunker = TextChunker()
    chunking_result = text_chunker.chunk_text_with_toc(full_text, toc_result)
    
    print(f"ğŸ“š ç« èŠ‚æ•°é‡: {len(chunking_result.first_level_chapters)}")
    print(f"ğŸ“¦ åˆ†å—æ•°é‡: {len(chunking_result.minimal_chunks)}")
    
    # 5. æµ‹è¯•ç²¾ç¡®ç« èŠ‚åˆ†é…
    print("\nğŸ¯ æµ‹è¯•ç²¾ç¡®ç« èŠ‚åˆ†é…...")
    chunk_metadata = extractor.extract_from_chunking_result(
        chunking_result,
        basic_metadata['image_metadata'],
        basic_metadata['table_metadata']
    )
    
    # éªŒè¯ç« èŠ‚åˆ†é…ç»“æœ
    assigned_images = [img for img in basic_metadata['image_metadata'] if img.chapter_id]
    assigned_tables = [tbl for tbl in basic_metadata['table_metadata'] if tbl.chapter_id]
    
    print(f"âœ… å·²åˆ†é…ç« èŠ‚çš„å›¾ç‰‡: {len(assigned_images)} / {len(basic_metadata['image_metadata'])}")
    print(f"âœ… å·²åˆ†é…ç« èŠ‚çš„è¡¨æ ¼: {len(assigned_tables)} / {len(basic_metadata['table_metadata'])}")
    
    # æ˜¾ç¤ºåˆ†é…ç»“æœç¤ºä¾‹
    for i, img in enumerate(assigned_images[:3]):  # æ˜¾ç¤ºå‰3ä¸ª
        print(f"   å›¾ç‰‡ {i+1}: ç« èŠ‚ {img.chapter_id} - {os.path.basename(img.image_path)}")
    
    for i, tbl in enumerate(assigned_tables[:3]):  # æ˜¾ç¤ºå‰3ä¸ª
        print(f"   è¡¨æ ¼ {i+1}: ç« èŠ‚ {tbl.chapter_id} - {os.path.basename(tbl.table_path)}")
    
    # 6. ä¿å­˜æµ‹è¯•ç»“æœ
    print("\nğŸ’¾ ä¿å­˜æµ‹è¯•ç»“æœ...")
    output_dir = "parser_output/metadata_test_fixed"
    os.makedirs(output_dir, exist_ok=True)
    
    extractor.save_extracted_metadata(
        output_dir,
        image_metadata=basic_metadata['image_metadata'],
        table_metadata=basic_metadata['table_metadata'],
        text_chunks=chunk_metadata['text_chunks']
    )
    
    print(f"âœ… æµ‹è¯•å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {output_dir}")

if __name__ == "__main__":
    test_fixed_metadata_extraction() 