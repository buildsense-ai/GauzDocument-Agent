#!/usr/bin/env python3
"""
å®Œæ•´å·¥ä½œæµæµ‹è¯•è„šæœ¬
æµ‹è¯•: page_split â†’ AIå¢å¼º â†’ TOCæå– â†’ AIåˆ†å— çš„å®Œæ•´æµç¨‹
"""

import os
import json
import time
from pathlib import Path

# å¯¼å…¥PDFè§£æå·¥å…·
from src.pdf_processing.pdf_parser_tool import PDFParserTool

def test_complete_workflow():
    """æµ‹è¯•å®Œæ•´çš„å·¥ä½œæµç¨‹"""
    
    # æµ‹è¯•æ–‡ä»¶è·¯å¾„
    test_pdf = "testfiles/åŒ»çµå¤åº™è®¾è®¡æ–¹æ¡ˆ.pdf"
    
    if not os.path.exists(test_pdf):
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_pdf}")
        return
    
    print("ğŸš€ å¼€å§‹å®Œæ•´å·¥ä½œæµæµ‹è¯•...")
    print("=" * 60)
    
    # åˆ›å»ºPDFè§£æå·¥å…·
    tool = PDFParserTool()
    
    # æ‰§è¡Œå®Œæ•´æµç¨‹
    start_time = time.time()
    
    result_json = tool.execute(
        action="parse_page_split",
        pdf_path=test_pdf,
        enable_ai_enhancement=True,
        docling_parallel_processing=False,  # Macä¸Šç¦ç”¨doclingå¹¶è¡Œå¤„ç†é¿å…PyTorché”™è¯¯
        ai_parallel_processing=True  # å¯ç”¨AIå¹¶è¡Œå¤„ç†ï¼Œæå‡æ€§èƒ½
    )
    
    end_time = time.time()
    total_time = end_time - start_time
    
    print("=" * 60)
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f} ç§’")
    
    # è§£æç»“æœ
    try:
        result = json.loads(result_json)
        
        if result["status"] == "success":
            print("âœ… å®Œæ•´æµç¨‹æµ‹è¯•æˆåŠŸ!")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {result['output_directory']}")
            print(f"ğŸ“„ å¤„ç†é¡µæ•°: {result['pages_count']}")
            print(f"ğŸ“– TOCç« èŠ‚æ•°: {result['toc_count']}")
            print(f"ğŸ”ª åˆ†å—æ•°é‡: {result['chunks_count']}")
            print(f"â±ï¸  å¤„ç†æ—¶é—´: {result['processing_time']:.2f} ç§’")
            
            # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
            output_dir = result['output_directory']
            check_output_files(output_dir)
            
        else:
            print("âŒ å®Œæ•´æµç¨‹æµ‹è¯•å¤±è´¥!")
            print(f"é”™è¯¯ä¿¡æ¯: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
            
    except json.JSONDecodeError as e:
        print(f"âŒ JSONè§£æå¤±è´¥: {e}")
        print("åŸå§‹å“åº”:")
        print(result_json)

def check_output_files(output_dir):
    """æ£€æŸ¥è¾“å‡ºæ–‡ä»¶"""
    print("\nğŸ“ æ£€æŸ¥è¾“å‡ºæ–‡ä»¶:")
    print("-" * 40)
    
    expected_files = [
        "page_split_processing_result.json",
        "toc_extraction_result.json", 
        "chunks_result.json"
    ]
    
    for filename in expected_files:
        filepath = os.path.join(output_dir, filename)
        if os.path.exists(filepath):
            file_size = os.path.getsize(filepath)
            print(f"âœ… {filename} ({file_size:,} bytes)")
            
            # å¦‚æœæ˜¯JSONæ–‡ä»¶ï¼Œæ£€æŸ¥å†…å®¹
            if filename.endswith('.json'):
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    if filename == "page_split_processing_result.json":
                        print(f"   ğŸ“„ é¡µé¢æ•°: {len(data.get('pages', []))}")
                        print(f"   ğŸ–¼ï¸  å›¾ç‰‡æ•°: {sum(len(page.get('images', [])) for page in data.get('pages', []))}")
                        print(f"   ğŸ“Š è¡¨æ ¼æ•°: {sum(len(page.get('tables', [])) for page in data.get('pages', []))}")
                    
                    elif filename == "toc_extraction_result.json":
                        toc_items = data.get('toc', [])
                        print(f"   ğŸ“– TOCé¡¹ç›®æ•°: {len(toc_items)}")
                        if toc_items:
                            level_counts = {}
                            for item in toc_items:
                                level = item.get('level', 0)
                                level_counts[level] = level_counts.get(level, 0) + 1
                            print(f"   ğŸ“Š å„çº§åˆ«æ•°é‡: {level_counts}")
                    
                    elif filename == "chunks_result.json":
                        print(f"   ğŸ“– ç« èŠ‚æ•°: {data.get('total_chapters', 0)}")
                        print(f"   ğŸ”ª åˆ†å—æ•°: {data.get('total_chunks', 0)}")
                        
                except Exception as e:
                    print(f"   âš ï¸  æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
        else:
            print(f"âŒ {filename} (ç¼ºå¤±)")
    
    # æ£€æŸ¥é¡µé¢ç›®å½•
    print("\nğŸ“ æ£€æŸ¥é¡µé¢ç›®å½•:")
    print("-" * 40)
    
    page_dirs = [d for d in os.listdir(output_dir) if d.startswith('page_') and os.path.isdir(os.path.join(output_dir, d))]
    page_dirs.sort(key=lambda x: int(x.split('_')[1]))
    
    total_images = 0
    total_tables = 0
    
    for page_dir in page_dirs[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé¡µé¢
        page_path = os.path.join(output_dir, page_dir)
        images = [f for f in os.listdir(page_path) if f.endswith('.png') and 'picture' in f]
        tables = [f for f in os.listdir(page_path) if f.endswith('.png') and 'table' in f]
        
        total_images += len(images)
        total_tables += len(tables)
        
        print(f"âœ… {page_dir}: {len(images)} å›¾ç‰‡, {len(tables)} è¡¨æ ¼")
    
    if len(page_dirs) > 5:
        print(f"   ... è¿˜æœ‰ {len(page_dirs) - 5} ä¸ªé¡µé¢")
    
    print(f"ğŸ“Š æ€»è®¡: {total_images} å›¾ç‰‡, {total_tables} è¡¨æ ¼")

if __name__ == "__main__":
    test_complete_workflow() 