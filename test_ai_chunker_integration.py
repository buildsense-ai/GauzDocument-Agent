#!/usr/bin/env python3
"""
æµ‹è¯•AIåˆ†å—å™¨é›†æˆ
éªŒè¯æ–°çš„AIåˆ†å—å™¨æ˜¯å¦ä¸ç°æœ‰pipelineæ­£å¸¸å·¥ä½œ
"""

import json
import os
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.pdf_processing.text_chunker import TextChunker
from src.pdf_processing.toc_extractor import TOCExtractor

def test_ai_chunker_integration():
    """æµ‹è¯•AIåˆ†å—å™¨é›†æˆ"""
    
    print("ğŸ§ª å¼€å§‹æµ‹è¯•AIåˆ†å—å™¨é›†æˆ...")
    
    # 1. ä½¿ç”¨åŸºç¡€å¤„ç†ç»“æœ
    basic_result_path = "parser_output/20250714_232720_vvmpc0/basic_processing_result.json"
    
    if not os.path.exists(basic_result_path):
        print(f"âŒ åŸºç¡€å¤„ç†ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {basic_result_path}")
        return False
    
    try:
        # 2. åˆ›å»ºTOCæå–å™¨ï¼Œç¼åˆå®Œæ•´æ–‡æœ¬
        print("ğŸ“„ æ­£åœ¨æå–TOC...")
        toc_extractor = TOCExtractor()
        full_text = toc_extractor.stitch_full_text(basic_result_path)
        
        # 3. æå–TOC
        toc_items, _ = toc_extractor.extract_toc_with_reasoning(full_text)
        
        if not toc_items:
            print("âŒ TOCæå–å¤±è´¥")
            return False
        
        # 4. æ„å»ºTOCç»“æœ
        toc_result = {
            'toc': [
                {
                    'id': item.id,
                    'title': item.title,
                    'level': item.level,
                    'start_text': item.start_text,
                    'parent_id': item.parent_id
                }
                for item in toc_items
            ]
        }
        
        # 5. åˆ›å»ºAIåˆ†å—å™¨
        print("ğŸ¤– åˆ›å»ºAIåˆ†å—å™¨...")
        ai_chunker = TextChunker(use_ai_chunker=True, ai_model="qwen-turbo")
        
        # 6. è¿›è¡Œåˆ†å—
        print("ğŸ”ª å¼€å§‹AIåˆ†å—...")
        chunking_result = ai_chunker.chunk_text_with_toc(full_text, toc_result)
        
        # 7. éªŒè¯ç»“æœ
        print("\nğŸ“Š åˆ†å—ç»“æœç»Ÿè®¡:")
        print(f"- ç¬¬ä¸€çº§ç« èŠ‚æ•°: {len(chunking_result.first_level_chapters)}")
        print(f"- æœ€å°åˆ†å—æ•°: {len(chunking_result.minimal_chunks)}")
        print(f"- åˆ†å—æ–¹æ³•: {chunking_result.processing_metadata.get('chunking_method', 'unknown')}")
        print(f"- AIæ¨¡å‹: {chunking_result.processing_metadata.get('ai_model', 'unknown')}")
        
        # 8. æ˜¾ç¤ºç« èŠ‚ä¿¡æ¯
        print("\nğŸ“š ç« èŠ‚ä¿¡æ¯:")
        for i, chapter in enumerate(chunking_result.first_level_chapters[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
            print(f"  {i+1}. {chapter.title} (ID: {chapter.chapter_id})")
            print(f"     - å­—æ•°: {chapter.word_count}")
            print(f"     - åŒ…å«å›¾ç‰‡: {'æ˜¯' if chapter.has_images else 'å¦'}")
            print(f"     - åŒ…å«è¡¨æ ¼: {'æ˜¯' if chapter.has_tables else 'å¦'}")
        
        # 9. æ˜¾ç¤ºåˆ†å—ä¿¡æ¯
        print("\nğŸ§© åˆ†å—ä¿¡æ¯ (å‰5ä¸ª):")
        for i, chunk in enumerate(chunking_result.minimal_chunks[:5]):
            print(f"  {i+1}. {chunk.chunk_id} ({chunk.chunk_type})")
            print(f"     - æ‰€å±ç« èŠ‚: {chunk.belongs_to_chapter} - {chunk.chapter_title}")
            print(f"     - å†…å®¹é¢„è§ˆ: {chunk.content[:50]}...")
            print(f"     - å­—æ•°: {chunk.word_count}")
        
        # 10. éªŒè¯å›¾ç‰‡è¡¨æ ¼ç« èŠ‚æ ‡æ³¨
        print("\nğŸ–¼ï¸ éªŒè¯å›¾ç‰‡è¡¨æ ¼ç« èŠ‚æ ‡æ³¨:")
        image_chunks = [chunk for chunk in chunking_result.minimal_chunks if '[å›¾ç‰‡:' in chunk.content]
        table_chunks = [chunk for chunk in chunking_result.minimal_chunks if '[è¡¨æ ¼:' in chunk.content]
        
        print(f"- åŒ…å«å›¾ç‰‡çš„åˆ†å—: {len(image_chunks)}")
        print(f"- åŒ…å«è¡¨æ ¼çš„åˆ†å—: {len(table_chunks)}")
        
        # æ˜¾ç¤ºä¸€äº›å›¾ç‰‡åˆ†å—çš„ç« èŠ‚å½’å±
        for i, chunk in enumerate(image_chunks[:3]):
            print(f"  å›¾ç‰‡åˆ†å— {i+1}: å½’å±ç« èŠ‚ {chunk.belongs_to_chapter} - {chunk.chapter_title}")
        
        # 11. ä¿å­˜ç»“æœ
        output_dir = "test_output"
        os.makedirs(output_dir, exist_ok=True)
        
        ai_chunker.save_chunking_result(chunking_result, output_dir)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_fallback_comparison():
    """æµ‹è¯•AIåˆ†å—å™¨ä¸ä¼ ç»Ÿåˆ†å—å™¨çš„å¯¹æ¯”"""
    
    print("\nğŸ”„ å¼€å§‹å¯¹æ¯”æµ‹è¯•...")
    
    basic_result_path = "parser_output/20250714_232720_vvmpc0/basic_processing_result.json"
    
    if not os.path.exists(basic_result_path):
        print(f"âŒ åŸºç¡€å¤„ç†ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {basic_result_path}")
        return False
    
    try:
        # å‡†å¤‡TOCæ•°æ®
        toc_extractor = TOCExtractor()
        full_text = toc_extractor.stitch_full_text(basic_result_path)
        toc_items, _ = toc_extractor.extract_toc_with_reasoning(full_text)
        
        toc_result = {
            'toc': [
                {
                    'id': item.id,
                    'title': item.title,
                    'level': item.level,
                    'start_text': item.start_text,
                    'parent_id': item.parent_id
                }
                for item in toc_items
            ]
        }
        
        # AIåˆ†å—å™¨
        ai_chunker = TextChunker(use_ai_chunker=True, ai_model="qwen-turbo")
        ai_result = ai_chunker.chunk_text_with_toc(full_text, toc_result)
        
        # ä¼ ç»Ÿåˆ†å—å™¨
        regex_chunker = TextChunker(use_ai_chunker=False)
        regex_result = regex_chunker.chunk_text_with_toc(full_text, toc_result)
        
        # å¯¹æ¯”ç»“æœ
        print("\nğŸ“Š å¯¹æ¯”ç»“æœ:")
        print(f"AIåˆ†å—å™¨ - ç« èŠ‚æ•°: {len(ai_result.first_level_chapters)}, åˆ†å—æ•°: {len(ai_result.minimal_chunks)}")
        print(f"ä¼ ç»Ÿåˆ†å—å™¨ - ç« èŠ‚æ•°: {len(regex_result.first_level_chapters)}, åˆ†å—æ•°: {len(regex_result.minimal_chunks)}")
        
        # æ˜¾ç¤ºç¬¬ä¸€ä¸ªç« èŠ‚çš„åˆ†å—å¯¹æ¯”
        if ai_result.first_level_chapters and regex_result.first_level_chapters:
            first_chapter_id = ai_result.first_level_chapters[0].chapter_id
            
            ai_chapter_chunks = [c for c in ai_result.minimal_chunks if c.belongs_to_chapter == first_chapter_id]
            regex_chapter_chunks = [c for c in regex_result.minimal_chunks if c.belongs_to_chapter == first_chapter_id]
            
            print(f"\nğŸ“– ç¬¬ä¸€ä¸ªç« èŠ‚åˆ†å—å¯¹æ¯”:")
            print(f"AIåˆ†å—å™¨: {len(ai_chapter_chunks)} ä¸ªåˆ†å—")
            print(f"ä¼ ç»Ÿåˆ†å—å™¨: {len(regex_chapter_chunks)} ä¸ªåˆ†å—")
            
            # æ˜¾ç¤ºå‰2ä¸ªåˆ†å—çš„å†…å®¹å¯¹æ¯”
            print("\nğŸ” åˆ†å—å†…å®¹å¯¹æ¯”:")
            for i in range(min(2, len(ai_chapter_chunks), len(regex_chapter_chunks))):
                print(f"\n--- åˆ†å— {i+1} ---")
                print(f"AIåˆ†å—å™¨: {ai_chapter_chunks[i].content[:80]}...")
                print(f"ä¼ ç»Ÿåˆ†å—å™¨: {regex_chapter_chunks[i].content[:80]}...")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¯¹æ¯”æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸš€ å¼€å§‹AIåˆ†å—å™¨é›†æˆæµ‹è¯•\n")
    
    # æµ‹è¯•1: AIåˆ†å—å™¨é›†æˆ
    success1 = test_ai_chunker_integration()
    
    # æµ‹è¯•2: å¯¹æ¯”æµ‹è¯•
    success2 = test_fallback_comparison()
    
    if success1 and success2:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("\nâœ… æ€»ç»“:")
        print("- AIåˆ†å—å™¨æˆåŠŸé›†æˆåˆ°ç°æœ‰pipeline")
        print("- å›¾ç‰‡è¡¨æ ¼èƒ½æ­£ç¡®æ ‡æ³¨ç« èŠ‚ID")
        print("- å¼‚æ­¥å¤„ç†åŠŸèƒ½æ­£å¸¸")
        print("- å›é€€æœºåˆ¶å·¥ä½œæ­£å¸¸")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥")

if __name__ == "__main__":
    main() 