#!/usr/bin/env python3
"""
æµ‹è¯•å®Œæ•´çš„PDFå¤„ç†pipelineï¼ˆåŒ…å«metadataå¤„ç†ï¼‰
éªŒè¯ä»PDFåˆ°å®Œæ•´metadataçš„ç«¯åˆ°ç«¯æµç¨‹
"""

import sys
import os
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
src_path = os.path.join(project_root, 'src')
if src_path not in sys.path:
    sys.path.insert(0, src_path)

def test_complete_pipeline():
    """æµ‹è¯•å®Œæ•´çš„PDFå¤„ç†æµç¨‹"""
    
    print("ğŸš€ æµ‹è¯•å®Œæ•´çš„PDFå¤„ç†æµç¨‹ï¼ˆåŒ…å«Metadataï¼‰...")
    
    # æµ‹è¯•æ–‡ä»¶
    test_pdf = "testfiles/åŒ»çµå¤åº™è®¾è®¡æ–¹æ¡ˆ.pdf"
    
    if not os.path.exists(test_pdf):
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_pdf}")
        return
    
    try:
        # å¯¼å…¥PDFè§£æå·¥å…·
        from pdf_processing.pdf_parser_tool import PDFParserTool
        
        # åˆ›å»ºå·¥å…·å®ä¾‹
        tool = PDFParserTool()
        
        print(f"ğŸ“„ å¼€å§‹å¤„ç†: {test_pdf}")
        
        # ä½¿ç”¨page_splitæ¨¡å¼ï¼ŒåŒ…å«æ‰€æœ‰å¤„ç†æ­¥éª¤
        result_json = tool.execute(
            action="parse_page_split",
            pdf_path=test_pdf,
            enable_ai_enhancement=True,
            docling_parallel_processing=False,  # Macä¸Šç¦ç”¨
            ai_parallel_processing=True
        )
        
        # è§£æç»“æœ
        result = json.loads(result_json)
        
        if result["status"] == "success":
            output_dir = result["output_directory"]
            print(f"âœ… å¤„ç†æˆåŠŸï¼è¾“å‡ºç›®å½•: {output_dir}")
            
            # æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
            expected_files = [
                "page_split_processing_result.json",
                "toc_extraction_result.json", 
                "chunks_result.json",
                "metadata/basic_metadata.json",
                "metadata/document_summary.json",
                "metadata/chapter_summaries.json",
                "metadata/derived_questions.json"
            ]
            
            print("\nğŸ“‹ æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶:")
            for file_path in expected_files:
                full_path = os.path.join(output_dir, file_path)
                if os.path.exists(full_path):
                    print(f"  âœ… {file_path}")
                    
                    # æ˜¾ç¤ºæ–‡ä»¶å¤§å°
                    size = os.path.getsize(full_path)
                    print(f"     å¤§å°: {size:,} å­—èŠ‚")
                    
                    # å¦‚æœæ˜¯JSONæ–‡ä»¶ï¼Œæ˜¾ç¤ºéƒ¨åˆ†å†…å®¹
                    if file_path.endswith('.json'):
                        with open(full_path, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            if isinstance(data, dict):
                                print(f"     é”®: {list(data.keys())}")
                            elif isinstance(data, list):
                                print(f"     æ•°ç»„é•¿åº¦: {len(data)}")
                else:
                    print(f"  âŒ {file_path} - æ–‡ä»¶ä¸å­˜åœ¨")
            
            # æ˜¾ç¤ºå¤„ç†ç»Ÿè®¡
            print(f"\nğŸ“Š å¤„ç†ç»Ÿè®¡:")
            print(f"  å¤„ç†æ—¶é—´: {result.get('processing_time', 0):.2f} ç§’")
            print(f"  é¡µé¢æ•°é‡: {result.get('pages_count', 0)}")
            print(f"  TOCç« èŠ‚: {result.get('toc_count', 0)}")
            print(f"  åˆ†å—æ•°é‡: {result.get('chunks_count', 0)}")
            
            # æ£€æŸ¥metadataç›®å½•
            metadata_dir = os.path.join(output_dir, "metadata")
            if os.path.exists(metadata_dir):
                print(f"\nğŸ—‚ï¸ Metadataç›®å½•å†…å®¹:")
                for file in os.listdir(metadata_dir):
                    file_path = os.path.join(metadata_dir, file)
                    size = os.path.getsize(file_path)
                    print(f"  ğŸ“„ {file} ({size:,} å­—èŠ‚)")
            
        else:
            print(f"âŒ å¤„ç†å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_complete_pipeline() 