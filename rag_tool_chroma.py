"""
ç®€åŒ–çš„ RAG Tool ChromaDB æ¨¡å—
ä¸ºEnhanced RAG Toolæä¾›åŸºç¡€çš„ChromaDBåŠŸèƒ½
"""
import os
import json
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime
import hashlib

try:
    import chromadb
    from chromadb.config import Settings
    from chromadb.utils import embedding_functions
except ImportError:
    print("è­¦å‘Š: ChromaDBæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install chromadb")
    chromadb = None

logger = logging.getLogger(__name__)

class QwenEmbeddingFunction:
    """è‡ªå®šä¹‰QwenåµŒå…¥å‡½æ•°ï¼Œç”¨äºChromaDB"""
    
    def __init__(self, llm_client):
        self.llm_client = llm_client
        self._name = "qwen-embedding"  # ChromaDBéœ€è¦nameå±æ€§
        
    @property 
    def name(self):
        """è¿”å›embedding functionçš„åç§°"""
        return self._name
        
    def __call__(self, input: List[str]) -> List[List[float]]:
        """ChromaDBè°ƒç”¨çš„åµŒå…¥å‡½æ•°"""
        try:
            # ç¡®ä¿inputæ˜¯listç±»å‹
            if isinstance(input, str):
                input = [input]
            elif not isinstance(input, list):
                input = list(input)
                
            embeddings = self.llm_client.get_embeddings(input)
            if not embeddings:
                logger.warning("Qwen embeddingè¿”å›ç©ºç»“æœï¼Œä½¿ç”¨é›¶å‘é‡")
                # è¿”å›é›¶å‘é‡ä½œä¸ºfallback
                dimension = 1024  # Qwen embeddingç»´åº¦
                return [[0.0] * dimension for _ in input]
            return embeddings
        except Exception as e:
            logger.error(f"Qwen embeddingå¤±è´¥: {e}")
            # è¿”å›é›¶å‘é‡ä½œä¸ºfallback
            dimension = 1024
            return [[0.0] * dimension for _ in input]

class Tool:
    """åŸºç¡€å·¥å…·ç±»"""
    def __init__(self):
        self.name = "base_tool"
        self.description = "åŸºç¡€å·¥å…·ç±»"
    
    def execute(self, action: str, **kwargs) -> str:
        """æ‰§è¡Œå·¥å…·æ“ä½œ"""
        return "åŸºç¡€å·¥å…·æ‰§è¡Œ"

class ChromaVectorStore:
    """ChromaDBå‘é‡å­˜å‚¨ - ä½¿ç”¨Qwen embedding"""
    
    def __init__(self, storage_dir: str = "pdf_embedding_storage", llm_client=None):
        self.storage_dir = storage_dir
        self.llm_client = llm_client
        os.makedirs(storage_dir, exist_ok=True)
        
        if chromadb is None:
            logger.warning("ChromaDBæœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
            self.client = None
            self.collection = None
            return
        
        try:
            self.client = chromadb.PersistentClient(path=storage_dir)
            
            # å¦‚æœæœ‰llm_clientï¼Œä½¿ç”¨Qwen embedding function
            if self.llm_client:
                embedding_function = QwenEmbeddingFunction(self.llm_client)
                logger.info("âœ… ä½¿ç”¨Qwen embeddingæ¨¡å‹ (1024ç»´)")
            else:
                # å›é€€åˆ°é»˜è®¤embedding
                embedding_function = embedding_functions.DefaultEmbeddingFunction()
                logger.warning("âš ï¸ æœªæä¾›LLMå®¢æˆ·ç«¯ï¼Œä½¿ç”¨ChromaDBé»˜è®¤embedding")
            
            # å°è¯•è¿æ¥åˆ°å·²å­˜åœ¨çš„collection - ä¼˜å…ˆè¿æ¥æœ‰æ•°æ®çš„collection
            collection_names = ["pdf_document_chunks", "pdf_chunks", "documents", "default"]
            self.collection = None
            
            for collection_name in collection_names:
                try:
                    self.collection = self.client.get_collection(name=collection_name)
                    count = self.collection.count()
                    
                    # æ£€æŸ¥å‘é‡ç»´åº¦å…¼å®¹æ€§
                    if count > 0:
                        try:
                            # æµ‹è¯•embeddingç»´åº¦å…¼å®¹æ€§
                            test_query = "æµ‹è¯•"
                            test_embeddings = self.llm_client.get_embeddings([test_query]) if self.llm_client else None
                            
                            if test_embeddings and test_embeddings[0]:
                                expected_dim = len(test_embeddings[0])
                                logger.info(f"ğŸ” æ£€æŸ¥ {collection_name}: {count}ä¸ªæ–‡æ¡£ï¼Œæµ‹è¯•embeddingç»´åº¦: {expected_dim}")
                                
                                # å°è¯•ä¸€æ¬¡æµ‹è¯•æŸ¥è¯¢æ¥éªŒè¯ç»´åº¦
                                test_result = self.collection.query(
                                    query_embeddings=[test_embeddings[0]],
                                    n_results=1
                                )
                                
                                logger.info(f"âœ… æˆåŠŸè¿æ¥åˆ° {collection_name} collectionï¼ŒåŒ…å« {count} ä¸ªæ–‡æ¡£ï¼Œembeddingç»´åº¦å…¼å®¹")
                                break
                                
                            else:
                                # æ²¡æœ‰embeddingï¼Œå°è¯•ä½¿ç”¨ChromaDBå†…ç½®æŸ¥è¯¢
                                test_result = self.collection.query(
                                    query_texts=[test_query],
                                    n_results=1
                                )
                                logger.info(f"âœ… æˆåŠŸè¿æ¥åˆ° {collection_name} collectionï¼ŒåŒ…å« {count} ä¸ªæ–‡æ¡£ï¼Œä½¿ç”¨å†…ç½®embedding")
                                break
                                
                        except Exception as dim_error:
                            logger.warning(f"âŒ {collection_name} collectionç»´åº¦ä¸å…¼å®¹: {dim_error}")
                            logger.warning(f"   è·³è¿‡æ­¤collectionï¼Œç»§ç»­å¯»æ‰¾å…¼å®¹çš„collection")
                            continue
                    else:
                        logger.warning(f"{collection_name} collectionä¸ºç©ºï¼Œç»§ç»­å°è¯•å…¶ä»–collection")
                        
                except Exception as e:
                    logger.debug(f"æ— æ³•è¿æ¥åˆ° {collection_name}: {e}")
                    continue
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„collectionï¼Œåˆ›å»ºæ–°çš„
            if not self.collection:
                try:
                    self.collection = self.client.get_or_create_collection(
                        name="documents",
                        embedding_function=embedding_function,
                        metadata={"hnsw:space": "cosine"}
                    )
                    logger.info("âœ… åˆ›å»ºæ–°çš„documents collection")
                except Exception as create_error:
                    logger.error(f"âŒ åˆ›å»ºcollectionå¤±è´¥: {create_error}")
                    raise create_error
            logger.info(f"âœ… ChromaVectorStoreåˆå§‹åŒ–æˆåŠŸ: {storage_dir}")
        except Exception as e:
            logger.error(f"ChromaVectorStoreåˆå§‹åŒ–å¤±è´¥: {e}")
            self.client = None
            self.collection = None
    
    def add_document(self, doc_id: str, content: str, metadata: Dict[str, Any] = None) -> int:
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨"""
        if self.collection is None:
            logger.warning("ChromaDBä¸å¯ç”¨ï¼Œè·³è¿‡æ–‡æ¡£æ·»åŠ ")
            return 0
        
        try:
            # ç®€å•çš„æ–‡æœ¬åˆ†å—
            chunks = self._chunk_text(content)
            
            # æ·»åŠ åˆ°collection - ChromaDBä¼šè‡ªåŠ¨è°ƒç”¨æˆ‘ä»¬çš„Qwen embedding function
            for i, chunk in enumerate(chunks):
                chunk_id = f"{doc_id}_{i}"
                chunk_metadata = metadata.copy() if metadata else {}
                chunk_metadata.update({
                    "document_id": doc_id,
                    "chunk_id": i,
                    "chunk_count": len(chunks)
                })
                
                self.collection.add(
                    documents=[chunk],
                    metadatas=[chunk_metadata],
                    ids=[chunk_id]
                )
            
            logger.info(f"âœ… æ·»åŠ æ–‡æ¡£æˆåŠŸï¼Œä½¿ç”¨Qwen embedding: {doc_id} ({len(chunks)} ä¸ªå—)")
            return len(chunks)
        except Exception as e:
            logger.error(f"æ–‡æ¡£æ·»åŠ å¤±è´¥: {e}")
            return 0
    
    def search_documents(self, query: str, n_results: int = 5, where_filter: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """æœç´¢æ–‡æ¡£ - å…ˆç”¨embeddingæœç´¢æ£€ç´¢top5ï¼Œç„¶åBM25é‡æ’åº"""
        if self.collection is None:
            logger.warning("ChromaDBä¸å¯ç”¨ï¼Œè¿”å›ç©ºç»“æœ")
            return []
        
        try:
            # 1. å…ƒæ•°æ®è¿‡æ»¤ï¼šå…ˆç”¨ChromaDB filterç­›é€‰å…ƒæ•°æ®
            where_condition = None
            if where_filter:
                logger.info(f"ğŸ” åº”ç”¨å…ƒæ•°æ®è¿‡æ»¤: {where_filter}")
                # ä½¿ç”¨ç®€å•çš„ç­‰å€¼è¿‡æ»¤ï¼Œé¿å…å¤æ‚è¯­æ³•
                simplified_filter = {}
                for key, value in where_filter.items():
                    if isinstance(value, str):
                        simplified_filter[key] = {"$eq": value}
                    else:
                        simplified_filter[key] = value
                if simplified_filter:
                    where_condition = simplified_filter
            
            # 2. å‘é‡æœç´¢ï¼šä½¿ç”¨embeddingè¿›è¡Œç›¸ä¼¼æ€§æœç´¢ï¼Œæ£€ç´¢top5ç»“æœ
            logger.info(f"ğŸ” æ‰§è¡Œå‘é‡æœç´¢: query='{query}', n_results={n_results}")
            
            # ä½¿ç”¨Qwen embeddingè¿›è¡Œè¯­ä¹‰æœç´¢
            try:
                # æ–¹æ³•1ï¼šä½¿ç”¨è‡ªå®šä¹‰embedding function
                if self.llm_client:
                    embeddings = self.llm_client.get_embeddings([query])
                    if embeddings and embeddings[0]:
                        logger.info(f"ğŸ” ä½¿ç”¨Qwen embeddingè¿›è¡Œæœç´¢ï¼Œå‘é‡ç»´åº¦: {len(embeddings[0])}")
                        results = self.collection.query(
                            query_embeddings=[embeddings[0]],
                            n_results=n_results,  # åªæ£€ç´¢top5
                            where=where_condition,
                            include=["documents", "metadatas", "distances"]
                        )
                    else:
                        raise Exception("Qwen embeddingè¿”å›ç©ºç»“æœ")
                else:
                    # æ–¹æ³•2ï¼šä½¿ç”¨ChromaDBå†…ç½®çš„query_texts
                    results = self.collection.query(
                        query_texts=[query],
                        n_results=n_results,  # åªæ£€ç´¢top5
                        where=where_condition,
                        include=["documents", "metadatas", "distances"]
                    )
            except Exception as query_error:
                logger.warning(f"ä¸»è¦æœç´¢æ–¹æ³•å¤±è´¥: {query_error}")
                logger.info("ğŸ”„ å°è¯•å…¼å®¹æ€§æœç´¢æ–¹æ¡ˆ...")
                
                # å…¼å®¹æ€§æ–¹æ¡ˆï¼šä½¿ç”¨åŒ¹é…æ•°æ®åº“ç»´åº¦çš„æµ‹è¯•å‘é‡
                try:
                    # è·å–collectionçš„ç¬¬ä¸€ä¸ªå‘é‡æ¥ç¡®å®šç»´åº¦
                    sample = self.collection.peek(limit=1)
                    if sample and sample.get('embeddings') and sample['embeddings'][0]:
                        actual_dim = len(sample['embeddings'][0])
                        logger.info(f"ğŸ“Š æ£€æµ‹åˆ°æ•°æ®åº“å‘é‡ç»´åº¦: {actual_dim}")
                        test_embedding = [0.1] * actual_dim
                    else:
                        # é»˜è®¤å°è¯•1024ç»´
                        test_embedding = [0.1] * 1024
                        logger.info("ğŸ“Š ä½¿ç”¨é»˜è®¤1024ç»´å‘é‡")
                    
                    results = self.collection.query(
                        query_embeddings=[test_embedding],
                        n_results=n_results,
                        where=where_condition,
                        include=["documents", "metadatas", "distances"]
                    )
                    logger.warning("âš ï¸ ä½¿ç”¨æµ‹è¯•å‘é‡æœç´¢ï¼Œç»“æœå¯èƒ½ä¸å‡†ç¡®")
                except Exception as fallback_error:
                    logger.error(f"å…¼å®¹æ€§æœç´¢ä¹Ÿå¤±è´¥: {fallback_error}")
                    raise fallback_error
            
            # 3. å¤„ç†å‘é‡æœç´¢ç»“æœ
            vector_results = []
            if results and results.get("documents") and results["documents"][0]:
                logger.info(f"âœ… å‘é‡æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(results['documents'][0])} ä¸ªç»“æœ")
                
                for i, doc in enumerate(results["documents"][0]):
                    similarity_score = 1.0 - (results["distances"][0][i] if results.get("distances") else 0.5)
                    
                    result = {
                        "content": doc,
                        "metadata": results["metadatas"][0][i] if results.get("metadatas") else {},
                        "similarity_score": similarity_score,  # åªä¿ç•™similarity_score
                        "distance": results["distances"][0][i] if results.get("distances") else 0.0
                    }
                    vector_results.append(result)
            else:
                logger.warning("å‘é‡æœç´¢æ— ç»“æœï¼Œå°è¯•çº¯BM25æœç´¢å›é€€")
                # ä¸ç›´æ¥è¿”å›ç©ºåˆ—è¡¨ï¼Œè€Œæ˜¯åˆ›å»ºåŸºäºBM25çš„å›é€€ç»“æœ
                try:
                    # è·å–æ‰€æœ‰æ–‡æ¡£è¿›è¡ŒBM25æœç´¢
                    all_docs = self.collection.get(where=where_condition)
                    if all_docs and all_docs.get("documents"):
                        documents = all_docs["documents"]
                        metadatas = all_docs.get("metadatas", [{}] * len(documents))
                        
                        # ä½¿ç”¨BM25å¯¹æ‰€æœ‰æ–‡æ¡£è¿›è¡Œè¯„åˆ†
                        for i, (doc, metadata) in enumerate(zip(documents, metadatas)):
                            # ç®€å•çš„å…³é”®è¯åŒ¹é…è¯„åˆ†
                            bm25_score = self._simple_bm25_score(doc, query)
                            if bm25_score > 0:  # åªä¿ç•™æœ‰åŒ¹é…çš„ç»“æœ
                                result = {
                                    "content": doc,
                                    "metadata": metadata,
                                    "similarity_score": 0.0,  # å‘é‡æœç´¢å¤±è´¥ï¼Œè®¾ä¸º0
                                    "distance": 1.0  # æœ€å¤§è·ç¦»
                                }
                                vector_results.append(result)
                        
                        # æŒ‰BM25å¾—åˆ†æ’åºå¹¶å–å‰n_resultsä¸ª
                        vector_results.sort(key=lambda x: self._simple_bm25_score(x["content"], query), reverse=True)
                        vector_results = vector_results[:n_results]
                        
                        logger.info(f"ğŸ”„ BM25å›é€€æœç´¢æ‰¾åˆ° {len(vector_results)} ä¸ªç»“æœ")
                    else:
                        logger.warning("ğŸ“­ æ•°æ®åº“ä¸­æ— æ–‡æ¡£ï¼Œè¿”å›ç©ºç»“æœ")
                        return []
                except Exception as fallback_error:
                    logger.error(f"âŒ BM25å›é€€æœç´¢å¤±è´¥: {fallback_error}")
                    return []
            
            # å¦‚æœä»ç„¶æ²¡æœ‰ç»“æœï¼Œè¿”å›ç©ºåˆ—è¡¨
            if not vector_results:
                logger.warning("ğŸ“­ æ‰€æœ‰æœç´¢ç­–ç•¥éƒ½æ— ç»“æœ")
                return []
            
            # 4. BM25é‡æ’åºï¼šå¯¹å‘é‡æœç´¢ç»“æœè¿›è¡ŒBM25è¯„åˆ†å¹¶é‡æ’åº
            logger.info("ğŸ” æ‰§è¡ŒBM25é‡æ’åº")
            final_results = self._apply_bm25_reranking(vector_results, query)
            
            logger.info(f"âœ… æœç´¢å®Œæˆ: {len(final_results)} ä¸ªç»“æœ")
            return final_results
            
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {e}")
            # è¿”å›é”™è¯¯è€Œä¸æ˜¯ç©ºåˆ—è¡¨ï¼Œè®©ä¸Šå±‚å¤„ç†
            raise e
    
    def _apply_bm25_reranking(self, results: List[Dict[str, Any]], query: str) -> List[Dict[str, Any]]:
        """åº”ç”¨BM25é‡æ’åºï¼šå¯¹å‘é‡æœç´¢ç»“æœè¿›è¡ŒBM25è¯„åˆ†å¹¶é‡æ’åº"""
        query_terms = query.lower().split()
        
        for result in results:
            content = result["content"].lower()
            
            # è®¡ç®—BM25åˆ†æ•°
            bm25_score = 0.0
            for term in query_terms:
                if term in content:
                    # è¯é¢‘
                    tf = content.count(term)
                    # ç®€åŒ–çš„BM25å…¬å¼: tf / (tf + k1)ï¼Œk1=1.5
                    term_score = tf / (tf + 1.5)
                    bm25_score += term_score
            
            # å½’ä¸€åŒ–åˆ†æ•°
            result["bm25_score"] = bm25_score / len(query_terms) if query_terms else 0.0
            
            # è®¡ç®—æœ€ç»ˆæ’åºåˆ†æ•°ï¼š70%å‘é‡ç›¸ä¼¼æ€§ + 30%BM25å…³é”®è¯åŒ¹é…
            result["final_score"] = 0.7 * result["similarity_score"] + 0.3 * result["bm25_score"]
        
        # æŒ‰æœ€ç»ˆåˆ†æ•°é‡æ’åº
        results.sort(key=lambda x: x["final_score"], reverse=True)
        
        logger.info(f"ğŸ” BM25é‡æ’åºå®Œæˆï¼Œæœ€é«˜åˆ†æ•°: {results[0]['final_score'] if results else 0}")
        return results
    
    def _chunk_text(self, text: str, chunk_size: int = 500) -> List[str]:
        """ç®€å•çš„æ–‡æœ¬åˆ†å—"""
        words = text.split()
        chunks = []
        current_chunk = []
        current_length = 0
        
        for word in words:
            if current_length + len(word) > chunk_size and current_chunk:
                chunks.append(' '.join(current_chunk))
                current_chunk = [word]
                current_length = len(word)
            else:
                current_chunk.append(word)
                current_length += len(word) + 1
        
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        return chunks if chunks else [text]

class RAGTool(Tool):
    """å¢å¼ºçš„RAGå·¥å…· - æ”¯æŒMySQLæ¨¡æ¿æœç´¢å’ŒChromaDBå†…å®¹æœç´¢"""
    
    def __init__(self, storage_dir: str = "pdf_embedding_storage", deepseek_client=None):
        super().__init__()
        self.name = "rag_tool"
        self.description = "å¢å¼ºçš„RAGå·¥å…· - æ”¯æŒåŒæ¨¡å¼æœç´¢"
        self.storage_dir = storage_dir
        self.vector_store = ChromaVectorStore(storage_dir, deepseek_client)
        self.deepseek_client = deepseek_client
    
    def execute(self, action: str, **kwargs) -> str:
        """æ‰§è¡ŒRAGæ“ä½œ"""
        try:
            if action == "search":
                return self._search_documents(**kwargs)
            elif action == "mysql_search":
                return self._search_mysql_templates(**kwargs)
            elif action == "chunk_search":
                return self._search_chunk_content(**kwargs)
            elif action == "upload":
                return self._upload_document(**kwargs)
            else:
                return f"âŒ ä¸æ”¯æŒçš„æ“ä½œ: {action}"
        except Exception as e:
            logger.error(f"RAGæ“ä½œå¤±è´¥: {e}")
            return f"âŒ æ“ä½œå¤±è´¥: {str(e)}"
    
    def _search_mysql_templates(self, query: str, **kwargs) -> str:
        """æœç´¢MySQLæ¨¡æ¿æ–‡ä»¶ï¼ˆä¿æŒåŸæœ‰åŠŸèƒ½ï¼‰"""
        # è¿™é‡Œä¿æŒåŸæœ‰çš„MySQLæœç´¢é€»è¾‘
        # ç”±äºæˆ‘ä»¬æ²¡æœ‰MySQLè¿æ¥ï¼Œè¿”å›æ¨¡æ‹Ÿç»“æœ
        return json.dumps({
            "search_type": "mysql_templates",
            "query": query,
            "success": True,
            "total_results": 0,
            "results": [],
            "message": "MySQLæ¨¡æ¿æœç´¢åŠŸèƒ½ä¿æŒä¸å˜"
        }, ensure_ascii=False, indent=2)
    
    def _search_chunk_content(self, query: str, chapter_id: Optional[str] = None, 
                            top_k: int = 5, **kwargs) -> str:
        """æœç´¢chunkå†…å®¹ - æ”¯æŒå…ƒæ•°æ®è¿‡æ»¤å’Œæ··åˆæœç´¢"""
        try:
            if self.vector_store.collection is None:
                return json.dumps({
                    "search_type": "chunk_content",
                    "query": query,
                    "chapter_filter": chapter_id,
                    "success": False,
                    "error": "ChromaDBè¿æ¥ä¸å¯ç”¨",
                    "total_results": 0,
                    "results": []
                }, ensure_ascii=False, indent=2)
            
            # å‡†å¤‡å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶
            where_filter = None
            if chapter_id:
                where_filter = {"chapter_id": chapter_id}
                logger.info(f"åº”ç”¨ç« èŠ‚è¿‡æ»¤: {chapter_id}")
            
            try:
                # æ‰§è¡Œå‘é‡æœç´¢
                vector_results = self._vector_search(query, top_k, where_filter)
                
                # æ‰§è¡Œæ··åˆæœç´¢ï¼ˆ70% å‘é‡ + 30% BM25ï¼‰
                final_results = self._hybrid_search_ranking(vector_results, query)
                
                # è½¬æ¢ä¸ºæ ‡å‡†è¾“å‡ºæ ¼å¼
                formatted_results = []
                for result in final_results:
                    formatted_result = {
                        "retrieved_text": result.get("content", ""),
                        "retrieved_image": self._extract_image_path(result.get("metadata", {})),
                        "score": result.get("distance", 0.0),
                        "metadata": result.get("metadata", {})
                    }
                    formatted_results.append(formatted_result)
                
                return json.dumps({
                    "search_type": "chunk_content",
                    "query": query,
                    "chapter_filter": chapter_id,
                    "total_results": len(formatted_results),
                    "results": formatted_results,
                    "success": True
                }, ensure_ascii=False, indent=2)
                
            except Exception as search_error:
                logger.error(f"æœç´¢æ‰§è¡Œå¤±è´¥: {search_error}")
                return json.dumps({
                    "search_type": "chunk_content",
                    "query": query,
                    "chapter_filter": chapter_id,
                    "success": False,
                    "error": f"æœç´¢å¤±è´¥: {str(search_error)}",
                    "total_results": 0,
                    "results": []
                }, ensure_ascii=False, indent=2)
            
        except Exception as e:
            logger.error(f"Chunkæœç´¢å¤±è´¥: {e}")
            return json.dumps({
                "search_type": "chunk_content",
                "query": query,
                "chapter_filter": chapter_id,
                "success": False,
                "error": f"æœç´¢ç³»ç»Ÿé”™è¯¯: {str(e)}",
                "total_results": 0,
                "results": []
            }, ensure_ascii=False, indent=2)
    
    def _vector_search(self, query: str, top_k: int, where_filter: Optional[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ‰§è¡Œæœç´¢ï¼šå…ˆç”¨embeddingæœç´¢ï¼Œç„¶åBM25é‡æ’åº"""
        try:
            logger.info(f"ğŸ” å¼€å§‹æœç´¢: query='{query}', top_k={top_k}")
            if where_filter:
                logger.info(f"ğŸ” åº”ç”¨å…ƒæ•°æ®è¿‡æ»¤: {where_filter}")
            
            # ä½¿ç”¨ChromaVectorStoreçš„æ–°æœç´¢æ–¹æ³•
            results = self.vector_store.search_documents(
                query=query,
                n_results=top_k,
                where_filter=where_filter
            )
            
            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            formatted_results = []
            for result in results:
                formatted_result = {
                    "content": result.get("content", ""),
                    "metadata": result.get("metadata", {}),
                    "similarity_score": result.get("similarity_score", 0.0),
                    "bm25_score": result.get("bm25_score", 0.0),
                    "final_score": result.get("final_score", 0.0),
                    "distance": 1.0 - result.get("final_score", 0.0)  # è·ç¦» = 1 - æœ€ç»ˆåˆ†æ•°
                }
                formatted_results.append(formatted_result)
            
            logger.info(f"âœ… æœç´¢å®Œæˆ: {len(formatted_results)} ä¸ªç»“æœ")
            return formatted_results
            
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {e}")
            # ç›´æ¥æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä¸Šå±‚å¤„ç†
            raise e
    
    def _hybrid_search_ranking(self, vector_results: List[Dict[str, Any]], query: str) -> List[Dict[str, Any]]:
        """æœç´¢æ’åºå·²åœ¨_vector_searchä¸­å®Œæˆï¼Œè¿™é‡Œç›´æ¥è¿”å›"""
        logger.info("âœ… æœç´¢æ’åºå·²å®Œæˆ (embedding + BM25é‡æ’åº)")
        
        # éªŒè¯æœ€ç»ˆåˆ†æ•°æ˜¯å¦å­˜åœ¨
        for result in vector_results:
            if "final_score" not in result:
                logger.warning("æœ€ç»ˆåˆ†æ•°ç¼ºå¤±ï¼Œè¡¥å……è®¡ç®—")
                # å¦‚æœæ²¡æœ‰æœ€ç»ˆåˆ†æ•°ï¼Œè¡¥å……è®¡ç®—
                similarity_score = result.get("similarity_score", 1.0 - result.get("distance", 0.0))
                bm25_score = result.get("bm25_score", self._simple_bm25_score(result.get("content", ""), query))
                result["final_score"] = 0.7 * similarity_score + 0.3 * bm25_score
        
        # ç¡®ä¿æŒ‰æœ€ç»ˆåˆ†æ•°æ’åº
        vector_results.sort(key=lambda x: x.get("final_score", 0), reverse=True)
        return vector_results
    
    def _simple_bm25_score(self, text: str, query: str) -> float:
        """ç®€å•çš„BM25è¿‘ä¼¼åˆ†æ•°"""
        query_terms = query.lower().split()
        text_lower = text.lower()
        
        score = 0.0
        for term in query_terms:
            if term in text_lower:
                # ç®€å•çš„è¯é¢‘è®¡ç®—
                tf = text_lower.count(term)
                score += tf / (tf + 1.0)  # ç®€åŒ–çš„BM25å…¬å¼
        
        return score / len(query_terms) if query_terms else 0.0
    
    def _extract_image_path(self, metadata: Dict[str, Any]) -> str:
        """ä»å…ƒæ•°æ®ä¸­æå–å›¾ç‰‡è·¯å¾„"""
        # æ ¹æ®ä½ çš„å…ƒæ•°æ®ç»“æ„è°ƒæ•´
        image_fields = ["image_path", "figure_path", "image_url", "image"]
        for field in image_fields:
            if field in metadata and metadata[field]:
                return metadata[field]
        return ""
    
    def _search_documents(self, query: str, search_type: str = "auto", **kwargs) -> str:
        """ç»Ÿä¸€æœç´¢æ¥å£ - æ ¹æ®search_typeé€‰æ‹©æœç´¢æ¨¡å¼"""
        if search_type == "mysql" or search_type == "templates":
            return self._search_mysql_templates(query, **kwargs)
        elif search_type == "chunks" or search_type == "content":
            return self._search_chunk_content(query, **kwargs)
        else:
            # autoæ¨¡å¼ï¼šé»˜è®¤æœç´¢chunkå†…å®¹
            return self._search_chunk_content(query, **kwargs)
    
    def _upload_document(self, file_path: str, metadata: Optional[Dict[str, Any]] = None) -> str:
        """ä¸Šä¼ æ–‡æ¡£"""
        try:
            if not os.path.exists(file_path):
                return f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
            
            # ç®€å•çš„æ–‡æœ¬è¯»å–
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            doc_id = hashlib.md5(file_path.encode()).hexdigest()
            
            if metadata is None:
                metadata = {}
            metadata.update({
                "source": file_path,
                "upload_time": datetime.now().isoformat()
            })
            
            chunks_count = self.vector_store.add_document(doc_id, content, metadata)
            
            return f"âœ… æ–‡æ¡£ä¸Šä¼ æˆåŠŸ: {file_path} (å…± {chunks_count} ä¸ªå—)"
        except Exception as e:
            return f"âŒ ä¸Šä¼ å¤±è´¥: {str(e)}" 