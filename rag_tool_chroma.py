"""
ç®€åŒ–çš„ RAG Tool ChromaDB æ¨¡å—
ä¸ºEnhanced RAG Toolæä¾›åŸºç¡€çš„ChromaDBåŠŸèƒ½
"""
import os
import json
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime
import hashlib

try:
    import chromadb
    from chromadb.config import Settings
    from chromadb.utils import embedding_functions
except ImportError:
    print("è­¦å‘Š: ChromaDBæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install chromadb")
    chromadb = None

logger = logging.getLogger(__name__)

class QwenEmbeddingFunction:
    """è‡ªå®šä¹‰QwenåµŒå…¥å‡½æ•°ï¼Œç”¨äºChromaDB"""
    
    def __init__(self, llm_client):
        self.llm_client = llm_client
        self._name = "qwen-embedding"  # ChromaDBéœ€è¦nameå±æ€§
        
    @property 
    def name(self):
        """è¿”å›embedding functionçš„åç§°"""
        return self._name
        
    def __call__(self, input: List[str]) -> List[List[float]]:
        """ChromaDBè°ƒç”¨çš„åµŒå…¥å‡½æ•°"""
        try:
            # ç¡®ä¿inputæ˜¯listç±»å‹
            if isinstance(input, str):
                input = [input]
            elif not isinstance(input, list):
                input = list(input)
                
            embeddings = self.llm_client.get_embeddings(input)
            if not embeddings:
                logger.warning("Qwen embeddingè¿”å›ç©ºç»“æœï¼Œä½¿ç”¨é›¶å‘é‡")
                # è¿”å›é›¶å‘é‡ä½œä¸ºfallback
                dimension = 1024  # Qwen embeddingç»´åº¦
                return [[0.0] * dimension for _ in input]
            return embeddings
        except Exception as e:
            logger.error(f"Qwen embeddingå¤±è´¥: {e}")
            # è¿”å›é›¶å‘é‡ä½œä¸ºfallback
            dimension = 1024
            return [[0.0] * dimension for _ in input]

class Tool:
    """åŸºç¡€å·¥å…·ç±»"""
    def __init__(self):
        self.name = "base_tool"
        self.description = "åŸºç¡€å·¥å…·ç±»"
    
    def execute(self, action: str, **kwargs) -> str:
        """æ‰§è¡Œå·¥å…·æ“ä½œ"""
        return "åŸºç¡€å·¥å…·æ‰§è¡Œ"

class ChromaVectorStore:
    """ChromaDBå‘é‡å­˜å‚¨ - ä½¿ç”¨Qwen embedding"""
    
    def __init__(self, storage_dir: str = "final_chromadb", llm_client=None):
        self.storage_dir = storage_dir
        self.llm_client = llm_client
        os.makedirs(storage_dir, exist_ok=True)
        
        if chromadb is None:
            logger.warning("ChromaDBæœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
            self.client = None
            self.collection = None
            return
        
        try:
            self.client = chromadb.PersistentClient(path=storage_dir)
            
            # å¦‚æœæœ‰llm_clientï¼Œä½¿ç”¨Qwen embedding function
            if self.llm_client:
                embedding_function = QwenEmbeddingFunction(self.llm_client)
                logger.info("âœ… ä½¿ç”¨Qwen embeddingæ¨¡å‹ (1024ç»´)")
            else:
                # å›é€€åˆ°é»˜è®¤embedding
                embedding_function = embedding_functions.DefaultEmbeddingFunction()
                logger.warning("âš ï¸ æœªæä¾›LLMå®¢æˆ·ç«¯ï¼Œä½¿ç”¨ChromaDBé»˜è®¤embedding")
            
            # ç›´æ¥ä½¿ç”¨æœ‰æ•°æ®çš„collection (140ä¸ªæ–‡æ¡£)
            collection_names = [
                "wrapped_documents_qwen_embedding",  # ä¸»è¦æ•°æ®collection (1024ç»´ï¼ŒåŒ»çµå¤åº™é¡¹ç›®)
            ]
            self.collection = None
            
            for collection_name in collection_names:
                try:
                    self.collection = self.client.get_collection(name=collection_name)
                    count = self.collection.count()
                    
                    # æ£€æŸ¥å‘é‡ç»´åº¦å…¼å®¹æ€§
                    if count > 0:
                        try:
                            # æ ¹æ®å½“å‰embeddingç±»å‹æµ‹è¯•ç»´åº¦å…¼å®¹æ€§
                            test_query = "æµ‹è¯•"
                            
                            if self.llm_client:
                                # æœ‰LLMå®¢æˆ·ç«¯ï¼Œä½¿ç”¨custom embeddingæµ‹è¯•
                                test_embeddings = self.llm_client.get_embeddings([test_query])
                                if test_embeddings and test_embeddings[0]:
                                    expected_dim = len(test_embeddings[0])
                                    logger.info(f"ğŸ” æ£€æŸ¥ {collection_name}: {count}ä¸ªæ–‡æ¡£ï¼ŒLLM embeddingç»´åº¦: {expected_dim}")
                                    
                                    # æµ‹è¯•æŸ¥è¯¢
                                    test_result = self.collection.query(
                                        query_embeddings=[test_embeddings[0]],
                                        n_results=1
                                    )
                                    logger.info(f"âœ… æˆåŠŸè¿æ¥åˆ° {collection_name} collectionï¼ŒåŒ…å« {count} ä¸ªæ–‡æ¡£ï¼ŒLLM embeddingå…¼å®¹")
                                    break
                            else:
                                # æ²¡æœ‰LLMå®¢æˆ·ç«¯ï¼Œä½¿ç”¨ChromaDBé»˜è®¤embeddingæµ‹è¯•
                                logger.info(f"ğŸ” æ£€æŸ¥ {collection_name}: {count}ä¸ªæ–‡æ¡£ï¼Œæµ‹è¯•é»˜è®¤embeddingå…¼å®¹æ€§")
                                
                                # ç›´æ¥å°è¯•æŸ¥è¯¢ï¼Œå¦‚æœç»´åº¦ä¸å…¼å®¹ä¼šæŠ›å‡ºå¼‚å¸¸
                                test_result = self.collection.query(
                                    query_texts=[test_query],
                                    n_results=1
                                )
                                logger.info(f"âœ… æˆåŠŸè¿æ¥åˆ° {collection_name} collectionï¼ŒåŒ…å« {count} ä¸ªæ–‡æ¡£ï¼Œé»˜è®¤embeddingå…¼å®¹")
                                break
                                
                        except Exception as dim_error:
                            if "dimension" in str(dim_error).lower() and "1024" in str(dim_error):
                                logger.error(f"âŒ {collection_name} collectionéœ€è¦1024ç»´embedding: {dim_error}")
                                if not self.llm_client:
                                    logger.error("ğŸ’¡ è§£å†³æ–¹æ¡ˆ: éœ€è¦é…ç½®LLMå®¢æˆ·ç«¯ä»¥æä¾›1024ç»´embedding")
                                    logger.error("   1. æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®é…ç½®")
                                    logger.error("   2. æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
                                    logger.error("   3. ç¡®ä¿LLMClientèƒ½æ­£å¸¸åˆå§‹åŒ–")
                                else:
                                    logger.error("ğŸ’¡ LLMå®¢æˆ·ç«¯å·²é…ç½®ä½†embeddingç»´åº¦ä»ä¸åŒ¹é…ï¼Œè¯·æ£€æŸ¥é…ç½®")
                            else:
                                logger.warning(f"âŒ {collection_name} collectionç»´åº¦ä¸å…¼å®¹: {dim_error}")
                            
                            self.collection = None  # é‡ç½®ä¸ºNone
                            continue
                    else:
                        logger.warning(f"âš ï¸ {collection_name} collectionä¸ºç©ºï¼Œç»§ç»­å°è¯•å…¶ä»–collection")
                        self.collection = None  # é‡ç½®ä¸ºNone
                        
                except Exception as e:
                    logger.debug(f"æ— æ³•è¿æ¥åˆ° {collection_name}: {e}")
                    self.collection = None  # é‡ç½®ä¸ºNone
                    continue
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„collectionï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯LLMå®¢æˆ·ç«¯é—®é¢˜
            if not self.collection:
                logger.error("âŒ æ— æ³•è¿æ¥åˆ°æ•°æ®collection")
                logger.error("ğŸ“Š ç³»ç»Ÿæ£€æµ‹åˆ° wrapped_documents_qwen_embedding collection åŒ…å«140ä¸ªæ–‡æ¡£")
                logger.error("ğŸ”§ ä½†éœ€è¦LLMå®¢æˆ·ç«¯æä¾›1024ç»´embeddingæ‰èƒ½è®¿é—®")
                
                if not self.llm_client:
                    logger.error("ğŸ’¡ è§£å†³æ–¹æ¡ˆ: é…ç½®LLMå®¢æˆ·ç«¯")
                    logger.error("   è¯·ç¡®ä¿APIå¯†é’¥æ­£ç¡®é…ç½®ï¼Œä»¥å¯ç”¨1024ç»´embeddingæ”¯æŒ")
                    # ä¸åˆ›å»ºæ–°collectionï¼Œå› ä¸ºæ•°æ®å·²ç»å­˜åœ¨äº1024ç»´collectionä¸­
                    raise RuntimeError("æ— æ³•è®¿é—®1024ç»´æ•°æ®collectionï¼Œéœ€è¦é…ç½®LLMå®¢æˆ·ç«¯")
                else:
                    logger.error("ğŸ’¡ LLMå®¢æˆ·ç«¯å·²é…ç½®ä½†ä»æ— æ³•è®¿é—®ï¼Œè¯·æ£€æŸ¥é…ç½®æˆ–é‡è¯•")
                    raise RuntimeError("LLMå®¢æˆ·ç«¯å·²é…ç½®ä½†æ— æ³•è®¿é—®1024ç»´æ•°æ®collection")
                
                # åŸæ¥çš„åˆ›å»ºæ–°collectioné€»è¾‘è¢«ç§»é™¤ï¼Œå› ä¸ºæˆ‘ä»¬è¦ä½¿ç”¨ç°æœ‰æ•°æ®
            logger.info(f"âœ… ChromaVectorStoreåˆå§‹åŒ–æˆåŠŸ: {storage_dir}")
        except Exception as e:
            logger.error(f"ChromaVectorStoreåˆå§‹åŒ–å¤±è´¥: {e}")
            self.client = None
            self.collection = None
    
    def add_document(self, doc_id: str, content: str, metadata: Dict[str, Any] = None) -> int:
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨"""
        if self.collection is None:
            logger.warning("ChromaDBä¸å¯ç”¨ï¼Œè·³è¿‡æ–‡æ¡£æ·»åŠ ")
            return 0
        
        try:
            # ç®€å•çš„æ–‡æœ¬åˆ†å—
            chunks = self._chunk_text(content)
            
            # æ·»åŠ åˆ°collection - ChromaDBä¼šè‡ªåŠ¨è°ƒç”¨æˆ‘ä»¬çš„Qwen embedding function
            for i, chunk in enumerate(chunks):
                chunk_id = f"{doc_id}_{i}"
                chunk_metadata = metadata.copy() if metadata else {}
                chunk_metadata.update({
                    "document_id": doc_id,
                    "chunk_id": i,
                    "chunk_count": len(chunks)
                })
                
                self.collection.add(
                    documents=[chunk],
                    metadatas=[chunk_metadata],
                    ids=[chunk_id]
                )
            
            logger.info(f"âœ… æ·»åŠ æ–‡æ¡£æˆåŠŸï¼Œä½¿ç”¨Qwen embedding: {doc_id} ({len(chunks)} ä¸ªå—)")
            return len(chunks)
        except Exception as e:
            logger.error(f"æ–‡æ¡£æ·»åŠ å¤±è´¥: {e}")
            return 0
    
    def search_documents(self, query: str, n_results: int = 5, where_filter: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """æœç´¢æ–‡æ¡£ - å…ˆç”¨embeddingæœç´¢æ£€ç´¢top5ï¼Œç„¶åBM25é‡æ’åº"""
        if self.collection is None:
            logger.warning("ChromaDBä¸å¯ç”¨ï¼Œè¿”å›ç©ºç»“æœ")
            return []
        
        try:
            # 1. å…ƒæ•°æ®è¿‡æ»¤ï¼šå…ˆç”¨ChromaDB filterç­›é€‰å…ƒæ•°æ®
            where_condition = None
            if where_filter:
                logger.info(f"ğŸ” åº”ç”¨å…ƒæ•°æ®è¿‡æ»¤: {where_filter}")
                # æ„å»ºChromaDBå…¼å®¹çš„whereæ¡ä»¶
                conditions = []
                for key, value in where_filter.items():
                    if isinstance(value, str):
                        conditions.append({key: {"$eq": value}})
                    else:
                        conditions.append({key: value})
                
                if conditions:
                    # å¦‚æœåªæœ‰ä¸€ä¸ªæ¡ä»¶ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦‚æœæœ‰å¤šä¸ªæ¡ä»¶ï¼Œä½¿ç”¨$and
                    if len(conditions) == 1:
                        where_condition = conditions[0]
                    else:
                        where_condition = {"$and": conditions}
                    logger.info(f"ğŸ” æ„å»ºçš„whereæ¡ä»¶: {where_condition}")
            
            # 2. å‘é‡æœç´¢ï¼šä½¿ç”¨embeddingè¿›è¡Œç›¸ä¼¼æ€§æœç´¢ï¼Œæ£€ç´¢top5ç»“æœ
            logger.info(f"ğŸ” æ‰§è¡Œå‘é‡æœç´¢: query='{query}', n_results={n_results}")
            
            # ä½¿ç”¨Qwen embeddingè¿›è¡Œè¯­ä¹‰æœç´¢
            try:
                # æ–¹æ³•1ï¼šä½¿ç”¨è‡ªå®šä¹‰embedding function
                if self.llm_client:
                    embeddings = self.llm_client.get_embeddings([query])
                    if embeddings and embeddings[0]:
                        logger.info(f"ğŸ” ä½¿ç”¨Qwen embeddingè¿›è¡Œæœç´¢ï¼Œå‘é‡ç»´åº¦: {len(embeddings[0])}")
                        results = self.collection.query(
                            query_embeddings=[embeddings[0]],
                            n_results=n_results,  # åªæ£€ç´¢top5
                            where=where_condition,
                            include=["documents", "metadatas", "distances"]
                        )
                    else:
                        raise Exception("Qwen embeddingè¿”å›ç©ºç»“æœ")
                else:
                    # æ–¹æ³•2ï¼šä½¿ç”¨ChromaDBå†…ç½®çš„query_texts
                    results = self.collection.query(
                        query_texts=[query],
                        n_results=n_results,  # åªæ£€ç´¢top5
                        where=where_condition,
                        include=["documents", "metadatas", "distances"]
                    )
            except Exception as query_error:
                logger.warning(f"ä¸»è¦æœç´¢æ–¹æ³•å¤±è´¥: {query_error}")
                
                # å¦‚æœæ˜¯ç»´åº¦ä¸åŒ¹é…é”™è¯¯ï¼Œç›´æ¥è¿”å›ç©ºç»“æœè€Œä¸æ˜¯å°è¯•å…¼å®¹æ€§æœç´¢
                if "dimension" in str(query_error).lower():
                    logger.info("ğŸ”„ æ£€æµ‹åˆ°ç»´åº¦ä¸åŒ¹é…ï¼Œè¿”å›ç©ºç»“æœ")
                    return []
                
                logger.info("ğŸ”„ å°è¯•å…¼å®¹æ€§æœç´¢æ–¹æ¡ˆ...")
                
                # å…¼å®¹æ€§æ–¹æ¡ˆï¼šä½¿ç”¨åŒ¹é…æ•°æ®åº“ç»´åº¦çš„æµ‹è¯•å‘é‡
                try:
                    # è·å–collectionçš„ç¬¬ä¸€ä¸ªå‘é‡æ¥ç¡®å®šç»´åº¦
                    sample = self.collection.peek(limit=1)
                    if sample and sample.get('embeddings') and sample['embeddings'][0]:
                        actual_dim = len(sample['embeddings'][0])
                        logger.info(f"ğŸ“Š æ£€æµ‹åˆ°æ•°æ®åº“å‘é‡ç»´åº¦: {actual_dim}")
                        test_embedding = [0.1] * actual_dim
                    else:
                        # é»˜è®¤å°è¯•1024ç»´
                        test_embedding = [0.1] * 1024
                        logger.info("ğŸ“Š ä½¿ç”¨é»˜è®¤1024ç»´å‘é‡")
                    
                    results = self.collection.query(
                        query_embeddings=[test_embedding],
                        n_results=n_results,
                        where=where_condition,
                        include=["documents", "metadatas", "distances"]
                    )
                    logger.warning("âš ï¸ ä½¿ç”¨æµ‹è¯•å‘é‡æœç´¢ï¼Œç»“æœå¯èƒ½ä¸å‡†ç¡®")
                except Exception as fallback_error:
                    logger.error(f"å…¼å®¹æ€§æœç´¢ä¹Ÿå¤±è´¥: {fallback_error}")
                    # è¿”å›ç©ºç»“æœè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
                    return []
            
            # 3. å¤„ç†å‘é‡æœç´¢ç»“æœ
            vector_results = []
            if results and results.get("documents") and results["documents"][0]:
                logger.info(f"âœ… å‘é‡æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(results['documents'][0])} ä¸ªç»“æœ")
                
                for i, doc in enumerate(results["documents"][0]):
                    similarity_score = 1.0 - (results["distances"][0][i] if results.get("distances") else 0.5)
                    
                    result = {
                        "content": doc,
                        "metadata": results["metadatas"][0][i] if results.get("metadatas") else {},
                        "similarity_score": similarity_score,  # åªä¿ç•™similarity_score
                        "distance": results["distances"][0][i] if results.get("distances") else 0.0
                    }
                    vector_results.append(result)
            else:
                logger.warning("å‘é‡æœç´¢æ— ç»“æœï¼Œå°è¯•çº¯BM25æœç´¢å›é€€")
                # ä¸ç›´æ¥è¿”å›ç©ºåˆ—è¡¨ï¼Œè€Œæ˜¯åˆ›å»ºåŸºäºBM25çš„å›é€€ç»“æœ
                try:
                    # è·å–æ‰€æœ‰æ–‡æ¡£è¿›è¡ŒBM25æœç´¢
                    all_docs = self.collection.get(where=where_condition)
                    if all_docs and all_docs.get("documents"):
                        documents = all_docs["documents"]
                        metadatas = all_docs.get("metadatas", [{}] * len(documents))
                        
                        # ä½¿ç”¨BM25å¯¹æ‰€æœ‰æ–‡æ¡£è¿›è¡Œè¯„åˆ†
                        for i, (doc, metadata) in enumerate(zip(documents, metadatas)):
                            # ç®€å•çš„å…³é”®è¯åŒ¹é…è¯„åˆ†
                            try:
                                bm25_score = self._simple_bm25_score(doc, query)
                                if bm25_score > 0:  # åªä¿ç•™æœ‰åŒ¹é…çš„ç»“æœ
                                    result = {
                                        "content": doc,
                                        "metadata": metadata,
                                        "similarity_score": 0.0,  # å‘é‡æœç´¢å¤±è´¥ï¼Œè®¾ä¸º0
                                        "distance": 1.0  # æœ€å¤§è·ç¦»
                                    }
                                    vector_results.append(result)
                            except Exception as bm25_error:
                                logger.warning(f"BM25è¯„åˆ†å¤±è´¥ï¼Œè·³è¿‡æ–‡æ¡£ {i}: {bm25_error}")
                                continue
                        
                        # æŒ‰BM25å¾—åˆ†æ’åºå¹¶å–å‰n_resultsä¸ª
                        vector_results.sort(key=lambda x: self._simple_bm25_score(x["content"], query), reverse=True)
                        vector_results = vector_results[:n_results]
                        
                        logger.info(f"ğŸ”„ BM25å›é€€æœç´¢æ‰¾åˆ° {len(vector_results)} ä¸ªç»“æœ")
                    else:
                        logger.warning("ğŸ“­ æ•°æ®åº“ä¸­æ— æ–‡æ¡£ï¼Œè¿”å›ç©ºç»“æœ")
                        return []
                except Exception as fallback_error:
                    logger.error(f"âŒ BM25å›é€€æœç´¢å¤±è´¥: {fallback_error}")
                    return []
            
            # å¦‚æœä»ç„¶æ²¡æœ‰ç»“æœï¼Œè¿”å›ç©ºåˆ—è¡¨
            if not vector_results:
                logger.warning("ğŸ“­ æ‰€æœ‰æœç´¢ç­–ç•¥éƒ½æ— ç»“æœ")
                return []
            
            # 4. BM25é‡æ’åºï¼šå¯¹å‘é‡æœç´¢ç»“æœè¿›è¡ŒBM25è¯„åˆ†å¹¶é‡æ’åº
            logger.info("ğŸ” æ‰§è¡ŒBM25é‡æ’åº")
            final_results = self._apply_bm25_reranking(vector_results, query)
            
            logger.info(f"âœ… æœç´¢å®Œæˆ: {len(final_results)} ä¸ªç»“æœ")
            return final_results
            
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {e}")
            # è¿”å›é”™è¯¯è€Œä¸æ˜¯ç©ºåˆ—è¡¨ï¼Œè®©ä¸Šå±‚å¤„ç†
            raise e
    
    def _apply_bm25_reranking(self, results: List[Dict[str, Any]], query: str) -> List[Dict[str, Any]]:
        """åº”ç”¨BM25é‡æ’åºï¼šå¯¹å‘é‡æœç´¢ç»“æœè¿›è¡ŒBM25è¯„åˆ†å¹¶é‡æ’åº"""
        query_terms = query.lower().split()
        
        for result in results:
            # æ ¹æ®å†…å®¹ç±»å‹æ„å»ºBM25æœç´¢æ–‡æœ¬
            bm25_text = self._build_bm25_text(result)
            
            # è®¡ç®—BM25åˆ†æ•°
            bm25_score = self._calculate_enhanced_bm25_score(bm25_text, query_terms)
            result["bm25_score"] = bm25_score
            
            # è®¡ç®—æœ€ç»ˆæ’åºåˆ†æ•°ï¼š70%å‘é‡ç›¸ä¼¼æ€§ + 30%BM25å…³é”®è¯åŒ¹é…
            result["final_score"] = 0.7 * result["similarity_score"] + 0.3 * result["bm25_score"]
        
        # æŒ‰æœ€ç»ˆåˆ†æ•°é‡æ’åº
        results.sort(key=lambda x: x["final_score"], reverse=True)
        
        logger.info(f"ğŸ” BM25é‡æ’åºå®Œæˆï¼Œæœ€é«˜åˆ†æ•°: {results[0]['final_score'] if results else 0}")
        return results
    
    def _build_bm25_text(self, result: Dict[str, Any]) -> str:
        """ä¸ºBM25è®¡ç®—æ„å»ºæœç´¢æ–‡æœ¬ - æ ¹æ®å†…å®¹ç±»å‹ä½¿ç”¨ä¸åŒå­—æ®µç»„åˆ"""
        metadata = result.get("metadata", {})
        content_type = metadata.get("type", "")
        
        # å¯¹äºå›¾ç‰‡å’Œè¡¨æ ¼ï¼Œä½¿ç”¨æŒ‡å®šçš„å­—æ®µç»„åˆ
        if content_type in ["image_chunk", "table_chunk"]:
            parts = []
            
            # ä¼˜å…ˆä½¿ç”¨ä¸‰ä¸ªä¸»è¦æè¿°å­—æ®µçš„ç»„åˆ
            search_summary = metadata.get("search_summary", "")
            detailed_description = metadata.get("detailed_description", "")
            engineering_details = metadata.get("engineering_details", "")
            
            if search_summary and search_summary.strip():
                parts.append(str(search_summary.strip()))
            
            if detailed_description and detailed_description.strip():
                # å¦‚æœdetailed_descriptionæ˜¯JSONæ ¼å¼ï¼Œå°è¯•è§£æ
                if detailed_description.startswith('```json'):
                    try:
                        # æå–JSONå†…å®¹
                        json_start = detailed_description.find('{')
                        json_end = detailed_description.rfind('}') + 1
                        if json_start != -1 and json_end > json_start:
                            import json
                            desc_data = json.loads(detailed_description[json_start:json_end])
                            
                            # æå–JSONä¸­çš„æè¿°å­—æ®µï¼Œç¡®ä¿è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                            if "search_summary" in desc_data and desc_data["search_summary"]:
                                parts.append(str(desc_data["search_summary"]))
                            if "detailed_description" in desc_data and desc_data["detailed_description"]:
                                parts.append(str(desc_data["detailed_description"]))
                            if "engineering_details" in desc_data and desc_data["engineering_details"]:
                                parts.append(str(desc_data["engineering_details"]))
                    except Exception as e:
                        # JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸæ–‡
                        parts.append(str(detailed_description.strip()))
                else:
                    parts.append(str(detailed_description.strip()))
            
            if engineering_details and engineering_details.strip():
                parts.append(str(engineering_details.strip()))
            
            # å¦‚æœä¸Šè¿°ä¸‰ä¸ªå­—æ®µéƒ½ä¸ºç©ºï¼Œæ‰ä½¿ç”¨page_contextä½œä¸ºå›é€€
            if not parts:
                page_context = metadata.get("page_context", "")
                if page_context and page_context.strip():
                    parts.append(str(page_context.strip()))
            
            # å¦‚æœä»ç„¶æ²¡æœ‰å†…å®¹ï¼Œä½¿ç”¨captionä½œä¸ºæœ€åå›é€€
            if not parts:
                caption = metadata.get("caption", "")
                if caption and caption.strip():
                    parts.append(f"{content_type.replace('_chunk', '')}: {str(caption)}")
            
            bm25_text = " ".join(parts)
            
        else:
            # å¯¹äºæ–‡æœ¬å†…å®¹ï¼Œç›´æ¥ä½¿ç”¨contentå­—æ®µ
            bm25_text = result.get("content", "")
        
        return bm25_text.lower() if bm25_text else ""
    
    def _calculate_enhanced_bm25_score(self, text: str, query_terms: List[str]) -> float:
        """å¢å¼ºçš„BM25åˆ†æ•°è®¡ç®—"""
        if not text or not query_terms:
            return 0.0
        
        # BM25å‚æ•°
        k1 = 1.2
        b = 0.75
        
        # æ–‡æ¡£é•¿åº¦å½’ä¸€åŒ–
        doc_length = len(text.split())
        avgdl = 100  # å‡è®¾å¹³å‡æ–‡æ¡£é•¿åº¦ä¸º100è¯
        
        bm25_score = 0.0
        
        for term in query_terms:
            if term in text:
                # è¯é¢‘
                tf = text.count(term)
                
                # BM25 TFéƒ¨åˆ†
                tf_component = (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_length / avgdl)))
                
                # ç®€åŒ–çš„IDF (å‡è®¾è¯çš„é€†æ–‡æ¡£é¢‘ç‡)
                idf = 1.0  # ç®€åŒ–ç‰ˆæœ¬ï¼Œå¯ä»¥æ ¹æ®éœ€è¦æ”¹è¿›
                
                bm25_score += tf_component * idf
        
        # å½’ä¸€åŒ–åˆ†æ•° (0-1èŒƒå›´)
        max_possible_score = len(query_terms) * (k1 + 1)
        normalized_score = bm25_score / max_possible_score if max_possible_score > 0 else 0.0
        
        return min(normalized_score, 1.0)  # ç¡®ä¿ä¸è¶…è¿‡1
    
    def _chunk_text(self, text: str, chunk_size: int = 500) -> List[str]:
        """ç®€å•çš„æ–‡æœ¬åˆ†å—"""
        words = text.split()
        chunks = []
        current_chunk = []
        current_length = 0
        
        for word in words:
            if current_length + len(word) > chunk_size and current_chunk:
                chunks.append(' '.join(current_chunk))
                current_chunk = [word]
                current_length = len(word)
            else:
                current_chunk.append(word)
                current_length += len(word) + 1
        
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        return chunks if chunks else [text]

class RAGTool(Tool):
    """å¢å¼ºçš„RAGå·¥å…· - æ”¯æŒMySQLæ¨¡æ¿æœç´¢å’ŒChromaDBå†…å®¹æœç´¢"""
    
    def __init__(self, storage_dir: str = "final_chromadb", deepseek_client=None):
        super().__init__()
        self.name = "rag_tool"
        self.description = "å¢å¼ºçš„RAGå·¥å…· - æ”¯æŒåŒæ¨¡å¼æœç´¢"
        self.storage_dir = storage_dir
        self.vector_store = ChromaVectorStore(storage_dir, deepseek_client)
        self.deepseek_client = deepseek_client
    
    def execute(self, action: str, **kwargs) -> str:
        """æ‰§è¡ŒRAGæ“ä½œ"""
        try:
            if action == "search":
                return self._search_documents(**kwargs)
            elif action == "mysql_search":
                return self._search_mysql_templates(**kwargs)
            elif action == "chunk_search":
                return self._search_chunk_content(**kwargs)
            elif action == "upload":
                return self._upload_document(**kwargs)
            else:
                return f"âŒ ä¸æ”¯æŒçš„æ“ä½œ: {action}"
        except Exception as e:
            logger.error(f"RAGæ“ä½œå¤±è´¥: {e}")
            return f"âŒ æ“ä½œå¤±è´¥: {str(e)}"
    
    def _search_mysql_templates(self, query: str, **kwargs) -> str:
        """æœç´¢MySQLæ¨¡æ¿æ–‡ä»¶ï¼ˆä¿æŒåŸæœ‰åŠŸèƒ½ï¼‰"""
        # è¿™é‡Œä¿æŒåŸæœ‰çš„MySQLæœç´¢é€»è¾‘
        # ç”±äºæˆ‘ä»¬æ²¡æœ‰MySQLè¿æ¥ï¼Œè¿”å›æ¨¡æ‹Ÿç»“æœ
        return json.dumps({
            "search_type": "mysql_templates",
            "query": query,
            "success": True,
            "total_results": 0,
            "results": [],
            "message": "MySQLæ¨¡æ¿æœç´¢åŠŸèƒ½ä¿æŒä¸å˜"
        }, ensure_ascii=False, indent=2)
    
    def _search_chunk_content(self, query: str, chapter_id: Optional[str] = None, 
                            top_k: int = 5, **kwargs) -> str:
        """æœç´¢chunkå†…å®¹ - æ”¯æŒå…ƒæ•°æ®è¿‡æ»¤å’Œæ··åˆæœç´¢"""
        try:
            if self.vector_store.collection is None:
                return json.dumps({
                    "search_type": "chunk_content",
                    "query": query,
                    "chapter_filter": chapter_id,
                    "success": False,
                    "error": "ChromaDBè¿æ¥ä¸å¯ç”¨",
                    "total_results": 0,
                    "results": []
                }, ensure_ascii=False, indent=2)
            
            # å‡†å¤‡å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶
            where_filter = None
            if chapter_id:
                where_filter = {"chapter_id": chapter_id}
                logger.info(f"åº”ç”¨ç« èŠ‚è¿‡æ»¤: {chapter_id}")
            
            try:
                # æ‰§è¡Œå‘é‡æœç´¢
                vector_results = self._vector_search(query, top_k, where_filter)
                
                # æ‰§è¡Œæ··åˆæœç´¢ï¼ˆ70% å‘é‡ + 30% BM25ï¼‰
                final_results = self._hybrid_search_ranking(vector_results, query)
                
                # è½¬æ¢ä¸ºæ ‡å‡†è¾“å‡ºæ ¼å¼
                formatted_results = []
                for result in final_results:
                    formatted_result = {
                        "retrieved_text": result.get("content", ""),
                        "retrieved_image": self._extract_image_path(result.get("metadata", {})),
                        "score": result.get("distance", 0.0),
                        "metadata": result.get("metadata", {})
                    }
                    formatted_results.append(formatted_result)
                
                return json.dumps({
                    "search_type": "chunk_content",
                    "query": query,
                    "chapter_filter": chapter_id,
                    "total_results": len(formatted_results),
                    "results": formatted_results,
                    "success": True
                }, ensure_ascii=False, indent=2)
                
            except Exception as search_error:
                logger.error(f"æœç´¢æ‰§è¡Œå¤±è´¥: {search_error}")
                return json.dumps({
                    "search_type": "chunk_content",
                    "query": query,
                    "chapter_filter": chapter_id,
                    "success": False,
                    "error": f"æœç´¢å¤±è´¥: {str(search_error)}",
                    "total_results": 0,
                    "results": []
                }, ensure_ascii=False, indent=2)
            
        except Exception as e:
            logger.error(f"Chunkæœç´¢å¤±è´¥: {e}")
            return json.dumps({
                "search_type": "chunk_content",
                "query": query,
                "chapter_filter": chapter_id,
                "success": False,
                "error": f"æœç´¢ç³»ç»Ÿé”™è¯¯: {str(e)}",
                "total_results": 0,
                "results": []
            }, ensure_ascii=False, indent=2)
    
    def _vector_search(self, query: str, top_k: int, where_filter: Optional[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ‰§è¡Œæœç´¢ï¼šå…ˆç”¨embeddingæœç´¢ï¼Œç„¶åBM25é‡æ’åº"""
        try:
            logger.info(f"ğŸ” å¼€å§‹æœç´¢: query='{query}', top_k={top_k}")
            if where_filter:
                logger.info(f"ğŸ” åº”ç”¨å…ƒæ•°æ®è¿‡æ»¤: {where_filter}")
            
            # ä½¿ç”¨ChromaVectorStoreçš„æ–°æœç´¢æ–¹æ³•
            results = self.vector_store.search_documents(
                query=query,
                n_results=top_k,
                where_filter=where_filter
            )
            
            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            formatted_results = []
            for result in results:
                formatted_result = {
                    "content": result.get("content", ""),
                    "metadata": result.get("metadata", {}),
                    "similarity_score": result.get("similarity_score", 0.0),
                    "bm25_score": result.get("bm25_score", 0.0),
                    "final_score": result.get("final_score", 0.0),
                    "distance": 1.0 - result.get("final_score", 0.0)  # è·ç¦» = 1 - æœ€ç»ˆåˆ†æ•°
                }
                formatted_results.append(formatted_result)
            
            logger.info(f"âœ… æœç´¢å®Œæˆ: {len(formatted_results)} ä¸ªç»“æœ")
            return formatted_results
            
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {e}")
            # ç›´æ¥æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä¸Šå±‚å¤„ç†
            raise e
    
    def _hybrid_search_ranking(self, vector_results: List[Dict[str, Any]], query: str) -> List[Dict[str, Any]]:
        """æœç´¢æ’åºå·²åœ¨_vector_searchä¸­å®Œæˆï¼Œè¿™é‡Œç›´æ¥è¿”å›"""
        logger.info("âœ… æœç´¢æ’åºå·²å®Œæˆ (embedding + BM25é‡æ’åº)")
        
        # éªŒè¯æœ€ç»ˆåˆ†æ•°æ˜¯å¦å­˜åœ¨
        for result in vector_results:
            if "final_score" not in result:
                logger.warning("æœ€ç»ˆåˆ†æ•°ç¼ºå¤±ï¼Œè¡¥å……è®¡ç®—")
                # å¦‚æœæ²¡æœ‰æœ€ç»ˆåˆ†æ•°ï¼Œè¡¥å……è®¡ç®—
                similarity_score = result.get("similarity_score", 1.0 - result.get("distance", 0.0))
                bm25_score = result.get("bm25_score", self._simple_bm25_score(result.get("content", ""), query))
                result["final_score"] = 0.7 * similarity_score + 0.3 * bm25_score
        
        # ç¡®ä¿æŒ‰æœ€ç»ˆåˆ†æ•°æ’åº
        vector_results.sort(key=lambda x: x.get("final_score", 0), reverse=True)
        return vector_results
    
    def _simple_bm25_score(self, text: str, query: str) -> float:
        """ç®€å•çš„BM25è¿‘ä¼¼åˆ†æ•°"""
        try:
            # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²ç±»å‹
            text_str = str(text) if text is not None else ""
            query_str = str(query) if query is not None else ""
            
            query_terms = query_str.lower().split()
            text_lower = text_str.lower()
            
            score = 0.0
            for term in query_terms:
                if term in text_lower:
                    # ç®€å•çš„è¯é¢‘è®¡ç®—
                    tf = text_lower.count(term)
                    score += tf / (tf + 1.0)  # ç®€åŒ–çš„BM25å…¬å¼
            
            return score / len(query_terms) if query_terms else 0.0
        except Exception as e:
            logger.warning(f"BM25åˆ†æ•°è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _extract_image_path(self, metadata: Dict[str, Any]) -> str:
        """ä»å…ƒæ•°æ®ä¸­æå–å›¾ç‰‡è·¯å¾„"""
        # æ ¹æ®ä½ çš„å…ƒæ•°æ®ç»“æ„è°ƒæ•´
        image_fields = ["image_path", "figure_path", "image_url", "image"]
        for field in image_fields:
            if field in metadata and metadata[field]:
                return metadata[field]
        return ""
    
    def _search_documents(self, query: str, search_type: str = "auto", **kwargs) -> str:
        """ç»Ÿä¸€æœç´¢æ¥å£ - æ ¹æ®search_typeé€‰æ‹©æœç´¢æ¨¡å¼"""
        if search_type == "mysql" or search_type == "templates":
            return self._search_mysql_templates(query, **kwargs)
        elif search_type == "chunks" or search_type == "content":
            return self._search_chunk_content(query, **kwargs)
        else:
            # autoæ¨¡å¼ï¼šé»˜è®¤æœç´¢chunkå†…å®¹
            return self._search_chunk_content(query, **kwargs)
    
    def _upload_document(self, file_path: str, metadata: Optional[Dict[str, Any]] = None) -> str:
        """ä¸Šä¼ æ–‡æ¡£"""
        try:
            if not os.path.exists(file_path):
                return f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
            
            # ç®€å•çš„æ–‡æœ¬è¯»å–
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            doc_id = hashlib.md5(file_path.encode()).hexdigest()
            
            if metadata is None:
                metadata = {}
            metadata.update({
                "source": file_path,
                "upload_time": datetime.now().isoformat()
            })
            
            chunks_count = self.vector_store.add_document(doc_id, content, metadata)
            
            return f"âœ… æ–‡æ¡£ä¸Šä¼ æˆåŠŸ: {file_path} (å…± {chunks_count} ä¸ªå—)"
        except Exception as e:
            return f"âŒ ä¸Šä¼ å¤±è´¥: {str(e)}" 