#!/usr/bin/env python3
"""
æµ‹è¯•PDFè§£æå·¥å…·çš„å®Œæ•´å·¥ä½œæµç¨‹
æµ‹è¯•parse_page_splitæ¨¡å¼çš„ç«¯åˆ°ç«¯å¤„ç†
"""

import os
import sys
import json
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.pdf_processing.pdf_parser_tool import PDFParserTool
from src.pdf_processing.config import PDFProcessingConfig

def test_complete_workflow():
    """æµ‹è¯•å®Œæ•´çš„PDFå¤„ç†å·¥ä½œæµç¨‹"""
    
    # é…ç½®æµ‹è¯•å‚æ•°
    test_pdf_path = "testfiles/åŒ»çµå¤åº™è®¾è®¡æ–¹æ¡ˆ.pdf"
    
    # æ£€æŸ¥æµ‹è¯•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(test_pdf_path):
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_pdf_path}")
        return False
    
    # åˆ›å»ºPDFè§£æå·¥å…·
    config = PDFProcessingConfig()
    pdf_tool = PDFParserTool(config)
    
    print("ğŸš€ å¼€å§‹æµ‹è¯•å®Œæ•´PDFå¤„ç†å·¥ä½œæµç¨‹")
    print(f"ğŸ“ æµ‹è¯•æ–‡ä»¶: {test_pdf_path}")
    print("="*60)
    
    start_time = time.time()
    
    try:
        # æ‰§è¡Œå®Œæ•´çš„é¡µé¢åˆ†å‰²è§£ææµç¨‹
        result_json = pdf_tool.execute(
            action="parse_page_split",
            pdf_path=test_pdf_path,
            enable_ai_enhancement=True,
            docling_parallel_processing=False,  # Macä¸Šç¦ç”¨doclingå¹¶è¡Œ
            ai_parallel_processing=True         # å¯ç”¨AIå¹¶è¡Œå¤„ç†
        )
        
        # è§£æç»“æœ
        result = json.loads(result_json)
        
        if result["status"] == "success":
            output_dir = result["output_directory"]
            processing_time = result["processing_time"]
            
            print(f"âœ… å¤„ç†æˆåŠŸ!")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
            print(f"â±ï¸ å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
            print(f"ğŸ“„ é¡µé¢æ•°é‡: {result['pages_count']}")
            print(f"ğŸ“– TOCç« èŠ‚æ•°: {result['toc_count']}")
            print(f"ğŸ”ª åˆ†å—æ•°é‡: {result['chunks_count']}")
            
            # éªŒè¯è¾“å‡ºæ–‡ä»¶
            print("\nğŸ“‹ éªŒè¯è¾“å‡ºæ–‡ä»¶:")
            verify_output_files(output_dir)
            
            return True
            
        else:
            print(f"âŒ å¤„ç†å¤±è´¥: {result['message']}")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        end_time = time.time()
        total_time = end_time - start_time
        print(f"\nâ±ï¸ æ€»æµ‹è¯•æ—¶é—´: {total_time:.2f} ç§’")

def verify_output_files(output_dir: str) -> None:
    """éªŒè¯è¾“å‡ºæ–‡ä»¶æ˜¯å¦ç”Ÿæˆ"""
    
    expected_files = [
        "page_split_processing_result.json",
        "toc_extraction_result.json", 
        "chunks_result.json",
        "metadata/basic_metadata.json",
        "metadata/document_summary.json",
        "metadata/chapter_summaries.json",
        "metadata/derived_questions.json"
    ]
    
    all_files_exist = True
    
    for file_path in expected_files:
        full_path = os.path.join(output_dir, file_path)
        if os.path.exists(full_path):
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(full_path)
            print(f"âœ… {file_path} ({file_size:,} bytes)")
        else:
            print(f"âŒ {file_path} (æ–‡ä»¶ä¸å­˜åœ¨)")
            all_files_exist = False
    
    # æ£€æŸ¥é¡µé¢ç›®å½•
    page_dirs = [d for d in os.listdir(output_dir) if d.startswith("page_")]
    if page_dirs:
        print(f"ğŸ“ é¡µé¢ç›®å½•: {len(page_dirs)} ä¸ª ({', '.join(sorted(page_dirs)[:5])}{'...' if len(page_dirs) > 5 else ''})")
        
        # æ£€æŸ¥ç¬¬ä¸€é¡µçš„åª’ä½“æ–‡ä»¶
        page_1_dir = os.path.join(output_dir, "page_1")
        if os.path.exists(page_1_dir):
            media_files = [f for f in os.listdir(page_1_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]
            if media_files:
                print(f"ğŸ–¼ï¸ ç¬¬1é¡µåª’ä½“æ–‡ä»¶: {len(media_files)} ä¸ª")
    
    # æ˜¾ç¤ºè¾“å‡ºç›®å½•çš„æ€»ä½“ç»Ÿè®¡
    print(f"\nğŸ“Š è¾“å‡ºç›®å½•ç»Ÿè®¡:")
    print(f"   - ç›®å½•: {output_dir}")
    print(f"   - ä¸»è¦æ–‡ä»¶: {sum(1 for f in expected_files if os.path.exists(os.path.join(output_dir, f)))}/{len(expected_files)}")
    print(f"   - é¡µé¢ç›®å½•: {len(page_dirs)} ä¸ª")
    
    if all_files_exist:
        print("ğŸ‰ æ‰€æœ‰é¢„æœŸæ–‡ä»¶éƒ½å·²ç”Ÿæˆ!")
    else:
        print("âš ï¸ æŸäº›æ–‡ä»¶ç¼ºå¤±ï¼Œè¯·æ£€æŸ¥å¤„ç†æ—¥å¿—")

def display_sample_results(output_dir: str) -> None:
    """æ˜¾ç¤ºéƒ¨åˆ†ç»“æœæ ·æœ¬"""
    
    print("\nğŸ“‹ ç»“æœæ ·æœ¬:")
    
    # æ˜¾ç¤ºTOCç»“æœ
    toc_file = os.path.join(output_dir, "toc_extraction_result.json")
    if os.path.exists(toc_file):
        try:
            with open(toc_file, 'r', encoding='utf-8') as f:
                toc_data = json.load(f)
            
            toc_items = toc_data.get("toc", [])
            print(f"ğŸ“– TOCç« èŠ‚ ({len(toc_items)} ä¸ª):")
            for i, item in enumerate(toc_items[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"   {item.get('level', 1)}. {item.get('title', 'Unknown')}")
            if len(toc_items) > 3:
                print(f"   ... è¿˜æœ‰ {len(toc_items) - 3} ä¸ªç« èŠ‚")
                
        except Exception as e:
            print(f"   âš ï¸ è¯»å–TOCæ–‡ä»¶å¤±è´¥: {e}")
    
    # æ˜¾ç¤ºåˆ†å—ç»“æœ
    chunks_file = os.path.join(output_dir, "chunks_result.json")
    if os.path.exists(chunks_file):
        try:
            with open(chunks_file, 'r', encoding='utf-8') as f:
                chunks_data = json.load(f)
            
            chapters = chunks_data.get("first_level_chapters", [])
            chunks = chunks_data.get("minimal_chunks", [])
            
            print(f"ğŸ”ª åˆ†å—ç»“æœ:")
            print(f"   - ä¸€çº§ç« èŠ‚: {len(chapters)} ä¸ª")
            print(f"   - æœ€å°åˆ†å—: {len(chunks)} ä¸ª")
            
            if chapters:
                print(f"   é¦–ç« æ ‡é¢˜: {chapters[0].get('title', 'Unknown')}")
                print(f"   é¦–ç« å­—æ•°: {chapters[0].get('word_count', 0):,} å­—")
                
        except Exception as e:
            print(f"   âš ï¸ è¯»å–åˆ†å—æ–‡ä»¶å¤±è´¥: {e}")
    
    # æ˜¾ç¤ºå…ƒæ•°æ®åŸºæœ¬ä¿¡æ¯
    metadata_file = os.path.join(output_dir, "metadata/basic_metadata.json")
    if os.path.exists(metadata_file):
        try:
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            doc_info = metadata.get("document", {})
            images = metadata.get("images", [])
            tables = metadata.get("tables", [])
            
            print(f"ğŸ“Š åŸºç¡€å…ƒæ•°æ®:")
            print(f"   - æ–‡æ¡£ID: {doc_info.get('document_id', 'Unknown')}")
            print(f"   - é¡µé¢æ•°: {doc_info.get('total_pages', 0)}")
            print(f"   - å›¾ç‰‡æ•°: {len(images)}")
            print(f"   - è¡¨æ ¼æ•°: {len(tables)}")
                
        except Exception as e:
            print(f"   âš ï¸ è¯»å–å…ƒæ•°æ®æ–‡ä»¶å¤±è´¥: {e}")

if __name__ == "__main__":
    print("ğŸ§ª PDFå¤„ç†å·¥å…·å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•")
    print("="*60)
    
    # è¿è¡Œæµ‹è¯•
    success = test_complete_workflow()
    
    if success:
        print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼æ‰€æœ‰ç»„ä»¶å·¥ä½œæ­£å¸¸ã€‚")
        
        # è·å–æœ€æ–°çš„è¾“å‡ºç›®å½•
        parser_output_dir = "parser_output"
        if os.path.exists(parser_output_dir):
            subdirs = [d for d in os.listdir(parser_output_dir) if d.endswith("_page_split")]
            if subdirs:
                latest_dir = os.path.join(parser_output_dir, sorted(subdirs)[-1])
                display_sample_results(latest_dir)
        
        print(f"\nğŸ“ è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹è¾“å‡ºç›®å½•ä¸­çš„æ–‡ä»¶")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å’Œæ—¥å¿—")
        sys.exit(1) 